{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "## Load shp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory:\n",
      " d:\\natcap\\urban-cooling-health\\code\n",
      "Parent folder:\n",
      " d:\\natcap\\urban-cooling-health\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define file path\n",
    "file1_path = r\"G:/Shared drives/Wellcome Trust Project Data/0_source_data/GiGL land use data/GiGL_Trees_font_point/GiGL_GLATrees_Pre2023.shp\"\n",
    "file2_path = r\"G:/Shared drives/Wellcome Trust Project Data/0_source_data/GiGL land use data/GiGL_Trees_font_point/GiGL_GLATrees_2023-24.shp\"\n",
    "\n",
    "# Define output path\n",
    "current_path = os.getcwd()\n",
    "print(\"Current working directory:\\n\", current_path)\n",
    "\n",
    "parent_folder = os.path.dirname(current_path)\n",
    "print(\"Parent folder:\\n\", parent_folder)\n",
    "\n",
    "##\n",
    "wd_shp = r'G:\\Shared drives\\Wellcome Trust Project Data\\1_preprocess\\UrbanCoolingModel'\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Where to save\n",
    "out_dir = Path(parent_folder) / \"data\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # Read the spatial data one by one\n",
    "# d = gpd.read_file(file1_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\natcap\\urban-cooling-health\\data\\GiGL_GLATrees_Pre2023_risk_2050.shp\n",
      "d:\\natcap\\urban-cooling-health\\data\\GiGL_GLATrees_2023-24_risk_2050.shp\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Read the spatial data\u001b[39;00m\n\u001b[32m      8\u001b[39m tree_risk_layer1 = gpd.read_file(f1, engine=\u001b[33m\"\u001b[39m\u001b[33mpyogrio\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m tree_risk_layer2 = \u001b[43mgpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpyogrio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\geopandas\\io\\file.py:294\u001b[39m, in \u001b[36m_read_file\u001b[39m\u001b[34m(filename, bbox, mask, columns, rows, engine, **kwargs)\u001b[39m\n\u001b[32m    291\u001b[39m             from_bytes = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mpyogrio\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_file_pyogrio\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mfiona\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    299\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pd.api.types.is_file_like(filename):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\geopandas\\io\\file.py:547\u001b[39m, in \u001b[36m_read_file_pyogrio\u001b[39m\u001b[34m(path_or_bytes, bbox, mask, rows, **kwargs)\u001b[39m\n\u001b[32m    538\u001b[39m     warnings.warn(\n\u001b[32m    539\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[33m'\u001b[39m\u001b[33minclude_fields\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mignore_fields\u001b[39m\u001b[33m'\u001b[39m\u001b[33m keywords are deprecated, and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    540\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwill be removed in a future release. You can use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m'\u001b[39m\u001b[33m keyword \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    543\u001b[39m         stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    544\u001b[39m     )\n\u001b[32m    545\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m] = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33minclude_fields\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpyogrio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyogrio\\geopandas.py:361\u001b[39m, in \u001b[36mread_dataframe\u001b[39m\u001b[34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, fid_as_index, use_arrow, on_invalid, arrow_to_pandas_kwargs, **kwargs)\u001b[39m\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    360\u001b[39m     index = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m dtype, c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(meta[\u001b[33m\"\u001b[39m\u001b[33mdtypes\u001b[39m\u001b[33m\"\u001b[39m], df.columns):\n\u001b[32m    363\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype.startswith(\u001b[33m\"\u001b[39m\u001b[33mdatetime\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    772\u001b[39m     mgr = \u001b[38;5;28mself\u001b[39m._init_mgr(\n\u001b[32m    773\u001b[39m         data, axes={\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m: index, \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: columns}, dtype=dtype, copy=copy\n\u001b[32m    774\u001b[39m     )\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    777\u001b[39m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m     mgr = \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma.MaskedArray):\n\u001b[32m    780\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[39m, in \u001b[36mdict_to_mgr\u001b[39m\u001b[34m(data, index, columns, dtype, typ, copy)\u001b[39m\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    500\u001b[39m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[32m    501\u001b[39m         arrays = [x.copy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:119\u001b[39m, in \u001b[36marrays_to_mgr\u001b[39m\u001b[34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[39m\n\u001b[32m    116\u001b[39m         index = ensure_index(index)\n\u001b[32m    118\u001b[39m     \u001b[38;5;66;03m# don't force copy because getting jammed in an ndarray anyway\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     arrays, refs = \u001b[43m_homogenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# _homogenize ensures\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m#  - all(len(x) == len(index) for x in arrays)\u001b[39;00m\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m#  - all(x.ndim == 1 for x in arrays)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    125\u001b[39m \n\u001b[32m    126\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    127\u001b[39m     index = ensure_index(index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:629\u001b[39m, in \u001b[36m_homogenize\u001b[39m\u001b[34m(data, index, dtype)\u001b[39m\n\u001b[32m    626\u001b[39m         val = \u001b[38;5;28mdict\u001b[39m(val)\n\u001b[32m    627\u001b[39m     val = lib.fast_multiget(val, oindex._values, default=np.nan)\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m val = \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m com.require_length_match(val, index)\n\u001b[32m    631\u001b[39m refs.append(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\construction.py:606\u001b[39m, in \u001b[36msanitize_array\u001b[39m\u001b[34m(data, index, dtype, copy, allow_2d)\u001b[39m\n\u001b[32m    604\u001b[39m subarr = data\n\u001b[32m    605\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data.dtype == \u001b[38;5;28mobject\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m606\u001b[39m     subarr = \u001b[43mmaybe_infer_to_datetimelike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    608\u001b[39m         object_index\n\u001b[32m    609\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m using_pyarrow_string_dtype()\n\u001b[32m    610\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m is_string_dtype(subarr)\n\u001b[32m    611\u001b[39m     ):\n\u001b[32m    612\u001b[39m         \u001b[38;5;66;03m# Avoid inference when string option is set\u001b[39;00m\n\u001b[32m    613\u001b[39m         subarr = data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\dtypes\\cast.py:1189\u001b[39m, in \u001b[36mmaybe_infer_to_datetimelike\u001b[39m\u001b[34m(value)\u001b[39m\n\u001b[32m   1184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[32m   1186\u001b[39m \u001b[38;5;66;03m# error: Incompatible return value type (got \"Union[ExtensionArray,\u001b[39;00m\n\u001b[32m   1187\u001b[39m \u001b[38;5;66;03m# ndarray[Any, Any]]\", expected \"Union[ndarray[Any, Any], DatetimeArray,\u001b[39;00m\n\u001b[32m   1188\u001b[39m \u001b[38;5;66;03m# TimedeltaArray, PeriodArray, IntervalArray]\")\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaybe_convert_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[32m   1190\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Here we do not convert numeric dtypes, as if we wanted that,\u001b[39;49;00m\n\u001b[32m   1192\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#  numpy would have done it for us.\u001b[39;49;00m\n\u001b[32m   1193\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_numeric\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_non_numeric\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_if_all_nat\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mM8[ns]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2543\u001b[39m, in \u001b[36mpandas._libs.lib.maybe_convert_objects\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\numeric.py:330\u001b[39m, in \u001b[36mfull\u001b[39m\u001b[34m(shape, fill_value, dtype, order, like)\u001b[39m\n\u001b[32m    328\u001b[39m     dtype = fill_value.dtype\n\u001b[32m    329\u001b[39m a = empty(shape, dtype, order)\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m \u001b[43mmultiarray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopyto\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43munsafe\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "## Load all the saved data\n",
    "\n",
    "f1 = os.path.join(parent_folder, \"data\", os.path.basename(file1_path).replace('.shp', '') + \"_risk_2050.shp\"); print(f1)\n",
    "f2 = os.path.join(parent_folder, \"data\", os.path.basename(file2_path).replace('.shp', '') + \"_risk_2050.shp\"); print(f2)\n",
    "\n",
    "\n",
    "# Read the spatial data\n",
    "tree_risk_layer1 = gpd.read_file(f1, engine=\"pyogrio\")\n",
    "tree_risk_layer2 = gpd.read_file(f2, engine=\"pyogrio\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combine shp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 1050054 features to d:\\natcap\\urban-cooling-health\\data\\GiGL_GLATrees_at_risk_2050_merged.gpkg (layer='GiGL_GLATrees_at_risk_2050_merged')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# You already have:\n",
    "# f1, f2\n",
    "# tree_risk_layer1 = gpd.read_file(f1, engine=\"pyogrio\")\n",
    "# tree_risk_layer2 = gpd.read_file(f2, engine=\"pyogrio\")\n",
    "\n",
    "g1 = tree_risk_layer1.copy()\n",
    "g2 = tree_risk_layer2.copy()\n",
    "\n",
    "# Tag source\n",
    "g1[\"source_file\"] = Path(f1).name\n",
    "g2[\"source_file\"] = Path(f2).name\n",
    "\n",
    "# Pick a target CRS (prefer the first non-None)\n",
    "target_crs = g1.crs or g2.crs\n",
    "\n",
    "# Align CRS\n",
    "def _align_crs(g, target):\n",
    "    if target is None:\n",
    "        return g\n",
    "    if g.crs is None:\n",
    "        return g.set_crs(target)\n",
    "    if g.crs != target:\n",
    "        return g.to_crs(target)\n",
    "    return g\n",
    "\n",
    "g1 = _align_crs(g1, target_crs)\n",
    "g2 = _align_crs(g2, target_crs)\n",
    "\n",
    "# Union schema (keep geometry last)\n",
    "all_cols = [c for c in sorted(set(g1.columns) | set(g2.columns)) if c != \"geometry\"] + [\"geometry\"]\n",
    "g1 = gpd.GeoDataFrame(g1.reindex(columns=all_cols), geometry=\"geometry\", crs=target_crs)\n",
    "g2 = gpd.GeoDataFrame(g2.reindex(columns=all_cols), geometry=\"geometry\", crs=target_crs)\n",
    "\n",
    "# Fix invalid geometries (best effort)\n",
    "try:\n",
    "    from shapely import make_valid\n",
    "    g1[\"geometry\"] = make_valid(g1.geometry)\n",
    "    g2[\"geometry\"] = make_valid(g2.geometry)\n",
    "except Exception:\n",
    "    g1[\"geometry\"] = g1.buffer(0)\n",
    "    g2[\"geometry\"] = g2.buffer(0)\n",
    "\n",
    "# Drop empty/null geoms\n",
    "g1 = g1[g1.geometry.notnull() & ~g1.geometry.is_empty]\n",
    "g2 = g2[g2.geometry.notnull() & ~g2.geometry.is_empty]\n",
    "\n",
    "# Concatenate\n",
    "combined = gpd.GeoDataFrame(\n",
    "    pd.concat([g1, g2], ignore_index=True, sort=False),\n",
    "    geometry=\"geometry\",\n",
    "    crs=target_crs\n",
    ")\n",
    "\n",
    "# (Optional) drop exact duplicates (including geometry)\n",
    "# combined = combined.drop_duplicates(subset=[c for c in combined.columns if c != \"geometry\"] + [\"geometry\"])\n",
    "\n",
    "# Save\n",
    "out_path = os.path.join(parent_folder, \"data\", \"GiGL_GLATrees_at_risk_2050_merged.gpkg\")\n",
    "layer_name = Path(out_path).stem  # \"GiGL_GLATrees_at_risk_2050_merged\"\n",
    "\n",
    "combined.to_file(out_path, driver=\"GPKG\", layer=layer_name)\n",
    "print(f\"Wrote {len(combined)} features to {out_path} (layer='{layer_name}')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year year_label   count  percent\n",
      "0  2050       2050  657622    62.63\n",
      "1  <NA>    Unknown  392432    37.37\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- make 'year' sortable & consistent ---\n",
    "if 'year' not in combined.columns:\n",
    "    raise KeyError(\"'year' column not found in combined\")\n",
    "\n",
    "# if it's datetime-like, grab the year; else coerce to numeric\n",
    "if pd.api.types.is_datetime64_any_dtype(combined['year']):\n",
    "    combined['year'] = combined['year'].dt.year.astype('Int64')\n",
    "else:\n",
    "    combined['year'] = pd.to_numeric(combined['year'], errors='coerce').astype('Int64')\n",
    "\n",
    "# --- summarize ---\n",
    "summary = (\n",
    "    combined\n",
    "    .groupby('year', dropna=False)\n",
    "    .size()\n",
    "    .rename('count')\n",
    "    .reset_index()\n",
    "    .sort_values('year', na_position='last')\n",
    ")\n",
    "\n",
    "total_n = len(combined)\n",
    "summary['percent'] = (summary['count'] / total_n * 100).round(2)\n",
    "\n",
    "# for display, keep a nice label while preserving numeric 'year'\n",
    "summary['year_label'] = summary['year'].astype(str).replace('<NA>', 'Unknown')\n",
    "\n",
    "print(summary[['year', 'year_label', 'count', 'percent']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1m grid shp to 10m grid\n",
    "\n",
    "Turn the 1 m × 1 m grid to a 10 m × 10 m grid -- snap cells to a 10 m lattice and aggregate. This avoids creating 100 overlapping squares per tile.\n",
    "\n",
    "* **Warning**: this will take a long time to run! Uncomment this block if update is needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 777007 tiles to d:\\natcap\\urban-cooling-health\\data\\GiGL_GLATrees_at_risk_2050_merged_tiles10.gpkg (layer='tiles10')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "\n",
    "gdf = combined.copy()\n",
    "\n",
    "# 0) Ensure CRS is metric (meters)\n",
    "if gdf.crs is None or not gdf.crs.is_projected:\n",
    "    raise ValueError(\"Project to a metric CRS first (e.g., EPSG:5070 or UTM).\")\n",
    "\n",
    "# 1) Define 10 m grid origin (snap to a multiple of 10 m)\n",
    "cell = 10.0\n",
    "xmin, ymin, xmax, ymax = gdf.total_bounds\n",
    "x0 = np.floor(xmin / cell) * cell\n",
    "y0 = np.floor(ymin / cell) * cell\n",
    "\n",
    "# 2) Compute grid indices for each 1 m cell using its lower-left corner\n",
    "b = gdf.geometry.bounds  # DataFrame with minx, miny, maxx, maxy\n",
    "i = np.floor((b[\"minx\"] - x0) / cell).astype(\"int64\")\n",
    "j = np.floor((b[\"miny\"] - y0) / cell).astype(\"int64\")\n",
    "gdf[\"i\"] = i\n",
    "gdf[\"j\"] = j\n",
    "\n",
    "# 3) Build unique 10 m tiles as polygons\n",
    "keys = gdf[[\"i\", \"j\"]].drop_duplicates().reset_index(drop=True)\n",
    "xs = x0 + keys[\"i\"].to_numpy() * cell\n",
    "ys = y0 + keys[\"j\"].to_numpy() * cell\n",
    "geoms = [box(x, y, x + cell, y + cell) for x, y in zip(xs, ys)]\n",
    "tiles10 = gpd.GeoDataFrame(keys, geometry=geoms, crs=gdf.crs)\n",
    "\n",
    "# 4) (Optional) carry aggregations to 10 m grid\n",
    "# Count how many 1 m cells fell into each 10 m tile (0–100, typically <=100 if partial coverage)\n",
    "counts = gdf.groupby([\"i\", \"j\"]).size().rename(\"n_cells\").reset_index()\n",
    "tiles10 = tiles10.merge(counts, on=[\"i\", \"j\"], how=\"left\")\n",
    "\n",
    "# Example: majority year per 10 m tile (if you have a 'year' column)\n",
    "if \"year\" in gdf.columns:\n",
    "    per_year = gdf.groupby([\"i\", \"j\", \"year\"]).size().rename(\"n\").reset_index()\n",
    "    idx = per_year.groupby([\"i\", \"j\"])[\"n\"].idxmax()\n",
    "    majority_year = per_year.loc[idx, [\"i\", \"j\", \"year\"]]\n",
    "    tiles10 = tiles10.merge(majority_year, on=[\"i\", \"j\"], how=\"left\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Best option: GeoPackage ---\n",
    "gpkg_path = out_dir / \"GiGL_GLATrees_at_risk_2050_merged_tiles10.gpkg\"\n",
    "layer_name = \"tiles10\"\n",
    "\n",
    "# keep grid keys as ints\n",
    "t10 = tiles10.copy()\n",
    "for c in (\"i\", \"j\"):\n",
    "    if c in t10.columns:\n",
    "        t10[c] = t10[c].astype(\"int64\")\n",
    "\n",
    "t10.to_file(gpkg_path, driver=\"GPKG\", layer=layer_name)  # engine=\"pyogrio\" also OK\n",
    "print(f\"Wrote {len(t10)} tiles to {gpkg_path} (layer='{layer_name}')\")\n",
    "\n",
    "# --- (Optional) Shapefile (use short field names, 32-bit ints) ---\n",
    "# shp_path = out_dir / \"tiles10_shp\" / \"tiles10.shp\"\n",
    "# (out_dir / \"tiles10_shp\").mkdir(exist_ok=True)\n",
    "# t_shp = t10.copy()\n",
    "# for c in (\"i\", \"j\"):\n",
    "#     if c in t_shp.columns:\n",
    "#         t_shp[c] = t_shp[c].astype(\"int32\")\n",
    "# t_shp.to_file(shp_path, driver=\"ESRI Shapefile\")\n",
    "# print(f\"Also wrote Shapefile to {shp_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate lc scenario - tree at risk\n",
    "\n",
    "* **Load data and process from here!** if only to update lc scenario raster\n",
    "* Pick one of the land cover inputs data\n",
    "\n",
    "only changes LC pixels that (a) overlap the risk polygons and (b) are currently one of your tree codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.features import rasterize\n",
    "\n",
    "\n",
    "## Load shapefile with tree cover will be at risk or loss\n",
    "shp_tree_risk = out_dir / \"GiGL_GLATrees_at_risk_2050_merged_tiles10.gpkg\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### land cover option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Saved to:\n",
      "\t G:\\Shared drives\\Wellcome Trust Project Data\\1_preprocess\\UrbanCoolingModel\\EP_preliminary_tests\\clipped_lulc\\UKECH\\LCM2021_london_clip2aoi.tif\n",
      "[ 0.  1.  2.  3.  4.  5.  6.  8.  9. 10. 12. 13. 14. 16. 18. 19. 20. 21.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## land cover data\n",
    "lc_input = r\"G:\\Shared drives\\Wellcome Trust Project Data\\1_preprocess\\UrbanCoolingModel\\EP_preliminary_tests\\clipped_lulc\\UKECH\\LCM2021_london.tif\"\n",
    "tree_cover_code = [1, 2]\n",
    "after_risk_lc_code = 4  # 4: 'Improved Grassland',\n",
    "# Define labels\n",
    "land_cover_labels = {\n",
    "    1: 'Deciduous woodland',\n",
    "    2: 'Coniferous woodland',\n",
    "    3: 'Arable',\n",
    "    4: 'Improved Grassland',\n",
    "    5: 'Neutral Grassland',\n",
    "    6: 'Calcareous Grassland',\n",
    "    7: 'Acid grassland',\n",
    "    8: 'Fen, Marsh, and Swamp',\n",
    "    9: 'Heather',\n",
    "    10: 'Heather grassland',\n",
    "    11: 'Bog',\n",
    "    12: 'Inland Rock',\n",
    "    13: 'Saltwater',\n",
    "    14: 'Freshwater',\n",
    "    15: 'Supralittoral Rock',\n",
    "    16: 'Supralittoral Sediment',\n",
    "    17: 'Littoral Rock',\n",
    "    18: 'Littoral Sediment',\n",
    "    19: 'Saltmarsh',\n",
    "    20: 'Urban',\n",
    "    21: 'Suburban',\n",
    "}\n",
    "\n",
    "\n",
    "## clip raster to AOI -------------------------------------------------------------------\n",
    "aoi_shapefile = os.path.join(wd_shp, \"London_Ward_aoi_prj.shp\") \n",
    "# Load administrative boundaries\n",
    "aoi = gpd.read_file(aoi_shapefile)\n",
    "\n",
    "\n",
    "# load function to use in notebook\n",
    "from function_clip_raster_to_aoi import clip_raster_to_aoi\n",
    "\n",
    "\n",
    "## clip \n",
    "# Output path (optional)\n",
    "lc_clipped_tif = lc_input.replace(\".tif\", \"_clip2aoi.tif\")\n",
    "\n",
    "filled_arr, filled_transform, filled_profile = clip_raster_to_aoi(\n",
    "    raster_path=lc_input,\n",
    "    aoi=aoi_shapefile,  # or pass your GeoDataFrame\n",
    "    out_path=lc_clipped_tif,\n",
    "    replace_nodata_with=0,     # convert 255/nodata -> 0\n",
    "    keep_nodata_tag=False      # keep False so 0 is treated as a real class\n",
    ")\n",
    "\n",
    "print(\"Done. Saved to:\\n\\t\", lc_clipped_tif)\n",
    "print(np.unique(filled_arr[~np.isnan(filled_arr)]))  # check valid classes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## update land cover input data to the clipped raster ---------------------------------\n",
    "lc_input = lc_clipped_tif  # Use the clipped raster as input\n",
    "lc_output = lc_input.replace(\".tif\", \"_scenario3_TreeRisk.tif\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### land cover option 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- File Paths ---\n",
    "# lc_input = r\"G:\\Shared drives\\Wellcome Trust Project Data\\1_preprocess\\UrbanCoolingModel\\ESA_WorldCover_10m_2021_v200_Mosaic_Mask_proj.tif\"\n",
    "# lc_output = lc_input.replace(\".tif\", \"_scenario3_TreeRisk.tif\")\n",
    "\n",
    "# tree_cover_code = 1      # ESA WorldCover: Tree cover\n",
    "# after_risk_lc_code = 3   # 3: 'Grassland',\n",
    "\n",
    "# # Define labels\n",
    "# land_cover_labels = {\n",
    "#     1: 'Tree cover',\n",
    "#     2: 'Shrubland',\n",
    "#     3: 'Grassland',\n",
    "#     4: 'Cropland',\n",
    "#     5: 'Built-up',\n",
    "#     6: \"Bare / sparse vegetation\",\n",
    "#     7: \"Snow and ice\",\n",
    "#     8: \"Permanent water bodies\",\n",
    "#     9: \"Herbaceous wetland\",\n",
    "#     10: \"Moss and lichen\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate lc scenario "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated land cover raster saved at: G:\\Shared drives\\Wellcome Trust Project Data\\1_preprocess\\UrbanCoolingModel\\EP_preliminary_tests\\clipped_lulc\\UKECH\\LCM2021_london_clip2aoi_scenario3_TreeRisk.tif\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ensure output folder exists\n",
    "Path(os.path.dirname(lc_output)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Load and filter polygons ---\n",
    "tree_cover_at_risk = gpd.read_file(shp_tree_risk)\n",
    "## Filter the tree cover data with year = 2050, which indicate these trees will be at risk by 2050\n",
    "tree_cover_at_risk = tree_cover_at_risk[tree_cover_at_risk[\"year\"].isin([2050])]\n",
    "tree_cover_at_risk = tree_cover_at_risk[\n",
    "    tree_cover_at_risk.geometry.notna() & ~tree_cover_at_risk.geometry.is_empty\n",
    "]\n",
    "\n",
    "# --- Read raster ---\n",
    "with rasterio.open(lc_input) as src:\n",
    "    meta      = src.meta.copy()\n",
    "    transform = src.transform\n",
    "    lc_crs    = src.crs\n",
    "    src_nodata = src.nodata\n",
    "    arr       = src.read(1)  # 2D array of land cover codes\n",
    "\n",
    "\n",
    "# Reproject polygons to raster CRS if needed\n",
    "if tree_cover_at_risk.crs and (tree_cover_at_risk.crs != lc_crs):\n",
    "    tree_cover_at_risk = tree_cover_at_risk.to_crs(lc_crs)\n",
    "\n",
    "\n",
    "# --- Rasterize risk polygons to a mask (1 = at risk) ---\n",
    "shape_mask = rasterize(\n",
    "    [(geom, 1) for geom in tree_cover_at_risk.geometry],\n",
    "    out_shape=arr.shape,\n",
    "    transform=transform,\n",
    "    fill=0,\n",
    "    dtype=\"uint8\",\n",
    "    # all_touched=True,  # uncomment if you want a slightly more inclusive burn-in\n",
    ")\n",
    "\n",
    "# --- Build masks ---\n",
    "valid_mask = np.ones_like(arr, dtype=bool)\n",
    "if src_nodata is not None:\n",
    "    valid_mask &= (arr != src_nodata)\n",
    "\n",
    "# If LC==0 is background you want to preserve, keep it out of edits:\n",
    "valid_mask &= (arr != 0)\n",
    "\n",
    "# Only modify cells that are both \"at risk\" and currently a tree class\n",
    "tree_mask = np.isin(arr, tree_cover_code)\n",
    "target_mask = (shape_mask == 1) & tree_mask & valid_mask\n",
    "\n",
    "# --- Apply scenario ---\n",
    "remapped = arr.copy()\n",
    "remapped[target_mask] = after_risk_lc_code\n",
    "\n",
    "# --- Save updated raster ---\n",
    "meta_out = meta.copy()\n",
    "# Keep original dtype unless you explicitly want uint8\n",
    "meta_out.update(\n",
    "    dtype=remapped.dtype,\n",
    "    compress=\"lzw\",\n",
    "    nodata=0                # <-- change from src_nodata to 0\n",
    ")\n",
    "\n",
    "with rasterio.open(lc_output, \"w\", **meta_out) as dst:\n",
    "    dst.write(remapped, 1)\n",
    "\n",
    "print(f\"Updated land cover raster saved at: {lc_output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lc change summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Class proportions BEFORE scenario ===\n",
      " class_code                  label   count  proportion  percent     area_m2  area_ha  area_km2\n",
      "        1.0     Deciduous woodland  867944    0.054427    5.443  86794400.0  8679.44     86.79\n",
      "        2.0    Coniferous woodland   65124    0.004084    0.408   6512400.0   651.24      6.51\n",
      "        3.0                 Arable  597008    0.037437    3.744  59700800.0  5970.08     59.70\n",
      "        4.0     Improved Grassland 3559567    0.223214   22.321 355956700.0 35595.67    355.96\n",
      "        5.0      Neutral Grassland      11    0.000001    0.000      1100.0     0.11      0.00\n",
      "        6.0   Calcareous Grassland   50726    0.003181    0.318   5072600.0   507.26      5.07\n",
      "        8.0  Fen, Marsh, and Swamp   49809    0.003123    0.312   4980900.0   498.09      4.98\n",
      "        9.0                Heather    1220    0.000077    0.008    122000.0    12.20      0.12\n",
      "       10.0      Heather grassland    8112    0.000509    0.051    811200.0    81.12      0.81\n",
      "       12.0            Inland Rock   12911    0.000810    0.081   1291100.0   129.11      1.29\n",
      "       13.0              Saltwater   11000    0.000690    0.069   1100000.0   110.00      1.10\n",
      "       14.0             Freshwater  335892    0.021063    2.106  33589200.0  3358.92     33.59\n",
      "       16.0 Supralittoral Sediment     428    0.000027    0.003     42800.0     4.28      0.04\n",
      "       18.0      Littoral Sediment    2067    0.000130    0.013    206700.0    20.67      0.21\n",
      "       19.0              Saltmarsh    9173    0.000575    0.058    917300.0    91.73      0.92\n",
      "       20.0                  Urban 5328123    0.334117   33.412 532812300.0 53281.23    532.81\n",
      "       21.0               Suburban 5047743    0.316535   31.654 504774300.0 50477.43    504.77\n",
      "\n",
      "=== Class proportions AFTER scenario ===\n",
      " class_code                  label   count  proportion  percent     area_m2  area_ha  area_km2\n",
      "        1.0     Deciduous woodland  834005    0.052299    5.230  83400500.0  8340.05     83.40\n",
      "        2.0    Coniferous woodland   62108    0.003895    0.389   6210800.0   621.08      6.21\n",
      "        3.0                 Arable  597008    0.037437    3.744  59700800.0  5970.08     59.70\n",
      "        4.0     Improved Grassland 3596522    0.225532   22.553 359652200.0 35965.22    359.65\n",
      "        5.0      Neutral Grassland      11    0.000001    0.000      1100.0     0.11      0.00\n",
      "        6.0   Calcareous Grassland   50726    0.003181    0.318   5072600.0   507.26      5.07\n",
      "        8.0  Fen, Marsh, and Swamp   49809    0.003123    0.312   4980900.0   498.09      4.98\n",
      "        9.0                Heather    1220    0.000077    0.008    122000.0    12.20      0.12\n",
      "       10.0      Heather grassland    8112    0.000509    0.051    811200.0    81.12      0.81\n",
      "       12.0            Inland Rock   12911    0.000810    0.081   1291100.0   129.11      1.29\n",
      "       13.0              Saltwater   11000    0.000690    0.069   1100000.0   110.00      1.10\n",
      "       14.0             Freshwater  335892    0.021063    2.106  33589200.0  3358.92     33.59\n",
      "       16.0 Supralittoral Sediment     428    0.000027    0.003     42800.0     4.28      0.04\n",
      "       18.0      Littoral Sediment    2067    0.000130    0.013    206700.0    20.67      0.21\n",
      "       19.0              Saltmarsh    9173    0.000575    0.058    917300.0    91.73      0.92\n",
      "       20.0                  Urban 5328123    0.334117   33.412 532812300.0 53281.23    532.81\n",
      "       21.0               Suburban 5047743    0.316535   31.654 504774300.0 50477.43    504.77\n",
      "\n",
      "Saved summaries:\n",
      "- G:\\Shared drives\\Wellcome Trust Project Data\\1_preprocess\\UrbanCoolingModel\\EP_preliminary_tests\\clipped_lulc\\UKECH\\LCM2021_london_clip2aoi_scenario3_TreeRisk_class_summary_BEFORE.csv\n",
      "- G:\\Shared drives\\Wellcome Trust Project Data\\1_preprocess\\UrbanCoolingModel\\EP_preliminary_tests\\clipped_lulc\\UKECH\\LCM2021_london_clip2aoi_scenario3_TreeRisk_class_summary_AFTER.csv\n"
     ]
    }
   ],
   "source": [
    "# load function to use in notebook\n",
    "from function_summarize_lc_classes import summarize_lc_classes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---- Compute pixel area if projected (optional) ----\n",
    "px_area_m2 = None\n",
    "if lc_crs and lc_crs.is_projected:\n",
    "    # rasterio Affine: transform.a = pixel width, transform.e = pixel height (negative)\n",
    "    px_area_m2 = abs(transform.a) * abs(transform.e)\n",
    "\n",
    "# ---- Summaries: BEFORE (original) and AFTER (remapped) ----\n",
    "summary_before = summarize_lc_classes(\n",
    "    arr,\n",
    "    land_cover_labels,\n",
    "    nodata=0,  # Use 0 if it is the nodata value in the original raster\n",
    "    px_area_m2=px_area_m2,\n",
    "    sort_by=\"class\"\n",
    ")\n",
    "\n",
    "# Optional: if you defined land_cover_labels_scenario; otherwise reuse land_cover_labels\n",
    "labels_after = globals().get(\"land_cover_labels_scenario\", land_cover_labels)\n",
    "\n",
    "summary_after = summarize_lc_classes(\n",
    "    remapped,\n",
    "    labels_after,\n",
    "    nodata=0,\n",
    "    px_area_m2=px_area_m2,\n",
    "    sort_by=\"class\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== Class proportions BEFORE scenario ===\")\n",
    "print(summary_before.to_string(index=False))\n",
    "\n",
    "print(\"\\n=== Class proportions AFTER scenario ===\")\n",
    "print(summary_after.to_string(index=False))\n",
    "\n",
    "# ---- Optional: save to CSV ----\n",
    "out_csv_before = lc_output.replace(\".tif\", \"_class_summary_BEFORE.csv\")\n",
    "out_csv_after  = lc_output.replace(\".tif\", \"_class_summary_AFTER.csv\")\n",
    "summary_before.to_csv(out_csv_before, index=False)\n",
    "summary_after.to_csv(out_csv_after, index=False)\n",
    "print(f\"\\nSaved summaries:\\n- {out_csv_before}\\n- {out_csv_after}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: G:\\Shared drives\\Wellcome Trust Project Data\\1_preprocess\\UrbanCoolingModel\\EP_preliminary_tests\\clipped_lulc\\UKECH\\LCM2021_london_clip2aoi_scenario3_TreeRisk_class_summary_CHANGE.csv\n",
      "\n",
      "=== Class proportions changed ===\n",
      " class_code  area_km2_before  area_km2_after  area_km2_change\n",
      "        1.0            86.79           83.40            -3.39\n",
      "        2.0             6.51            6.21            -0.30\n",
      "        3.0            59.70           59.70             0.00\n",
      "        4.0           355.96          359.65             3.69\n",
      "        5.0             0.00            0.00             0.00\n",
      "        6.0             5.07            5.07             0.00\n",
      "        8.0             4.98            4.98             0.00\n",
      "        9.0             0.12            0.12             0.00\n",
      "       10.0             0.81            0.81             0.00\n",
      "       12.0             1.29            1.29             0.00\n",
      "       13.0             1.10            1.10             0.00\n",
      "       14.0            33.59           33.59             0.00\n",
      "       16.0             0.04            0.04             0.00\n",
      "       18.0             0.21            0.21             0.00\n",
      "       19.0             0.92            0.92             0.00\n",
      "       20.0           532.81          532.81             0.00\n",
      "       21.0           504.77          504.77             0.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## compute the change in area_km2 ------------------------------------------------------------------ \n",
    "import pandas as pd\n",
    "\n",
    "# Read\n",
    "df_before = pd.read_csv(out_csv_before)\n",
    "df_after  = pd.read_csv(out_csv_after)\n",
    "\n",
    "# Keep only what we need and rename\n",
    "b = df_before[['class_code', 'area_km2']].rename(columns={'area_km2': 'area_km2_before'})\n",
    "a = df_after [['class_code', 'area_km2']].rename(columns={'area_km2': 'area_km2_after'})\n",
    "\n",
    "# Outer join on class_code\n",
    "merged = b.merge(a, on='class_code', how='outer')\n",
    "\n",
    "# Ensure numeric, then compute change\n",
    "for c in ['area_km2_before', 'area_km2_after']:\n",
    "    merged[c] = pd.to_numeric(merged[c], errors='coerce')\n",
    "\n",
    "merged['area_km2_change'] = merged['area_km2_after'].fillna(0) - merged['area_km2_before'].fillna(0)\n",
    "\n",
    "# Save\n",
    "out_csv_change = lc_output.replace(\".tif\", \"_class_summary_CHANGE.csv\")\n",
    "merged.to_csv(out_csv_change, index=False)\n",
    "print(f\"Saved: {out_csv_change}\")\n",
    "\n",
    "print(\"\\n=== Class proportions changed ===\")\n",
    "print(merged.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
