{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d707702f",
   "metadata": {},
   "source": [
    "## Package and dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7fc1d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Environment Info ===\n",
      "Python version : 3.11.13\n",
      "Python env     : c:\\Users\\pc\\.conda\\envs\\geo_env\\python.exe\n",
      "Platform       : Windows-10-10.0.22631-SP0\n",
      "numpy          : 1.26.4\n",
      "pandas         : 2.3.2\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "# %pip install pyarrow\n",
    "# %pip install --upgrade pandas\n",
    "# %pip install xlsxwriter\n",
    "\n",
    "\n",
    "import pyarrow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import sys, platform\n",
    "print(\"=== Environment Info ===\")\n",
    "print(f\"Python version : {sys.version.split()[0]}\")\n",
    "print(f\"Python env     : {sys.executable}\")\n",
    "print(f\"Platform       : {platform.platform()}\")\n",
    "print(f\"numpy          : {np.__version__}\")\n",
    "print(f\"pandas         : {pd.__version__}\")\n",
    "print(\"========================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb614768",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the file path\n",
    "folder = Path(r\"E:\\London\\colouringbritain\\data-extract-2025-09-01\")\n",
    "csv_file = folder / \"building_attributes.csv\"\n",
    "\n",
    "# Subfolder for cached files\n",
    "cache_dir = folder / \"cache\"\n",
    "cache_dir.mkdir(exist_ok=True)   # create if not exists\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fc2278",
   "metadata": {},
   "source": [
    "## Load building attributes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527e06a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Load CSV --- 1. very slow and cause errors\n",
    "# df_building = pd.read_csv(\n",
    "#     csv_file, \n",
    "#     engine=\"pyarrow\",\n",
    "#     dtype_backend=\"pyarrow\", # arrow dtypes (lower memory usage)\n",
    "# )\n",
    "\n",
    "# # Print first 5 rows\n",
    "# print(df_building.head())\n",
    "\n",
    "\n",
    "# 2. 2nd way to load csv data \n",
    "import pyarrow.csv as pv\n",
    "import pyarrow.parquet as pq\n",
    "# 1) Read with Arrow (tolerates newlines-in-values)\n",
    "table = pv.read_csv(\n",
    "    csv_file,\n",
    "    read_options=pv.ReadOptions(block_size=1<<26),  # 64MB blocks (tweak if needed)\n",
    "    parse_options=pv.ParseOptions(newlines_in_values=True)\n",
    ")\n",
    "\n",
    "# 2) Save once to Parquet for instant reloads later\n",
    "parquet_path = cache_dir / \"building_attributes.parquet\"\n",
    "pq.write_table(table, parquet_path, compression=\"snappy\")\n",
    "\n",
    "# 3) (optional) Convert to pandas right now\n",
    "df_building = table.to_pandas(types_mapper=pd.ArrowDtype)\n",
    "print(df_building.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b188efcc",
   "metadata": {},
   "source": [
    "Save as Pickle format for fast reload\n",
    "\n",
    "✅ Pros: very fast, preserves dtypes exactly.\n",
    "⚠️ Cons: Python-specific, not portable across languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a553005",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Pickle file path\n",
    "# pkl_file = cache_dir / \"building_attributes.pkl\"\n",
    "\n",
    "# # Load CSV (if pickle not already saved)\n",
    "# if not pkl_file.exists():\n",
    "#     print(\"Pickle not found, reading CSV and saving as pickle...\")\n",
    "#     df_building = pd.read_csv(csv_file)\n",
    "#     df_building.to_pickle(pkl_file)\n",
    "# else:\n",
    "#     print(\"Pickle found, loading directly...\")\n",
    "#     df_building = pd.read_pickle(pkl_file)\n",
    "\n",
    "# print(df_building.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Fast reload\n",
    "# parquet_path = cache_dir / \"building_attributes.parquet\"\n",
    "# df_building = pd.read_parquet(parquet_path, engine=\"pyarrow\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feb1cf0",
   "metadata": {},
   "source": [
    "### Data summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0042129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_building.columns.tolist())\n",
    "\n",
    "\n",
    "\n",
    "# for col in df_building.columns:\n",
    "#     print(f\"\\n--- {col} ---\")\n",
    "#     try:\n",
    "#         # value_counts is efficient and sorts by frequency\n",
    "#         vals = df_building[col].value_counts(dropna=False).head(10)\n",
    "#         print(vals)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Could not process column {col}: {e}\")\n",
    "\n",
    "# # Basic info: rows, columns, dtypes, memory\n",
    "# print(\"\\n=== DataFrame Info ===\")\n",
    "# print(df_building.info())\n",
    "\n",
    "# # Summary stats for numeric columns\n",
    "# print(\"\\n=== Summary Statistics (numeric) ===\")\n",
    "# print(df_building.describe().T)\n",
    "\n",
    "# # Summary stats for categorical/text columns\n",
    "# print(\"\\n=== Summary (categorical) ===\")\n",
    "# print(df_building.describe(include=['object', 'category']).T)\n",
    "\n",
    "# # Count missing values per column\n",
    "# print(\"\\n=== Missing Values ===\")\n",
    "# print(df_building.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9817e5f2",
   "metadata": {},
   "source": [
    "### data summary - columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cca0de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path for output\n",
    "out_xlsx = cache_dir / \"building_attributes_summary.xlsx\"\n",
    "\n",
    "# Create a dict to hold results\n",
    "summary = {}\n",
    "\n",
    "for col in df_building.columns:\n",
    "    try:\n",
    "        vals = df_building[col].value_counts(dropna=False).head(10)\n",
    "        # convert to DataFrame for nice export\n",
    "        summary[col] = vals.reset_index().rename(columns={\"index\": col, col: \"count\"})\n",
    "    except Exception as e:\n",
    "        summary[col] = pd.DataFrame({\"error\": [str(e)]})\n",
    "\n",
    "# Export to Excel (each column in a separate sheet)\n",
    "with pd.ExcelWriter(out_xlsx, engine=\"xlsxwriter\") as writer:\n",
    "    for col, df in summary.items():\n",
    "        # Excel sheet names must be ≤31 chars\n",
    "        sheet_name = str(col)[:31]\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(f\"Summary exported to {out_xlsx}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384a7554",
   "metadata": {},
   "source": [
    "### select columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbf3616",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_selected = [\n",
    "    'building_id', 'ref_toid', 'ref_osm_id',      \n",
    "    #  'revision_id', \n",
    "    'location_name', \n",
    "    #  'location_name_link', 'location_number', 'location_street', 'location_line_two', \n",
    "    'location_town', 'location_postcode', \n",
    "    #  'location_address_source', 'location_address_links', \n",
    "\n",
    "    'location_latitude', 'location_longitude', \n",
    "    'current_landuse_group', 'current_landuse_order', 'building_attachment_form', \n",
    "    'date_change_building_use', 'date_year', \n",
    "\n",
    "    #  'date_lower', 'date_upper', 'date_source', 'date_source_detail', 'date_link', \n",
    "    #  'facade_year', 'facade_upper', 'facade_lower', 'facade_source', 'facade_source_detail', \n",
    "    'size_storeys_attic', 'size_storeys_core', 'size_storeys_basement', \n",
    "    #  'size_storeys_source_type', 'size_storeys_source_links', \n",
    "    'size_height_apex', \n",
    "    #  'size_height_apex_source_type', 'size_height_apex_source_links', 'size_height_eaves', 'size_height_eaves_source_type', 'size_height_eaves_source_links', \n",
    "    'size_floor_area_ground', 'size_floor_area_total', \n",
    "    #  'size_floor_area_source_type', 'size_floor_area_source_links', 'size_width_frontage', \n",
    "    'sust_breeam_rating', 'sust_dec', 'sust_retrofit_date', 'construction_core_material', 'construction_secondary_materials', 'construction_roof_covering', \n",
    "    #  'planning_portal_link', 'planning_in_conservation_area_id', 'planning_in_conservation_area_url', 'planning_conservation_area_name', 'planning_in_list', \n",
    "    #  'planning_list_id', 'planning_list_cat', 'planning_list_grade', 'planning_heritage_at_risk_url', 'planning_world_list_id', 'planning_glher_url', \n",
    "    #  'planning_in_apa_url', 'planning_local_list_url', 'planning_historic_area_assessment_url', \n",
    "    'is_domestic', \n",
    "    #  'is_domestic_source', 'is_domestic_links', \n",
    "    #  'community_type_worth_keeping_total', 'likes_total', 'survival_status', 'survival_source', 'survival_source_links', 'disaster_type', 'disaster_severity', \n",
    "    #  'disaster_assessment_method', 'disaster_source_link', 'disaster_start_date', 'disaster_end_date', 'size_far_ratio', 'size_far_ratio_source_type', \n",
    "    #  'size_far_ratio_source_links', 'size_plot_area_total', 'size_plot_area_total_source_type', 'size_plot_area_total_source_links', 'size_parcel_geometry', \n",
    "    #  'size_parcel_geometry_source_type', 'size_parcel_geometry_source_links', \n",
    "\n",
    "    # 'context_front_garden', 'context_back_garden', 'context_flats_garden', \n",
    "\n",
    "    #  'context_garden_source_type', 'context_garden_source_type', 'context_street_width', 'context_street_width_source_type', 'context_street_width_source_links', \n",
    "    #  'context_pavement_width', 'context_pavement_width_source_type', 'context_pavement_width_source_links', 'context_street_geometry', \n",
    "    #  'context_street_geometry_source_type', 'context_street_geometry_source_links', \n",
    "    #  'context_green_space_distance_source_links', \n",
    "\n",
    "    # 'context_green_space_distance', 'context_tree_distance', \n",
    "\n",
    "    # 'context_tree_distance_source_type', 'context_tree_distance_source_links', \n",
    "    # 'age_cladding_date', 'age_cladding_date_source_type', 'age_cladding_date_source_links', 'age_extension_date', 'age_extension_date_source_type', \n",
    "    # 'age_extension_date_source_links', \n",
    "    # 'age_retrofit_date', \n",
    "    #  'age_retrofit_date_source_type', 'age_retrofit_date_source_links'\n",
    "    ]\n",
    "\n",
    "\n",
    "df_building_subset = df_building[cols_selected]\n",
    "\n",
    "print(df_building_subset.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a61f55",
   "metadata": {},
   "source": [
    "### save subset data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8705d7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Save as CSV\n",
    "# out_csv =  cache_dir / \"building_attributes_subset.csv\"\n",
    "# df_building_subset.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "# print(f\"Subset saved to: {out_csv}\")\n",
    "\n",
    "# import pyarrow.csv as pv\n",
    "# import pyarrow.parquet as pq\n",
    "# out_csv =  cache_dir / \"building_attributes_subset.csv\"\n",
    "# df_building_subset = pv.read_csv(\n",
    "#     out_csv,\n",
    "#     read_options=pv.ReadOptions(block_size=1<<26),  # 64MB blocks (tweak if needed)\n",
    "#     parse_options=pv.ParseOptions(newlines_in_values=True)\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "## 2. Save as Parquet (fast reload for analysis)\n",
    "# out_parquet = cache_dir / \"building_attributes_subset.parquet\"\n",
    "# df_building_subset.to_parquet(out_parquet, engine=\"pyarrow\", compression=\"snappy\", index=False)\n",
    "# print(f\"Saved Parquet: {out_parquet}\")\n",
    "\n",
    "\n",
    "\n",
    "## 3. Save as Feather (optional, even faster reload if only in Python)\n",
    "# import pyarrow.feather as feather\n",
    "# out_feather = cache_dir / \"building_attributes_subset.feather\"\n",
    "# feather.write_feather(df_building_subset, out_feather)\n",
    "# print(f\"Saved Feather: {out_feather}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad706b8",
   "metadata": {},
   "source": [
    "### reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679f8ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.feather as feather\n",
    "\n",
    "out_feather = cache_dir / \"building_attributes_subset.feather\"\n",
    "building_points = feather.read_feather(out_feather)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366cd0aa",
   "metadata": {},
   "source": [
    "## Load building footprint data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297ab562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import geopandas as gpd\n",
    "\n",
    "# # Path to your shapefile (make sure to use raw string r\"\" or double slashes \\\\ for Windows paths)\n",
    "# shapefile_path = r\"E:\\London\\colouringbritain\\London building footprints_2025-09-11\\buildings.shp\"\n",
    "\n",
    "# # Load into GeoDataFrame\n",
    "# gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "# print(gdf.head())\n",
    "# print(gdf.crs)  # coordinate reference system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae0c3fb",
   "metadata": {},
   "source": [
    "### subset data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067f08e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop specific columns\n",
    "# buildings_gdf = gdf.drop(columns=[\"layer\", \"path\"])\n",
    "\n",
    "\n",
    "# # # Save cleaned GeoDataFrame to a new file (GeoPackage format) \n",
    "# # out_gpkg = cache_dir / \"buildings_clean.gpkg\"\n",
    "# # buildings_gdf.to_file(out_gpkg, driver=\"GPKG\")\n",
    "\n",
    "\n",
    "# # Save cleaned GeoDataFrame to Parquet format -- the fastest option if you’ll only use the data in Python.\n",
    "# out_parquet = cache_dir / \"buildings_clean.parquet\"\n",
    "# buildings_gdf.to_parquet(out_parquet, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22e4c33",
   "metadata": {},
   "source": [
    "### reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b299c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "buildings_parquet = cache_dir / \"buildings_clean.parquet\"\n",
    "\n",
    "# Reload instantly\n",
    "buildings_gdf = gpd.read_parquet(buildings_parquet)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
