{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d707702f",
   "metadata": {},
   "source": [
    "## Package and dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7fc1d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Environment Info ===\n",
      "Python version : 3.11.13\n",
      "Python env     : c:\\Users\\pc\\.conda\\envs\\geo_env\\python.exe\n",
      "Platform       : Windows-10-10.0.22631-SP0\n",
      "numpy          : 1.26.4\n",
      "pandas         : 2.3.2\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "# %pip install pyarrow\n",
    "# %pip install --upgrade pandas\n",
    "# %pip install xlsxwriter\n",
    "\n",
    "\n",
    "import pyarrow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import sys, platform\n",
    "print(\"=== Environment Info ===\")\n",
    "print(f\"Python version : {sys.version.split()[0]}\")\n",
    "print(f\"Python env     : {sys.executable}\")\n",
    "print(f\"Platform       : {platform.platform()}\")\n",
    "print(f\"numpy          : {np.__version__}\")\n",
    "print(f\"pandas         : {pd.__version__}\")\n",
    "print(\"========================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb614768",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the file path\n",
    "folder = Path(r\"E:\\London\\colouringbritain\\data-extract-2025-09-01\")\n",
    "csv_file = folder / \"building_attributes.csv\"\n",
    "\n",
    "# Subfolder for cached files\n",
    "cache_dir = folder / \"cache\"\n",
    "cache_dir.mkdir(exist_ok=True)   # create if not exists\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fc2278",
   "metadata": {},
   "source": [
    "## Load building attributes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527e06a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Load CSV --- 1. very slow and cause errors\n",
    "# df_building = pd.read_csv(\n",
    "#     csv_file, \n",
    "#     engine=\"pyarrow\",\n",
    "#     dtype_backend=\"pyarrow\", # arrow dtypes (lower memory usage)\n",
    "# )\n",
    "\n",
    "# # Print first 5 rows\n",
    "# print(df_building.head())\n",
    "\n",
    "\n",
    "# 2. 2nd way to load csv data \n",
    "import pyarrow.csv as pv\n",
    "import pyarrow.parquet as pq\n",
    "# 1) Read with Arrow (tolerates newlines-in-values)\n",
    "table = pv.read_csv(\n",
    "    csv_file,\n",
    "    read_options=pv.ReadOptions(block_size=1<<26),  # 64MB blocks (tweak if needed)\n",
    "    parse_options=pv.ParseOptions(newlines_in_values=True)\n",
    ")\n",
    "\n",
    "# 2) Save once to Parquet for instant reloads later\n",
    "parquet_path = cache_dir / \"building_attributes.parquet\"\n",
    "pq.write_table(table, parquet_path, compression=\"snappy\")\n",
    "\n",
    "# 3) (optional) Convert to pandas right now\n",
    "df_building = table.to_pandas(types_mapper=pd.ArrowDtype)\n",
    "print(df_building.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b188efcc",
   "metadata": {},
   "source": [
    "Save as Pickle format for fast reload\n",
    "\n",
    "✅ Pros: very fast, preserves dtypes exactly.\n",
    "⚠️ Cons: Python-specific, not portable across languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a553005",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Pickle file path\n",
    "# pkl_file = cache_dir / \"building_attributes.pkl\"\n",
    "\n",
    "# # Load CSV (if pickle not already saved)\n",
    "# if not pkl_file.exists():\n",
    "#     print(\"Pickle not found, reading CSV and saving as pickle...\")\n",
    "#     df_building = pd.read_csv(csv_file)\n",
    "#     df_building.to_pickle(pkl_file)\n",
    "# else:\n",
    "#     print(\"Pickle found, loading directly...\")\n",
    "#     df_building = pd.read_pickle(pkl_file)\n",
    "\n",
    "# print(df_building.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Fast reload\n",
    "# parquet_path = cache_dir / \"building_attributes.parquet\"\n",
    "# df_building = pd.read_parquet(parquet_path, engine=\"pyarrow\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feb1cf0",
   "metadata": {},
   "source": [
    "### Data summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0042129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_building.columns.tolist())\n",
    "\n",
    "\n",
    "\n",
    "# for col in df_building.columns:\n",
    "#     print(f\"\\n--- {col} ---\")\n",
    "#     try:\n",
    "#         # value_counts is efficient and sorts by frequency\n",
    "#         vals = df_building[col].value_counts(dropna=False).head(10)\n",
    "#         print(vals)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Could not process column {col}: {e}\")\n",
    "\n",
    "# # Basic info: rows, columns, dtypes, memory\n",
    "# print(\"\\n=== DataFrame Info ===\")\n",
    "# print(df_building.info())\n",
    "\n",
    "# # Summary stats for numeric columns\n",
    "# print(\"\\n=== Summary Statistics (numeric) ===\")\n",
    "# print(df_building.describe().T)\n",
    "\n",
    "# # Summary stats for categorical/text columns\n",
    "# print(\"\\n=== Summary (categorical) ===\")\n",
    "# print(df_building.describe(include=['object', 'category']).T)\n",
    "\n",
    "# # Count missing values per column\n",
    "# print(\"\\n=== Missing Values ===\")\n",
    "# print(df_building.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9817e5f2",
   "metadata": {},
   "source": [
    "### data summary - columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cca0de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path for output\n",
    "out_xlsx = cache_dir / \"building_attributes_summary.xlsx\"\n",
    "\n",
    "# Create a dict to hold results\n",
    "summary = {}\n",
    "\n",
    "for col in df_building.columns:\n",
    "    try:\n",
    "        vals = df_building[col].value_counts(dropna=False).head(10)\n",
    "        # convert to DataFrame for nice export\n",
    "        summary[col] = vals.reset_index().rename(columns={\"index\": col, col: \"count\"})\n",
    "    except Exception as e:\n",
    "        summary[col] = pd.DataFrame({\"error\": [str(e)]})\n",
    "\n",
    "# Export to Excel (each column in a separate sheet)\n",
    "with pd.ExcelWriter(out_xlsx, engine=\"xlsxwriter\") as writer:\n",
    "    for col, df in summary.items():\n",
    "        # Excel sheet names must be ≤31 chars\n",
    "        sheet_name = str(col)[:31]\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(f\"Summary exported to {out_xlsx}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384a7554",
   "metadata": {},
   "source": [
    "### select columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbf3616",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_selected = [\n",
    "    'building_id', 'ref_toid', 'ref_osm_id',      \n",
    "    #  'revision_id', \n",
    "    'location_name', \n",
    "    #  'location_name_link', 'location_number', 'location_street', 'location_line_two', \n",
    "    'location_town', 'location_postcode', \n",
    "    #  'location_address_source', 'location_address_links', \n",
    "\n",
    "    'location_latitude', 'location_longitude', \n",
    "    'current_landuse_group', 'current_landuse_order', 'building_attachment_form', \n",
    "    'date_change_building_use', 'date_year', \n",
    "\n",
    "    #  'date_lower', 'date_upper', 'date_source', 'date_source_detail', 'date_link', \n",
    "    #  'facade_year', 'facade_upper', 'facade_lower', 'facade_source', 'facade_source_detail', \n",
    "    'size_storeys_attic', 'size_storeys_core', 'size_storeys_basement', \n",
    "    #  'size_storeys_source_type', 'size_storeys_source_links', \n",
    "    'size_height_apex', \n",
    "    #  'size_height_apex_source_type', 'size_height_apex_source_links', 'size_height_eaves', 'size_height_eaves_source_type', 'size_height_eaves_source_links', \n",
    "    'size_floor_area_ground', 'size_floor_area_total', \n",
    "    #  'size_floor_area_source_type', 'size_floor_area_source_links', 'size_width_frontage', \n",
    "    'sust_breeam_rating', 'sust_dec', 'sust_retrofit_date', 'construction_core_material', 'construction_secondary_materials', 'construction_roof_covering', \n",
    "    #  'planning_portal_link', 'planning_in_conservation_area_id', 'planning_in_conservation_area_url', 'planning_conservation_area_name', 'planning_in_list', \n",
    "    #  'planning_list_id', 'planning_list_cat', 'planning_list_grade', 'planning_heritage_at_risk_url', 'planning_world_list_id', 'planning_glher_url', \n",
    "    #  'planning_in_apa_url', 'planning_local_list_url', 'planning_historic_area_assessment_url', \n",
    "    'is_domestic', \n",
    "    #  'is_domestic_source', 'is_domestic_links', \n",
    "    #  'community_type_worth_keeping_total', 'likes_total', 'survival_status', 'survival_source', 'survival_source_links', 'disaster_type', 'disaster_severity', \n",
    "    #  'disaster_assessment_method', 'disaster_source_link', 'disaster_start_date', 'disaster_end_date', 'size_far_ratio', 'size_far_ratio_source_type', \n",
    "    #  'size_far_ratio_source_links', 'size_plot_area_total', 'size_plot_area_total_source_type', 'size_plot_area_total_source_links', 'size_parcel_geometry', \n",
    "    #  'size_parcel_geometry_source_type', 'size_parcel_geometry_source_links', \n",
    "\n",
    "    # 'context_front_garden', 'context_back_garden', 'context_flats_garden', \n",
    "\n",
    "    #  'context_garden_source_type', 'context_garden_source_type', 'context_street_width', 'context_street_width_source_type', 'context_street_width_source_links', \n",
    "    #  'context_pavement_width', 'context_pavement_width_source_type', 'context_pavement_width_source_links', 'context_street_geometry', \n",
    "    #  'context_street_geometry_source_type', 'context_street_geometry_source_links', \n",
    "    #  'context_green_space_distance_source_links', \n",
    "\n",
    "    # 'context_green_space_distance', 'context_tree_distance', \n",
    "\n",
    "    # 'context_tree_distance_source_type', 'context_tree_distance_source_links', \n",
    "    # 'age_cladding_date', 'age_cladding_date_source_type', 'age_cladding_date_source_links', 'age_extension_date', 'age_extension_date_source_type', \n",
    "    # 'age_extension_date_source_links', \n",
    "    # 'age_retrofit_date', \n",
    "    #  'age_retrofit_date_source_type', 'age_retrofit_date_source_links'\n",
    "    ]\n",
    "\n",
    "\n",
    "df_building_subset = df_building[cols_selected]\n",
    "\n",
    "print(df_building_subset.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a61f55",
   "metadata": {},
   "source": [
    "### save subset data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8705d7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Save as CSV\n",
    "# out_csv =  cache_dir / \"building_attributes_subset.csv\"\n",
    "# df_building_subset.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "# print(f\"Subset saved to: {out_csv}\")\n",
    "\n",
    "# import pyarrow.csv as pv\n",
    "# import pyarrow.parquet as pq\n",
    "# out_csv =  cache_dir / \"building_attributes_subset.csv\"\n",
    "# df_building_subset = pv.read_csv(\n",
    "#     out_csv,\n",
    "#     read_options=pv.ReadOptions(block_size=1<<26),  # 64MB blocks (tweak if needed)\n",
    "#     parse_options=pv.ParseOptions(newlines_in_values=True)\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "## 2. Save as Parquet (fast reload for analysis)\n",
    "# out_parquet = cache_dir / \"building_attributes_subset.parquet\"\n",
    "# df_building_subset.to_parquet(out_parquet, engine=\"pyarrow\", compression=\"snappy\", index=False)\n",
    "# print(f\"Saved Parquet: {out_parquet}\")\n",
    "\n",
    "\n",
    "\n",
    "## 3. Save as Feather (optional, even faster reload if only in Python)\n",
    "# import pyarrow.feather as feather\n",
    "# out_feather = cache_dir / \"building_attributes_subset.feather\"\n",
    "# feather.write_feather(df_building_subset, out_feather)\n",
    "# print(f\"Saved Feather: {out_feather}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad706b8",
   "metadata": {},
   "source": [
    "### reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679f8ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.feather as feather\n",
    "\n",
    "out_feather = cache_dir / \"building_attributes_subset.feather\"\n",
    "building_points = feather.read_feather(out_feather)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366cd0aa",
   "metadata": {},
   "source": [
    "## Load building footprint data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297ab562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import geopandas as gpd\n",
    "\n",
    "# # Path to your shapefile (make sure to use raw string r\"\" or double slashes \\\\ for Windows paths)\n",
    "# shapefile_path = r\"E:\\London\\colouringbritain\\London building footprints_2025-09-11\\buildings.shp\"\n",
    "\n",
    "# # Load into GeoDataFrame\n",
    "# gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "# print(gdf.head())\n",
    "# print(gdf.crs)  # coordinate reference system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae0c3fb",
   "metadata": {},
   "source": [
    "### subset data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067f08e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop specific columns\n",
    "# buildings_gdf = gdf.drop(columns=[\"layer\", \"path\"])\n",
    "\n",
    "\n",
    "# # # Save cleaned GeoDataFrame to a new file (GeoPackage format) \n",
    "# # out_gpkg = cache_dir / \"buildings_clean.gpkg\"\n",
    "# # buildings_gdf.to_file(out_gpkg, driver=\"GPKG\")\n",
    "\n",
    "\n",
    "# # Save cleaned GeoDataFrame to Parquet format -- the fastest option if you’ll only use the data in Python.\n",
    "# out_parquet = cache_dir / \"buildings_clean.parquet\"\n",
    "# buildings_gdf.to_parquet(out_parquet, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22e4c33",
   "metadata": {},
   "source": [
    "### reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b299c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "buildings_parquet = cache_dir / \"buildings_clean.parquet\"\n",
    "\n",
    "# Reload instantly\n",
    "buildings_gdf = gpd.read_parquet(buildings_parquet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca807cd1",
   "metadata": {},
   "source": [
    "## Merge building polygons with attributes data from points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee0ee99",
   "metadata": {},
   "source": [
    "### reload building attributes - points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e97e316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.feather as feather\n",
    "\n",
    "# cache_dir = Path(r\"C:\\Users\\yingjiel\\data\\colouringbritain\")  # --> when using SDSS remote desktop\n",
    "\n",
    "out_feather = cache_dir / \"building_attributes_subset.feather\"\n",
    "df_building_subset = feather.read_feather(out_feather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3be2b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['building_id', 'ref_toid', 'ref_osm_id', 'location_name', 'location_town', 'location_postcode', 'location_latitude', 'location_longitude', 'current_landuse_group', 'current_landuse_order', 'building_attachment_form', 'date_change_building_use', 'date_year', 'size_storeys_attic', 'size_storeys_core', 'size_storeys_basement', 'size_height_apex', 'size_floor_area_ground', 'size_floor_area_total', 'sust_breeam_rating', 'sust_dec', 'sust_retrofit_date', 'construction_core_material', 'construction_secondary_materials', 'construction_roof_covering', 'is_domestic', 'context_front_garden', 'context_back_garden', 'context_flats_garden', 'context_green_space_distance', 'context_green_space_distance', 'context_tree_distance', 'age_retrofit_date']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# df_building_subset.head()\n",
    "print(df_building_subset.columns.tolist())\n",
    "\n",
    "# # Find duplicates based on building_id\n",
    "# duplicates = df_building_subset[df_building_subset.duplicated(subset=\"building_id\", keep=False)]\n",
    "# # Optionally, sort for easier inspection\n",
    "# duplicates = duplicates.sort_values(by=\"building_id\")\n",
    "# print(duplicates)\n",
    "\n",
    "cols_pts_selected = ['building_id', 'location_postcode', \n",
    "                     'location_latitude', 'location_longitude', \n",
    "                     'current_landuse_group', 'current_landuse_order',\n",
    "                     'sust_breeam_rating', 'sust_dec', 'sust_retrofit_date',\n",
    "                     ]\n",
    "\n",
    "building_points = df_building_subset[cols_pts_selected].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f60211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10180876, 9)\n",
      "(10180876, 9)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Drop duplicate rows across all columns\n",
    "building_points_nodup = building_points.drop_duplicates()               # step 2: drop duplicate rows\n",
    "\n",
    "print(building_points.shape)\n",
    "print(building_points_nodup.shape) # --> the same shape, so no duplicate rows\n",
    "\n",
    "\n",
    "# Auto-rename duplicated columns (if any) by appending _1, _2, etc.\n",
    "def dedup_columns(df):\n",
    "    seen = {}\n",
    "    new_cols = []\n",
    "    for col in df.columns:\n",
    "        if col not in seen:\n",
    "            seen[col] = 0\n",
    "            new_cols.append(col)\n",
    "        else:\n",
    "            seen[col] += 1\n",
    "            new_cols.append(f\"{col}_{seen[col]}\")\n",
    "    df.columns = new_cols\n",
    "    return df\n",
    "\n",
    "building_points = dedup_columns(building_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba45f2b",
   "metadata": {},
   "source": [
    "### convert to point shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d968a278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make points GeoDataFrame from lat/lon\n",
    "building_points_df = building_points.dropna(subset=[\"location_latitude\", \"location_longitude\"]).copy()\n",
    "\n",
    "building_pts = gpd.GeoDataFrame(\n",
    "    building_points_df,\n",
    "    geometry=gpd.points_from_xy(\n",
    "        x=building_points_df[\"location_longitude\"],\n",
    "        y=building_points_df[\"location_latitude\"],\n",
    "    ),\n",
    "    crs=\"EPSG:4326\",  # WGS84  # <<< CHANGED (move crs to GeoDataFrame for clarity)\n",
    ")\n",
    "\n",
    "\n",
    "# # Save as GeoPackage -- will take 13 mins\n",
    "# gpkg_path = cache_dir / \"building_pts.gpkg\"\n",
    "# building_pts.to_file(gpkg_path, driver=\"GPKG\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56884594",
   "metadata": {},
   "source": [
    "### reload building polygon  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17ed8072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "buildings_parquet = cache_dir / \"buildings_clean.feather\"\n",
    "\n",
    "# Reload instantly\n",
    "building_poly = gpd.read_feather(buildings_parquet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10d4a9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fid', 'area', 'perimeter', 'geometry']\n",
      "(2223481, 4)\n"
     ]
    }
   ],
   "source": [
    "# print(building_poly.head())\n",
    "print(building_poly.columns.tolist())\n",
    "\n",
    "print(building_poly.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0853cdbf",
   "metadata": {},
   "source": [
    "### spatial join \n",
    "\n",
    "It might be poosible one polygon contains multiple points -- need to find a way to address this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8c9b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "# 2) Match CRS to building_poly (whatever it is)\n",
    "if building_pts.crs != building_poly.crs:\n",
    "    building_pts = building_pts.to_crs(building_poly.crs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e159efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# 3.2) Pick the right predicate:\n",
    "#    - \"contains\" : point must be strictly inside polygon (boundary points excluded)\n",
    "#    - \"covers\"   : includes points on polygon boundary (often safer)\n",
    "predicate = \"covers\"\n",
    "\n",
    "bld_with_pts = gpd.sjoin(\n",
    "    buildings_gdf,   # LEFT: keep all buildings\n",
    "    building_pts,         # RIGHT: bring point attrs in where they fall inside\n",
    "    how=\"left\",\n",
    "    predicate=predicate,\n",
    "    lsuffix=\"bld\",\n",
    "    rsuffix=\"pt\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cda2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(bld_with_pts.columns.tolist())\n",
    "# print(building_pts.columns.tolist())\n",
    "\n",
    "# Step 2: Identify points that didn't match any building\n",
    "matched_pts = bld_with_pts[\"building_id\"].dropna().unique()\n",
    "pts_unmatched = building_pts.loc[~building_pts[\"building_id\"].isin(matched_pts)].copy()\n",
    "print(f\"Unmatched points: {len(pts_unmatched)}\")\n",
    "\n",
    "# Step 3: Fallback nearest join for unmatched points\n",
    "if not pts_unmatched.empty:\n",
    "    MAX_DIST = 5  # adjust to your CRS units (e.g., meters if projected)\n",
    "    nearest = gpd.sjoin_nearest(\n",
    "        building_poly,     # keep buildings as LEFT again\n",
    "        pts_unmatched,     # bring in the unmatched points\n",
    "        how=\"left\",\n",
    "        max_distance=MAX_DIST,\n",
    "        lsuffix=\"bld\",\n",
    "        rsuffix=\"pt\",\n",
    "        distance_col=\"fallback_dist_m\",\n",
    "    )\n",
    "\n",
    "    # Step 4: Merge the fallback results into the main result\n",
    "    # Prefer the original matches, then fill in with fallback\n",
    "    def dedup_columns(df):\n",
    "        # keep first occurrence of each name\n",
    "        return df.loc[:, ~df.columns.duplicated()]\n",
    "    bld_with_pts = dedup_columns(bld_with_pts)\n",
    "    nearest      = dedup_columns(nearest)\n",
    "\n",
    "    bld_with_pts = pd.concat([bld_with_pts, nearest], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21c21ac",
   "metadata": {},
   "source": [
    "### save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6ce2f8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5065340, 38)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(bld_with_pts.shape)\n",
    "\n",
    "# # 1. Save as Feather (very fast reload in Python, good for local caching) -- will take 12 seconds\n",
    "# feather_path = cache_dir / \"bld_with_pts.feather\"\n",
    "# bld_with_pts.to_feather(feather_path)\n",
    "# print(bld_with_pts.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2. Save as GeoPackage (portable, works with GIS software like QGIS/ArcGIS) -- will take 13 mins\n",
    "gpkg_path = cache_dir / \"bld_with_pts.gpkg\"\n",
    "\n",
    "# 1) avoid writing the pandas index as a field\n",
    "# 2) rename any user column named 'fid' (and similar reserved names)\n",
    "reserved = {\"fid\", \"ogc_fid\", \"rowid\"}  # common reserved PK names in GPKG\n",
    "rename_map = {c: f\"{c}_attr\" for c in bld_with_pts.columns if c.lower() in reserved}\n",
    "bld_with_pts_safe = bld_with_pts.rename(columns=rename_map)\n",
    "\n",
    "# save without the index\n",
    "bld_with_pts_safe.to_file(gpkg_path, driver=\"GPKG\", index=False)\n",
    "# print(bld_with_pts_safe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfb270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def first_nonnull(s: pd.Series):\n",
    "    return next((v for v in s if pd.notna(v)), pd.NA)\n",
    "\n",
    "def mode_safe(s: pd.Series):\n",
    "    # returns most frequent non-null value; if tie, pick first by appearance\n",
    "    s = s.dropna()\n",
    "    if s.empty:\n",
    "        return pd.NA\n",
    "    counts = s.value_counts()\n",
    "    top = counts.index[counts.eq(counts.max())]\n",
    "    # preserve original order for tie-break\n",
    "    for v in s:\n",
    "        if v in set(top):\n",
    "            return v\n",
    "\n",
    "def unique_list(s: pd.Series, max_items=10):\n",
    "    # unique preserving order; cap list length\n",
    "    seen, out = set(), []\n",
    "    for v in s:\n",
    "        if pd.notna(v) and v not in seen:\n",
    "            seen.add(v)\n",
    "            out.append(v)\n",
    "        if len(out) >= max_items:\n",
    "            break\n",
    "    return out\n",
    "\n",
    "def concat_unique(s: pd.Series, sep=\"; \", max_items=10):\n",
    "    ul = unique_list(s, max_items=max_items)\n",
    "    return sep.join(map(str, ul)) if ul else \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7668c8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 5: (Optional) Aggregate to one row per building\n",
    "bld_summary = (\n",
    "    bld_with_pts\n",
    "    .groupby(\"building_id\")\n",
    "    .agg(\n",
    "        n_pts=(\"fid\", \"count\"),\n",
    "        avg_attr=(\"some_attr\", \"mean\"),\n",
    "        min_fallback_dist=(\"fallback_dist_m\", \"min\"),\n",
    "    )\n",
    "    .reset_index()\n",
    "    .merge(buildings_gdf, on=\"building_id\", how=\"right\")  # keep all buildings\n",
    "    .fillna({\"n_pts\": 0})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc74705c",
   "metadata": {},
   "source": [
    "### check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fe3790c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8400190, 38)\n",
      "(8400190, 27)\n"
     ]
    }
   ],
   "source": [
    "# pts_in_bldgs.head()\n",
    "print(pts_in_bldgs.shape)\n",
    "# print(sorted(pts_in_bldgs.columns.tolist()))\n",
    "\n",
    "\n",
    "\n",
    "############################################################\n",
    "# Check for duplicates\n",
    "############################################################\n",
    "columns_to_drop = ['index_bld', \n",
    "                   'ref_toid', 'ref_osm_id',\n",
    "                   'geometry',\n",
    "                   'context_front_garden',\n",
    "                    'context_back_garden',\n",
    "                    'context_flats_garden',\n",
    "                    'context_green_space_distance',\n",
    "                    'context_green_space_distance_1',\n",
    "                    'context_tree_distance',\n",
    "                   'perimeter'] \n",
    "# Drop duplicate rows across all columns\n",
    "pts_in_bldgs_nodup = (\n",
    "    pts_in_bldgs\n",
    "    .drop(columns=columns_to_drop)   # step 1: remove unwanted columns\n",
    "    .drop_duplicates()               # step 2: drop duplicate rows\n",
    ")\n",
    "# # Alternatively, drop duplicates based on a subset of columns (excluding unwanted ones)\n",
    "# pts_in_bldgs_nodup = pts_in_bldgs.drop_duplicates(subset=[col for col in pts_in_bldgs.columns if col not in columns_to_drop])\n",
    "\n",
    "\n",
    "# Reset index if you want a clean index\n",
    "pts_in_bldgs_nodup = pts_in_bldgs_nodup.reset_index(drop=True)\n",
    "print(pts_in_bldgs_nodup.shape)\n",
    "\n",
    "\n",
    "\n",
    "# # Find duplicates based on building_id\n",
    "# duplicates = pts_in_bldgs_nodup[pts_in_bldgs_nodup.duplicated(subset=\"building_id\", keep=False)]\n",
    "\n",
    "# # Optionally, sort for easier inspection\n",
    "# duplicates = duplicates.sort_values(by=\"building_id\")\n",
    "\n",
    "# # Show result\n",
    "# columns_to_show = ['building_id', 'location_latitude', \n",
    "#                 #    'location_longitude', \n",
    "#                    'current_landuse_order']\n",
    "# print(duplicates[columns_to_show])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c82d652",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "# Save for fast reload and GIS use\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "\n",
    "# # 1. Save as Feather (very fast reload in Python, good for local caching) -- will take 12 seconds\n",
    "# feather_path = cache_dir / \"points_joined_buildings.feather\"\n",
    "# pts_in_bldgs.to_feather(feather_path)\n",
    "# print(pts_in_bldgs.head())\n",
    "\n",
    "\n",
    "\n",
    "# # 2. Save as GeoPackage (portable, works with GIS software like QGIS/ArcGIS) -- will take 13 mins\n",
    "# gpkg_path = cache_dir / \"points_joined_buildings.gpkg\"\n",
    "\n",
    "# # 1) avoid writing the pandas index as a field\n",
    "# # 2) rename any user column named 'fid' (and similar reserved names)\n",
    "# reserved = {\"fid\", \"ogc_fid\", \"rowid\"}  # common reserved PK names in GPKG\n",
    "# rename_map = {c: f\"{c}_attr\" for c in pts_in_bldgs.columns if c.lower() in reserved}\n",
    "# pts_in_bldgs_safe = pts_in_bldgs.rename(columns=rename_map)\n",
    "\n",
    "# # save without the index\n",
    "# pts_in_bldgs_safe.to_file(gpkg_path, driver=\"GPKG\", index=False)\n",
    "# print(pts_in_bldgs_safe.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
