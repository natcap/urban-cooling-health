{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d707702f",
   "metadata": {},
   "source": [
    "## Package and dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7fc1d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Environment Info ===\n",
      "Python version : 3.11.13\n",
      "Python env     : c:\\Users\\pc\\.conda\\envs\\geo_env\\python.exe\n",
      "Platform       : Windows-10-10.0.22631-SP0\n",
      "numpy          : 1.26.4\n",
      "pandas         : 2.3.2\n",
      "geopandas      : 0.14.4\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "# %pip install pyarrow\n",
    "# %pip install --upgrade pandas\n",
    "# %pip install xlsxwriter\n",
    "\n",
    "\n",
    "import pyarrow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "import sys, platform\n",
    "print(\"=== Environment Info ===\")\n",
    "print(f\"Python version : {sys.version.split()[0]}\")\n",
    "print(f\"Python env     : {sys.executable}\")\n",
    "print(f\"Platform       : {platform.platform()}\")\n",
    "print(f\"numpy          : {np.__version__}\")\n",
    "print(f\"pandas         : {pd.__version__}\")\n",
    "print(f\"geopandas      : {gpd.__version__}\")\n",
    "print(\"========================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb614768",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the file path\n",
    "folder = Path(r\"E:\\London\\colouringbritain\\data-extract-2025-09-01\")\n",
    "csv_file = folder / \"building_attributes.csv\"\n",
    "\n",
    "# Subfolder for cached files\n",
    "cache_dir = folder / \"cache\"\n",
    "cache_dir.mkdir(exist_ok=True)   # create if not exists\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca807cd1",
   "metadata": {},
   "source": [
    "## Merge building polygons with attributes data from points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee0ee99",
   "metadata": {},
   "source": [
    "### 1. reload building attributes - points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e97e316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.feather as feather\n",
    "\n",
    "# cache_dir = Path(r\"C:\\Users\\yingjiel\\data\\colouringbritain\")  # --> when using SDSS remote desktop\n",
    "\n",
    "out_feather = cache_dir / \"building_attributes_subset.feather\"\n",
    "df_building_subset = feather.read_feather(out_feather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3be2b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['building_id', 'ref_toid', 'ref_osm_id', 'location_name', 'location_town', 'location_postcode', 'location_latitude', 'location_longitude', 'current_landuse_group', 'current_landuse_order', 'building_attachment_form', 'date_change_building_use', 'date_year', 'size_storeys_attic', 'size_storeys_core', 'size_storeys_basement', 'size_height_apex', 'size_floor_area_ground', 'size_floor_area_total', 'sust_breeam_rating', 'sust_dec', 'sust_retrofit_date', 'construction_core_material', 'construction_secondary_materials', 'construction_roof_covering', 'is_domestic', 'context_front_garden', 'context_back_garden', 'context_flats_garden', 'context_green_space_distance', 'context_green_space_distance', 'context_tree_distance', 'age_retrofit_date']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# df_building_subset.head()\n",
    "print(df_building_subset.columns.tolist())\n",
    "\n",
    "# # Find duplicates based on building_id\n",
    "# duplicates = df_building_subset[df_building_subset.duplicated(subset=\"building_id\", keep=False)]\n",
    "# # Optionally, sort for easier inspection\n",
    "# duplicates = duplicates.sort_values(by=\"building_id\")\n",
    "# print(duplicates)\n",
    "\n",
    "cols_pts_selected = ['building_id', 'location_postcode', \n",
    "                     'location_latitude', 'location_longitude', \n",
    "                     'current_landuse_group', 'current_landuse_order',\n",
    "                     'sust_breeam_rating', 'sust_dec', 'sust_retrofit_date',\n",
    "                     ]\n",
    "\n",
    "building_points = df_building_subset[cols_pts_selected].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57f60211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10180876, 9)\n",
      "(10180876, 9)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Drop duplicate rows across all columns\n",
    "building_points_nodup = building_points.drop_duplicates()               # step 2: drop duplicate rows\n",
    "\n",
    "print(building_points.shape)\n",
    "print(building_points_nodup.shape) # --> the same shape, so no duplicate rows\n",
    "\n",
    "\n",
    "# Auto-rename duplicated columns (if any) by appending _1, _2, etc.\n",
    "def dedup_columns(df):\n",
    "    seen = {}\n",
    "    new_cols = []\n",
    "    for col in df.columns:\n",
    "        if col not in seen:\n",
    "            seen[col] = 0\n",
    "            new_cols.append(col)\n",
    "        else:\n",
    "            seen[col] += 1\n",
    "            new_cols.append(f\"{col}_{seen[col]}\")\n",
    "    df.columns = new_cols\n",
    "    return df\n",
    "\n",
    "building_points = dedup_columns(building_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba45f2b",
   "metadata": {},
   "source": [
    "### convert to point shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d968a278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make points GeoDataFrame from lat/lon\n",
    "building_points_df = building_points.dropna(subset=[\"location_latitude\", \"location_longitude\"]).copy()\n",
    "\n",
    "building_pts = gpd.GeoDataFrame(\n",
    "    building_points_df,\n",
    "    geometry=gpd.points_from_xy(\n",
    "        x=building_points_df[\"location_longitude\"],\n",
    "        y=building_points_df[\"location_latitude\"],\n",
    "    ),\n",
    "    crs=\"EPSG:4326\",  # WGS84  # <<< CHANGED (move crs to GeoDataFrame for clarity)\n",
    ")\n",
    "\n",
    "\n",
    "# # Save as GeoPackage -- will take 13 mins\n",
    "# gpkg_path = cache_dir / \"building_pts.gpkg\"\n",
    "# building_pts.to_file(gpkg_path, driver=\"GPKG\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56884594",
   "metadata": {},
   "source": [
    "### 2. reload building polygon  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17ed8072",
   "metadata": {},
   "outputs": [],
   "source": [
    "buildings_parquet = cache_dir / \"buildings_clean.feather\"\n",
    "\n",
    "# Reload instantly\n",
    "building_poly = gpd.read_feather(buildings_parquet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10d4a9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fid', 'area', 'perimeter', 'geometry']\n",
      "(2223481, 4)\n"
     ]
    }
   ],
   "source": [
    "# print(building_poly.head())\n",
    "print(building_poly.columns.tolist())\n",
    "\n",
    "print(building_poly.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0853cdbf",
   "metadata": {},
   "source": [
    "### spatial join \n",
    "\n",
    "It might be poosible one polygon contains multiple points -- need to find a way to address this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a8c9b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPSG:4326\n",
      "{\"$schema\": \"https://proj.org/schemas/v0.7/projjson.schema.json\", \"type\": \"ProjectedCRS\", \"name\": \"OSGB36 / British National Grid\", \"base_crs\": {\"name\": \"OSGB36\", \"datum\": {\"type\": \"GeodeticReferenceFrame\", \"name\": \"Ordnance Survey of Great Britain 1936\", \"ellipsoid\": {\"name\": \"Airy 1830\", \"semi_major_axis\": 6377563.396, \"inverse_flattening\": 299.3249646}}, \"coordinate_system\": {\"subtype\": \"ellipsoidal\", \"axis\": [{\"name\": \"Geodetic latitude\", \"abbreviation\": \"Lat\", \"direction\": \"north\", \"unit\": \"degree\"}, {\"name\": \"Geodetic longitude\", \"abbreviation\": \"Lon\", \"direction\": \"east\", \"unit\": \"degree\"}]}, \"id\": {\"authority\": \"EPSG\", \"code\": 4277}}, \"conversion\": {\"name\": \"British National Grid\", \"method\": {\"name\": \"Transverse Mercator\", \"id\": {\"authority\": \"EPSG\", \"code\": 9807}}, \"parameters\": [{\"name\": \"Latitude of natural origin\", \"value\": 49, \"unit\": \"degree\", \"id\": {\"authority\": \"EPSG\", \"code\": 8801}}, {\"name\": \"Longitude of natural origin\", \"value\": -2, \"unit\": \"degree\", \"id\": {\"authority\": \"EPSG\", \"code\": 8802}}, {\"name\": \"Scale factor at natural origin\", \"value\": 0.9996012717, \"unit\": \"unity\", \"id\": {\"authority\": \"EPSG\", \"code\": 8805}}, {\"name\": \"False easting\", \"value\": 400000, \"unit\": \"metre\", \"id\": {\"authority\": \"EPSG\", \"code\": 8806}}, {\"name\": \"False northing\", \"value\": -100000, \"unit\": \"metre\", \"id\": {\"authority\": \"EPSG\", \"code\": 8807}}]}, \"coordinate_system\": {\"subtype\": \"Cartesian\", \"axis\": [{\"name\": \"Easting\", \"abbreviation\": \"E\", \"direction\": \"east\", \"unit\": \"metre\"}, {\"name\": \"Northing\", \"abbreviation\": \"N\", \"direction\": \"north\", \"unit\": \"metre\"}]}, \"scope\": \"Engineering survey, topographic mapping.\", \"area\": \"United Kingdom (UK) - offshore to boundary of UKCS within 49\\u00b045'N to 61\\u00b0N and 9\\u00b0W to 2\\u00b0E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\", \"bbox\": {\"south_latitude\": 49.75, \"west_longitude\": -9.01, \"north_latitude\": 61.01, \"east_longitude\": 2.01}, \"id\": {\"authority\": \"EPSG\", \"code\": 27700}}\n",
      "EPSG:27700\n",
      "EPSG:27700\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "print(building_pts.crs)\n",
    "print(building_poly.crs)\n",
    "\n",
    "\n",
    "# Reproject both GeoDataFrames\n",
    "# For London, use a projected CRS in meters. The two best choices:\n",
    "#   EPSG:27700 — OSGB 1936 / British National Grid (BNG)\n",
    "#   Standard for Great Britain; great local distance accuracy around London.\n",
    "building_pts = building_pts.to_crs(epsg=27700)\n",
    "building_poly = building_poly.to_crs(epsg=27700)\n",
    "\n",
    "print(building_pts.crs)\n",
    "print(building_poly.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e159efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2841826, 14)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Pick the right predicate:\n",
    "#    - \"contains\" : point must be strictly inside polygon (boundary points excluded)\n",
    "#    - \"covers\"   : includes points on polygon boundary (often safer)\n",
    "predicate = \"covers\"\n",
    "\n",
    "bld_with_pts = gpd.sjoin(\n",
    "    building_poly,   # LEFT: keep all buildings\n",
    "    building_pts,    # RIGHT: bring point attrs in where they fall inside\n",
    "    how=\"left\",\n",
    "    predicate=predicate,\n",
    "    lsuffix=\"bld\",\n",
    "    rsuffix=\"pt\",\n",
    ")\n",
    "\n",
    "print(bld_with_pts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb2acb2",
   "metadata": {},
   "source": [
    "`sjoin` is a row-wise join.\n",
    "\n",
    "If one polygon covers multiple points, then the polygon row is **repeated** once for each matching point.\n",
    "\n",
    "So you’ll get one row per (polygon, point) pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55cda2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fid          area  perimeter  \\\n",
      "2  3.0  4.776945e-09   0.000326   \n",
      "3  4.0  5.876390e-08   0.002627   \n",
      "3  4.0  5.876390e-08   0.002627   \n",
      "3  4.0  5.876390e-08   0.002627   \n",
      "3  4.0  5.876390e-08   0.002627   \n",
      "\n",
      "                                            geometry   index_pt  building_id  \\\n",
      "2  POLYGON ((530958.680 200117.570, 530962.290 20...  2981433.0    1164790.0   \n",
      "3  POLYGON ((526208.730 198238.610, 526212.740 19...  3787818.0     937225.0   \n",
      "3  POLYGON ((526208.730 198238.610, 526212.740 19...  4497128.0    3546140.0   \n",
      "3  POLYGON ((526208.730 198238.610, 526212.740 19...  2926106.0     956250.0   \n",
      "3  POLYGON ((526208.730 198238.610, 526212.740 19...  4040057.0    1003486.0   \n",
      "\n",
      "  location_postcode  location_latitude  location_longitude  \\\n",
      "2                             51.68447            -0.10702   \n",
      "3                             51.66841            -0.17639   \n",
      "3                             51.66847            -0.17635   \n",
      "3                             51.66848            -0.17594   \n",
      "3                             51.66866            -0.17636   \n",
      "\n",
      "  current_landuse_group current_landuse_order sust_breeam_rating sust_dec  \\\n",
      "2                  None           Residential                               \n",
      "3                  None           Residential                               \n",
      "3                  None                                                     \n",
      "3                  None           Residential                               \n",
      "3                  None           Residential                               \n",
      "\n",
      "   sust_retrofit_date  \n",
      "2                 NaN  \n",
      "3                 NaN  \n",
      "3                 NaN  \n",
      "3                 NaN  \n",
      "3                 NaN  \n"
     ]
    }
   ],
   "source": [
    "# print(bld_with_pts.columns.tolist())\n",
    "# print(building_poly.columns.tolist())\n",
    "# print(bld_with_pts.head())\n",
    "\n",
    "# Identify points that didn't match any building\n",
    "pts_matched = bld_with_pts[bld_with_pts[\"building_id\"].notna()]  # building_id from points that matched\n",
    "print(pts_matched.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9be8f0",
   "metadata": {},
   "source": [
    "#### Identify records that one polygon covers multiple points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0e7ab48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fid          area  perimeter  \\\n",
      "3  4.0  5.876390e-08   0.002627   \n",
      "3  4.0  5.876390e-08   0.002627   \n",
      "3  4.0  5.876390e-08   0.002627   \n",
      "3  4.0  5.876390e-08   0.002627   \n",
      "3  4.0  5.876390e-08   0.002627   \n",
      "\n",
      "                                            geometry   index_pt  building_id  \\\n",
      "3  POLYGON ((526208.730 198238.610, 526212.740 19...  3787818.0     937225.0   \n",
      "3  POLYGON ((526208.730 198238.610, 526212.740 19...  4497128.0    3546140.0   \n",
      "3  POLYGON ((526208.730 198238.610, 526212.740 19...  2926106.0     956250.0   \n",
      "3  POLYGON ((526208.730 198238.610, 526212.740 19...  4040057.0    1003486.0   \n",
      "3  POLYGON ((526208.730 198238.610, 526212.740 19...  3937069.0     927964.0   \n",
      "\n",
      "  location_postcode  location_latitude  location_longitude  \\\n",
      "3                             51.66841            -0.17639   \n",
      "3                             51.66847            -0.17635   \n",
      "3                             51.66848            -0.17594   \n",
      "3                             51.66866            -0.17636   \n",
      "3                             51.66871            -0.17597   \n",
      "\n",
      "  current_landuse_group current_landuse_order sust_breeam_rating sust_dec  \\\n",
      "3                  None           Residential                               \n",
      "3                  None                                                     \n",
      "3                  None           Residential                               \n",
      "3                  None           Residential                               \n",
      "3                  None           Residential                               \n",
      "\n",
      "   sust_retrofit_date  pts_count  \n",
      "3                 NaN          5  \n",
      "3                 NaN          5  \n",
      "3                 NaN          5  \n",
      "3                 NaN          5  \n",
      "3                 NaN          5  \n"
     ]
    }
   ],
   "source": [
    "# add a count of how many points each polygon covers\n",
    "bld_with_pts[\"pts_count\"] = bld_with_pts.groupby(bld_with_pts.index)[\"building_id\"].transform(\"count\")\n",
    "\n",
    "# filter polygons that cover more than 1 point\n",
    "polys_multi_pts = bld_with_pts[bld_with_pts[\"pts_count\"] > 1]\n",
    "\n",
    "print(polys_multi_pts.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca74fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Choose a stable polygon key\n",
    "poly_key = \"fid\"\n",
    "pts_key = \"building_id\"\n",
    "\n",
    "# helpers\n",
    "def mode_or_na(s: pd.Series):\n",
    "    \"\"\"\n",
    "    Return the most frequent (mode) value in a pandas Series.\n",
    "    - Drop missing values first.\n",
    "    - If the Series is not empty after dropping NaN, return the first mode.\n",
    "      (pandas mode() can return multiple values if there are ties, but\n",
    "       we just take the first to keep it simple.)\n",
    "    - If the Series is empty, return pandas.NA (missing).\n",
    "    \"\"\"\n",
    "    s = s.dropna()\n",
    "    return s.mode().iloc[0] if not s.empty else pd.NA\n",
    "\n",
    "\n",
    "# Alternative mode function that breaks ties by first appearance\n",
    "def mode_safe(s: pd.Series):\n",
    "    \"\"\"\n",
    "    Return the most frequent non-null, non-empty value.\n",
    "    If tie, pick the one that appears first in the original order.\n",
    "    If no valid values, return pd.NA.\n",
    "    \"\"\"\n",
    "    # Drop NA and empty/whitespace-only strings\n",
    "    s = s.dropna().replace(r'^\\s*$', pd.NA, regex=True).dropna()\n",
    "    if s.empty:\n",
    "        return pd.NA\n",
    "    \n",
    "    # Count frequencies\n",
    "    counts = s.value_counts()\n",
    "    # all values tied for max frequency\n",
    "    top = set(counts.index[counts.eq(counts.max())])\n",
    "    # preserve original order for tie-break\n",
    "    for v in s:\n",
    "        if v in top:\n",
    "            return v\n",
    "        \n",
    "\n",
    "def uniques_sorted(s: pd.Series):\n",
    "    \"\"\"\n",
    "    Return a sorted list of unique, non-null, non-empty values.\n",
    "    - Drop missing values first.\n",
    "    - Use pandas.unique() to extract unique values in order of appearance.\n",
    "    - Sort the list so the output is consistent across runs.\n",
    "    - If the Series is empty, return an empty list [].\n",
    "    \"\"\"\n",
    "    s = s.dropna().replace(r'^\\s*$', pd.NA, regex=True).dropna()\n",
    "    return sorted(pd.unique(s)) if not s.empty else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e4fb3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3) aggregate to one row per polygon\n",
    "grp = bld_with_pts.groupby(poly_key, dropna=False)\n",
    "agg_pts = grp.agg(\n",
    "    point_count          = (pts_key, \"count\"),                        # number of matched points\n",
    "    landuse_mode         = (\"current_landuse_order\", mode_safe),      # most common land use\n",
    "    landuse_unique       = (\"current_landuse_order\", uniques_sorted), # all unique land uses\n",
    "    breeam_mode          = (\"sust_breeam_rating\", mode_safe),         # most common BREEAM rating\n",
    "    breeam_unique        = (\"sust_breeam_rating\", uniques_sorted),    # all unique BREEAM ratings\n",
    ").reset_index()\n",
    "\n",
    "# 4) merge back to polygons so geometry is included\n",
    "polys_unique = building_poly.copy()\n",
    "if poly_key not in polys_unique.columns:\n",
    "    polys_unique = polys_unique.reset_index().rename(columns={\"index\": poly_key})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd91c9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fid  point_count landuse_mode landuse_unique breeam_mode breeam_unique\n",
      "0  1.0            0         <NA>             []        <NA>            []\n",
      "1  2.0            0         <NA>             []        <NA>            []\n",
      "2  3.0            1  Residential  [Residential]        <NA>            []\n",
      "3  4.0            5  Residential  [Residential]        <NA>            []\n",
      "4  5.0            0         <NA>             []        <NA>            []\n",
      "(2223481, 6)\n",
      "(2223481, 4)\n"
     ]
    }
   ],
   "source": [
    "print(agg_pts.head())\n",
    "print(agg_pts.shape)\n",
    "print(polys_unique.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65e37225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fid          area  perimeter  \\\n",
      "0  1.0  3.503437e-09   0.000266   \n",
      "1  2.0  3.830134e-09   0.000308   \n",
      "2  3.0  4.776945e-09   0.000326   \n",
      "3  4.0  5.876390e-08   0.002627   \n",
      "4  5.0  2.593630e-09   0.000232   \n",
      "\n",
      "                                            geometry  point_count  \\\n",
      "0  POLYGON ((530850.030 200467.340, 530853.140 20...            0   \n",
      "1  POLYGON ((530946.580 200101.170, 530942.780 20...            0   \n",
      "2  POLYGON ((530958.680 200117.570, 530962.290 20...            1   \n",
      "3  POLYGON ((526208.730 198238.610, 526212.740 19...            5   \n",
      "4  POLYGON ((524271.910 197156.219, 524277.781 19...            0   \n",
      "\n",
      "  landuse_mode landuse_unique breeam_mode breeam_unique  \n",
      "0         <NA>             []        <NA>            []  \n",
      "1         <NA>             []        <NA>            []  \n",
      "2  Residential  [Residential]        <NA>            []  \n",
      "3  Residential  [Residential]        <NA>            []  \n",
      "4         <NA>             []        <NA>            []  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "merge1 = polys_unique.merge(agg_pts, on=poly_key, how=\"left\")\n",
    "merge1 = gpd.GeoDataFrame(merge1, geometry=\"geometry\", crs=building_poly.crs)\n",
    "\n",
    "# merge1: one row per polygon with counts, modes, and unique lists\n",
    "print(merge1.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7969e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2223481, 9)\n"
     ]
    }
   ],
   "source": [
    "print(merge1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8663518e",
   "metadata": {},
   "source": [
    "#### Fallback nearest join for unmatched points\n",
    "\n",
    "(1) finds which points were already matched by your primary join, \n",
    "\n",
    "(2) computes a nearest polygon only for the unmatched points (in meters), and \n",
    "\n",
    "(3) returns a per-point assignment table you can merge anywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e213ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Matched points: 2632484\n",
      "\n",
      " Unmatched points: 5693084\n"
     ]
    }
   ],
   "source": [
    "# 1) Points matched by the primary join (some polygons may repeat; we just need the set)\n",
    "matched_pts = set(bld_with_pts[pts_key].dropna().unique())\n",
    "print(f\"\\n Matched points: {len(matched_pts)}\")\n",
    "\n",
    "# 2) Split points into matched / unmatched\n",
    "unmatched_pts = building_pts[~building_pts[pts_key].isin(matched_pts)].copy()\n",
    "print(f\"\\n Unmatched points: {len(unmatched_pts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2afd786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fid                                           geometry\n",
      "0    1.0  POLYGON ((530850.030 200467.340, 530853.140 20...\n",
      "1    2.0  POLYGON ((530946.580 200101.170, 530942.780 20...\n",
      "4    5.0  POLYGON ((524271.910 197156.219, 524277.781 19...\n",
      "10  11.0  POLYGON ((523756.723 197059.965, 523761.078 19...\n",
      "12  13.0  POLYGON ((526317.830 197736.050, 526317.720 19...\n"
     ]
    }
   ],
   "source": [
    "# print(merge1.head())\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Assumptions:\n",
    "# - `merge1` is already built from the PRIMARY polygons-left join (\"covers\")\n",
    "#   and contains per-polygon fields like `point_count`, plus mode/unique cols.\n",
    "# - We now only handle polygons with point_count == 0 via NEAREST fallback.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# 1) Find polygons that did not get any matched points in the primary join\n",
    "no_match_polys = merge1.loc[merge1[\"point_count\"].fillna(0).eq(0), [poly_key, \"geometry\"]]\n",
    "attr_cols = [\"current_landuse_order\", \"sust_breeam_rating\",\n",
    "             \"sust_dec\", \"sust_retrofit_date\"]\n",
    "print(no_match_polys.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75d8cd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fid', 'area', 'perimeter', 'geometry', 'point_count', 'landuse_mode', 'landuse_unique', 'breeam_mode', 'breeam_unique', 'nearest_dist_m', 'match_method', 'point_count_fb', 'nearest_dist_m_fb', 'landuse_mode_fb', 'landuse_unique_fb', 'breeam_mode_fb', 'breeam_unique_fb']\n",
      "   fid  point_count match_method nearest_dist_m landuse_mode breeam_mode\n",
      "0  1.0          1.0      nearest       0.605724  Residential        <NA>\n",
      "1  2.0          1.0      nearest       0.707167  Residential        <NA>\n",
      "2  3.0          1.0          pip           <NA>  Residential        <NA>\n",
      "3  4.0          5.0          pip           <NA>  Residential        <NA>\n",
      "4  5.0          1.0      nearest       1.694663  Residential        <NA>\n"
     ]
    }
   ],
   "source": [
    "# constants for clarity / reruns\n",
    "BNG_EPSG = 27700       # British National Grid (meters)\n",
    "MAX_DIST = 5           # meters in BNG; tune as needed\n",
    "\n",
    "if not no_match_polys.empty:\n",
    "    # 2) Project both layers for distance-based join (London → EPSG:27700)\n",
    "    poly_m = no_match_polys.to_crs(BNG_EPSG) # British National Grid (meters)\n",
    "    pts_m  = building_pts.to_crs(BNG_EPSG)\n",
    "\n",
    "    # 3) Nearest join: polygon-left to pull attributes from closest point\n",
    "    #    Nearest join (polygons-left, without k=1) → may return multiple matches per polygon\n",
    "    nearest = gpd.sjoin_nearest(\n",
    "        poly_m,\n",
    "        pts_m[[pts_key, \"geometry\"] + attr_cols],\n",
    "        how=\"left\",\n",
    "        distance_col=\"nearest_dist_m\",\n",
    "        max_distance=MAX_DIST,\n",
    "        # k=1, # intentionally omitted to allow ties at equal distance\n",
    "        lsuffix=\"bld\",\n",
    "        rsuffix=\"pt\",\n",
    "    ).to_crs(no_match_polys.crs)\n",
    "\n",
    "    # 4) Aggregate fallback attributes per polygon\n",
    "    #    Because we may now have >1 nearest point at equal distance,\n",
    "    #    we summarize all their attributes.\n",
    "    fallback_agg = nearest.groupby(poly_key).agg(\n",
    "        point_count    = (pts_key, \"count\"),               # count of tied nearest points\n",
    "        nearest_dist_m = (\"nearest_dist_m\", \"min\"),        # min = actual nearest distance\n",
    "        landuse_mode   = (\"current_landuse_order\", mode_safe),\n",
    "        landuse_unique = (\"current_landuse_order\", uniques_sorted),\n",
    "        breeam_mode    = (\"sust_breeam_rating\", mode_safe),\n",
    "        breeam_unique  = (\"sust_breeam_rating\", uniques_sorted),\n",
    "    ).reset_index()\n",
    "\n",
    "    # Identify polygons that truly received a nearest match (within MAX_DIST)\n",
    "    fb_ids = set(fallback_agg.loc[fallback_agg[\"point_count\"].fillna(0) > 0, poly_key])\n",
    "\n",
    "\n",
    "    # # Mark source\n",
    "    # fallback_agg[\"match_method\"] = \"nearest\"\n",
    "\n",
    "    # 5) Update `merge1`: attach fallback columns (keep primary values!)\n",
    "    # Ensure primary columns exist so combine_first works even if they were absent in merge1\n",
    "    for col in [\"point_count\", \"nearest_dist_m\",\n",
    "                \"landuse_mode\", \"landuse_unique\", \"breeam_mode\", \"breeam_unique\"]:\n",
    "        if col not in merge1.columns:\n",
    "            merge1[col] = pd.NA\n",
    "\n",
    "    # Q: Should we add `match_method` now?\n",
    "    # A: Yes — add it before merging and default to \"pip\" for polygons\n",
    "    #    that had any primary points; leave as NA for those with none.\n",
    "    if \"match_method\" not in merge1.columns:\n",
    "        merge1[\"match_method\"] = pd.NA\n",
    "        # mark polygons that had primary matches as \"pip\"\n",
    "        merge1.loc[merge1[\"point_count\"].fillna(0) > 0, \"match_method\"] = \"pip\"\n",
    "\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # Merge fallback summaries as *_fb columns (do NOT drop primary columns) \n",
    "    # ----------------------------------------------------------------------------\n",
    "    merge2 = merge1.merge(\n",
    "        fallback_agg,               # contains fallback summaries\n",
    "        on=poly_key,\n",
    "        how=\"left\",\n",
    "        suffixes=(\"\", \"_fb\")        # fallback columns carry \"_fb\"; primary keep base name\n",
    "    )\n",
    "\n",
    "    print(merge2.columns.tolist())\n",
    "\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # For polygons that had primary matches, keep original values.\n",
    "    # For polygons with no primary (point_count == 0 or NA), fill from fallback (_fb).\n",
    "    # --------------------------------------------------------------------------\n",
    "    import numpy as np\n",
    "\n",
    "    # build a mask where we should use fallback values:\n",
    "    # - primary point_count is 0 or NA\n",
    "    # - AND fallback point_count_fb > 0 (i.e., a nearest point was actually found)\n",
    "    pc_primary  = merge2.get(\"point_count\")\n",
    "    pc_fallback = merge2.get(\"point_count_fb\")\n",
    "    use_fb = pc_primary.fillna(0).eq(0) & pc_fallback.fillna(0).gt(0)\n",
    "\n",
    "    # 1) point_count: replace 0/NA with fallback count where use_fb is True\n",
    "    if \"point_count_fb\" in merge2.columns:\n",
    "        merge2[\"point_count\"] = np.where(use_fb, merge2[\"point_count_fb\"], merge2[\"point_count\"])\n",
    "        merge2.drop(columns=[\"point_count_fb\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    # 2) other attributes: fill from fallback ONLY where use_fb is True; keep primary otherwise\n",
    "    for col in [\"nearest_dist_m\", \n",
    "                \"landuse_mode\", \"landuse_unique\", \"breeam_mode\", \"breeam_unique\"]:\n",
    "        fb_col = col + \"_fb\"\n",
    "        if fb_col in merge2.columns:\n",
    "            # np.where(use_fb, merge2[fb_col], merge2[col]) works row by row:\n",
    "            # - If the polygon had no primary match (use_fb == True),\n",
    "            #   take the fallback value from the *_fb column.\n",
    "            # - Otherwise, keep the original primary value from the base column.\n",
    "            # This ensures we only overwrite for polygons that really need fallback.\n",
    "            merge2[col] = np.where(use_fb, merge2[fb_col], merge2[col])\n",
    "\n",
    "            # Drop the temporary fallback column once we've merged its values in.\n",
    "            # This keeps the final dataframe clean (only the unified column remains).\n",
    "            merge2.drop(columns=[fb_col], inplace=True)\n",
    "\n",
    "    \n",
    "    # Update match_method:\n",
    "    # - If a polygon got a valid nearest match (in fb_ids) AND its current method is NA,\n",
    "    #   label it \"nearest\".\n",
    "    # - If it already was \"pip\", keep it as \"pip\" (primary wins).\n",
    "    if \"match_method\" not in merge2.columns:\n",
    "        merge2[\"match_method\"] = pd.NA\n",
    "    mask_nearest_fill = merge2[poly_key].isin(fb_ids) & merge2[\"match_method\"].isna()\n",
    "    merge2.loc[mask_nearest_fill, \"match_method\"] = \"nearest\"\n",
    "\n",
    "\n",
    "    # --- final enriched polygons (merge2) ---\n",
    "    print(\n",
    "        merge2[\n",
    "            [poly_key, \"point_count\", \"match_method\", \"nearest_dist_m\",\n",
    "             \"landuse_mode\", \"breeam_mode\"]\n",
    "        ].head()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bede5d",
   "metadata": {},
   "source": [
    "The above code has merged the primary and fallback data\n",
    "\n",
    "- merge1 → polygons with attributes from the primary join (covers)\n",
    "\n",
    "- merge2 → same polygons, but with fallback nearest values filled for those that had no points in the primary join → The final complete data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d390645e",
   "metadata": {},
   "source": [
    "### check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39dac91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2223481, 11)\n",
      "(2223481, 11)\n",
      "Index(['fid', 'area', 'perimeter', 'geometry', 'point_count', 'landuse_mode',\n",
      "       'landuse_unique', 'breeam_mode', 'breeam_unique', 'nearest_dist_m',\n",
      "       'match_method'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(merge1.shape)\n",
    "print(merge2.shape)\n",
    "\n",
    "\n",
    "# merge1 = merge1.drop(columns=[\"area\", \"perimeter\"], errors=\"ignore\")\n",
    "# merge2 = merge2.drop(columns=[\"area\", \"perimeter\"], errors=\"ignore\")\n",
    "\n",
    "\n",
    "# print(merge1.head())\n",
    "# print(merge2.head())\n",
    "\n",
    "\n",
    "# print(merge1.columns)\n",
    "print(merge2.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21c21ac",
   "metadata": {},
   "source": [
    "### save data\n",
    "\n",
    "`.copy()` is important: without it, bld_with_attr = merge2 just creates a second reference to the same object. Any edits to one would also change the other.\n",
    "\n",
    "With .copy(), you get an independent object that you can modify safely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce2f8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bld_with_attr = merge2.copy()\n",
    "\n",
    "# 1. Save as Feather (very fast reload in Python, good for local caching) -- will take 12 seconds\n",
    "feather_path = cache_dir / \"bld_with_attr.feather\"\n",
    "bld_with_attr.to_feather(feather_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8060f1",
   "metadata": {},
   "source": [
    "### reload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08d1e6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2223481 features\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "\n",
    "# path\n",
    "feather_path = cache_dir / \"bld_with_attr.feather\"\n",
    "\n",
    "# reload\n",
    "bld_with_attr = gpd.read_feather(feather_path)\n",
    "\n",
    "print(\"Loaded\", len(bld_with_attr), \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27f1557",
   "metadata": {},
   "source": [
    "GeoPackage fields can’t store Python lists/dicts, so Fiona raises that error. Convert those columns to something GPKG understands (e.g., TEXT via JSON) or explode them before writing.\n",
    "\n",
    "Here’s a drop-in fixer that:\n",
    "- detects non-scalar columns (list/dict/set/tuple/ndarray),\n",
    "- converts them to JSON strings (keeping None as null),\n",
    "- then writes the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a62bb04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, numpy as np, pandas as pd\n",
    "import geopandas as gpd\n",
    "# EDITED: add this import once at the top of your file/cell\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "def sanitize_for_gpkg(gdf: gpd.GeoDataFrame):\n",
    "    g = gdf.copy()\n",
    "    geom = g.geometry.name\n",
    "\n",
    "    # 1) Rename reserved PK names\n",
    "    # avoid writing the pandas index as a field\n",
    "    # rename any user column named 'fid' (and similar reserved names)\n",
    "    # common reserved PK names in GPKG\n",
    "    reserved = {\"fid\", \"ogc_fid\", \"rowid\"}\n",
    "    rename_map = {c: f\"{c}_attr\" for c in g.columns if c.lower() in reserved}\n",
    "    if rename_map:\n",
    "        g = g.rename(columns=rename_map)\n",
    "\n",
    "    # 2) Convert non-scalar values (list/dict/array/tuple/set) to JSON TEXT\n",
    "    def is_non_scalar(v):\n",
    "        return isinstance(v, (list, dict, tuple, set, np.ndarray))\n",
    "\n",
    "    def to_text(v):\n",
    "        if v is None or (isinstance(v, float) and pd.isna(v)):\n",
    "            return None\n",
    "        if is_non_scalar(v):\n",
    "            try:\n",
    "                return json.dumps(v, ensure_ascii=False)\n",
    "            except Exception:\n",
    "                return str(v)\n",
    "        return v\n",
    "\n",
    "    for c in g.columns:\n",
    "        if c == geom:\n",
    "            continue\n",
    "        if g[c].map(is_non_scalar).any():\n",
    "            g[c] = g[c].map(to_text).astype(\"string\")\n",
    "\n",
    "    # 3) Normalize dtypes to OGR-friendly ones\n",
    "    for c in g.columns:\n",
    "        if c == geom:\n",
    "            continue\n",
    "        s = g[c]\n",
    "        if pd.api.types.is_datetime64_any_dtype(s):\n",
    "            g[c] = s.dt.strftime(\"%Y-%m-%d %H:%M:%S\")  # TEXT\n",
    "        \n",
    "        # elif pd.api.types.is_categorical_dtype(s):\n",
    "        elif isinstance(s.dtype, CategoricalDtype):  # EDITED\n",
    "            g[c] = s.astype(\"string\")\n",
    "\n",
    "        elif s.dtype == \"object\":\n",
    "            g[c] = s.astype(\"string\")\n",
    "        elif pd.api.types.is_bool_dtype(s):\n",
    "            g[c] = s.astype(\"int8\")  # or keep bool; int is safest\n",
    "        elif pd.api.types.is_integer_dtype(s):\n",
    "            g[c] = s.astype(\"int64\")\n",
    "        elif pd.api.types.is_float_dtype(s):\n",
    "            g[c] = s.astype(\"float64\")\n",
    "\n",
    "    # Safety check: ensure no reserved names remain\n",
    "    assert not any(c.lower() in reserved for c in g.columns), \"Reserved field name still present.\"\n",
    "\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1adc6329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Save as GeoPackage (portable, works with GIS software like QGIS/ArcGIS) -- will take 13 mins\n",
    "gpkg_path = cache_dir / \"bld_with_attr.gpkg\"\n",
    "\n",
    "# ---- use it ---- \n",
    "g = sanitize_for_gpkg(bld_with_attr) \n",
    "# g.to_file(gpkg_path, driver=\"GPKG\", layer=\"bld_with_attr\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcb48f1",
   "metadata": {},
   "source": [
    "### save data - gpkg - smaller "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b90f475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fid_attr', 'area', 'perimeter', 'geometry', 'point_count', 'landuse_mode', 'landuse_unique', 'breeam_mode', 'breeam_unique', 'nearest_dist_m', 'match_method']\n"
     ]
    }
   ],
   "source": [
    "print(g.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1e7555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fid_attr', 'geometry', 'point_count', 'landuse_mode', 'breeam_mode', 'nearest_dist_m', 'match_method']\n"
     ]
    }
   ],
   "source": [
    "# %pip install pyogrio\n",
    "# pyogrio is faster and smaller.\n",
    "\n",
    "from shapely import set_precision\n",
    "import pandas as pd\n",
    "import pyogrio\n",
    "\n",
    "\n",
    "g_small = g.copy().drop(columns=[\"area\", \"perimeter\", 'landuse_unique', 'breeam_unique'], errors=\"ignore\")\n",
    "\n",
    "print(g_small.columns.tolist())\n",
    "\n",
    "# code strings → ints + lookups\n",
    "for col in [\"landuse_mode\",\"breeam_mode\",\"match_method\"]:\n",
    "    if col in g_small.columns:  # EDITED: guard if column missing\n",
    "        codes, cats = pd.factorize(g_small[col], sort=True)\n",
    "        g_small[col+\"_code\"] = pd.Series(codes, index=g_small.index).astype(\"Int16\")\n",
    "        g_small.drop(columns=[col], inplace=True)\n",
    "        csv_path = cache_dir / f\"bld_with_attr_{col}_lookup.csv\"\n",
    "        pd.DataFrame({col+\"_code\": range(len(cats)), col: cats}).to_csv(csv_path, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1925e873",
   "metadata": {},
   "source": [
    "#### compress shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b1939fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import set_precision\n",
    "from shapely.ops import unary_union\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "\n",
    "def remove_small_parts(geom, min_area_m2=1.0):\n",
    "    \"\"\"Drop tiny polygon parts (m²) to reduce vertices.\"\"\"\n",
    "    if geom is None or geom.is_empty:\n",
    "        return geom\n",
    "    if isinstance(geom, Polygon):\n",
    "        return geom if geom.area >= min_area_m2 else None\n",
    "    if isinstance(geom, MultiPolygon):\n",
    "        parts = [g for g in geom.geoms if g.area >= min_area_m2]\n",
    "        if not parts:\n",
    "            return None\n",
    "        return MultiPolygon(parts) if len(parts) > 1 else parts[0]\n",
    "    return geom\n",
    "\n",
    "\n",
    "# 3) Quantize coordinates (snap to grid) — reduces duplicate/near-duplicate vertices\n",
    "#    Pick a grid that’s safe for your CRS (meters in BNG). 0.5 m is often visually lossless for buildings.\n",
    "# g_small[\"geometry\"] = g_small.geometry.apply(lambda geom: set_precision(geom, grid_size=0.5))  # meters\n",
    "\n",
    "# 4) Topology-preserving simplification — removes vertices while keeping shape\n",
    "#    Use a *small* tolerance first (e.g., 0.2–0.5 m for building outlines).\n",
    "#    Note: shapely.simplify(preserve_topology=True) keeps topology but is heavier.\n",
    "g_small[\"geometry\"] = g_small.geometry.simplify(tolerance=0.5, preserve_topology=True)\n",
    "\n",
    "# 5) Remove tiny parts/rings that contribute many vertices but add no value\n",
    "g_small[\"geometry\"] = g_small.geometry.apply(lambda g: remove_small_parts(g, min_area_m2=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db93111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# quantize nearest distance to centimeters\n",
    "if \"nearest_dist_m\" in g_small.columns:  # EDITED: guard if missing\n",
    "    nd = pd.to_numeric(g_small[\"nearest_dist_m\"], errors=\"coerce\")  # EDITED: robust to non-numeric\n",
    "    nd = nd.replace([np.inf, -np.inf], np.nan)  # guard against inf\n",
    "\n",
    "    # Convert meters → centimeters, round to nearest integer, then build a nullable Int32 array\n",
    "    nearest_cm_ext = pd.array(\n",
    "        np.rint(nd.to_numpy() * 100),          # integer-valued float array (NaN stays NaN)\n",
    "        dtype=pd.Int32Dtype()                  # pandas nullable Int32 (supports <NA>)\n",
    "    )\n",
    "    # EDITED: use pandas nullable Int32 that supports NA values\n",
    "    g_small[\"nearest_dist_cm\"] = pd.Series(nearest_cm_ext, index=g_small.index)\n",
    "    g_small.drop(columns=[\"nearest_dist_m\"], inplace=True)\n",
    "\n",
    "# counts to Int16\n",
    "if \"point_count\" in g_small.columns:  # EDITED: guard + robust cast\n",
    "    g_small[\"point_count\"] = pd.to_numeric(g_small[\"point_count\"], errors=\"coerce\").astype(pd.Int16Dtype())\n",
    "\n",
    "# # write compact GPKG without spatial index\n",
    "# gpkg_path = cache_dir / \"bld_with_attr_compact.gpkg\"\n",
    "# pyogrio.write_dataframe(\n",
    "#     g_small, \n",
    "#     gpkg_path, \n",
    "#     layer=\"bld_attr\",\n",
    "#     driver=\"GPKG\", \n",
    "#     layer_options={\"SPATIAL_INDEX\": \"NO\"},\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de71984f",
   "metadata": {},
   "source": [
    "### prep for urban cooling model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf5b241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fid_attr', 'geometry', 'point_count', 'type', 'breeam_mode_code', 'match_method_code', 'nearest_dist_cm']\n",
      "['fid_attr', 'geometry', 'point_count', 'type', 'breeam_mode_code', 'match_method_code', 'nearest_dist_cm']\n",
      "Saved: E:\\London\\colouringbritain\\data-extract-2025-09-01\\cache\\bld_with_attr_compact_ucm2.gpkg\n"
     ]
    }
   ],
   "source": [
    "import pyogrio\n",
    "\n",
    "print(g_small.columns.tolist())  # ### prep for urban cooling model\n",
    "\n",
    "# If the column exists, rename it\n",
    "if \"landuse_mode_code\" in g_small.columns:\n",
    "    g_small = g_small.rename(columns={\"landuse_mode_code\": \"type\"})\n",
    "\n",
    "# EDITED: fallback — if \"type\" still missing but a label exists, use it\n",
    "if \"type\" not in g_small.columns and \"landuse_mode\" in g_small.columns:  # EDITED\n",
    "    g_small = g_small.rename(columns={\"landuse_mode\": \"type\"})           # EDITED\n",
    "\n",
    "# EDITED: ensure fid_attr exists (common source column is \"fid\")\n",
    "if \"fid_attr\" not in g_small.columns and \"fid\" in g_small.columns:       # EDITED\n",
    "    g_small = g_small.rename(columns={\"fid\": \"fid_attr\"})                # EDITED\n",
    "\n",
    "print(g_small.columns.tolist())\n",
    "\n",
    "# make a copy with only selected columns\n",
    "keep_cols = [\"fid_attr\", \"type\", \"geometry\"]\n",
    "\n",
    "g_selected = g_small[[col for col in keep_cols if col in g_small.columns]].copy()\n",
    "\n",
    "# sanity check to avoid writing an empty/invalid layer\n",
    "missing = [c for c in keep_cols if c not in g_selected.columns]\n",
    "if missing:\n",
    "    print(f\"Warning: missing columns in output: {missing}\")\n",
    "\n",
    "# write compact GPKG without spatial index\n",
    "gpkg_path = cache_dir / \"bld_with_attr_compact_ucm2.gpkg\"\n",
    "\n",
    "# ensure output directory exists\n",
    "gpkg_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pyogrio.write_dataframe(\n",
    "    g_selected, \n",
    "    gpkg_path, \n",
    "    layer=\"bld_attr_ucm\",                                                # EDITED: distinct layer name\n",
    "    driver=\"GPKG\", \n",
    "    layer_options={\"SPATIAL_INDEX\": \"NO\"},\n",
    ")\n",
    "print(f\"Saved: {gpkg_path}\")                                             # EDITED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950388a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved shapefile to: E:\\London\\colouringbritain\\data-extract-2025-09-01\\cache\\bld_with_attr_compact_ucm.shp\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# # --- Make a safe copy so we don’t mutate the original GeoDataFrame ---\n",
    "# g_out = g_selected.copy()\n",
    "\n",
    "# # --- Loop through each column and normalize its dtype ---\n",
    "# for col in g_out.columns:\n",
    "#     if col == g_out.geometry.name:  # Skip the geometry column (leave as is)\n",
    "#         continue\n",
    "#     s = g_out[col]\n",
    "\n",
    "#     # 1) Integers (including nullable Int16/Int32) → plain int32\n",
    "#     #    Shapefile/DBF only supports fixed-width numeric fields, no NA.\n",
    "#     #    So: convert to numeric, replace NaN with -1, then cast to int32.\n",
    "#     if pd.api.types.is_integer_dtype(s):\n",
    "#         g_out[col] = pd.to_numeric(s, errors=\"coerce\").fillna(-1).astype(np.int32)\n",
    "\n",
    "#     # 2) Floats → float64\n",
    "#     #    Shapefile supports floats, so keep as float64 for maximum compatibility.\n",
    "#     elif pd.api.types.is_float_dtype(s):\n",
    "#         g_out[col] = pd.to_numeric(s, errors=\"coerce\").astype(np.float64)\n",
    "\n",
    "#     # 3) Booleans → int8\n",
    "#     #    DBF doesn’t have a native boolean type; store as 0/1 integers.\n",
    "#     elif pd.api.types.is_bool_dtype(s):\n",
    "#         g_out[col] = s.astype(np.int8)\n",
    "\n",
    "#     # 4) Datetime → string\n",
    "#     #    Shapefile DBF doesn’t handle full pandas datetimes well.\n",
    "#     #    Convert to human-readable text (YYYY-MM-DD HH:MM:SS).\n",
    "#     elif pd.api.types.is_datetime64_any_dtype(s):\n",
    "#         g_out[col] = s.dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "#     # 5) Everything else (object, string, category, mixed) → string\n",
    "#     #    Ensure missing values stay as None (DBF null), not \"nan\" text.\n",
    "#     else:\n",
    "#         g_out[col] = s.astype(str).replace(\"nan\", None)\n",
    "\n",
    "# # --- Save as ESRI Shapefile ---\n",
    "# # Shapefile consists of .shp/.shx/.dbf/.prj/.cpg, all created automatically.\n",
    "# shp_path = cache_dir / \"bld_with_attr_compact_ucm.shp\"\n",
    "# g_out.to_file(shp_path, driver=\"ESRI Shapefile\", encoding=\"utf-8\")\n",
    "\n",
    "# print(f\"Saved shapefile to: {shp_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d85538e",
   "metadata": {},
   "source": [
    "### data sumamry - total area by buidling type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d23a1807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import geopandas as gpd\n",
    "# import pandas as pd\n",
    "# from pathlib import Path\n",
    "\n",
    "# # --- paths ---\n",
    "# gpkg_path = cache_dir / \"bld_with_attr_compact.gpkg\"\n",
    "\n",
    "# # --- load GeoPackage ---\n",
    "# g_small = gpd.read_file(gpkg_path, layer=\"bld_attr\")\n",
    "# print(\"Loaded\", len(g_small), \"features\")\n",
    "# print(\"Columns:\", g_small.columns.tolist())\n",
    "\n",
    "# # --- ensure CRS in meters (BNG for London) ---\n",
    "# if g_small.crs is None or g_small.crs.to_epsg() != 27700:\n",
    "#     g_small = g_small.to_crs(27700)\n",
    "\n",
    "# # --- calculate area ---\n",
    "# g_small[\"area_m2\"] = g_small.geometry.area\n",
    "\n",
    "# # --- detect land use column ---\n",
    "# landuse_col = None\n",
    "# if \"landuse_mode\" in g_small.columns:\n",
    "#     landuse_col = \"landuse_mode\"\n",
    "# elif \"landuse_mode_code\" in g_small.columns:\n",
    "#     landuse_col = \"landuse_mode_code\"\n",
    "# else:\n",
    "#     # create placeholder column if missing\n",
    "#     landuse_col = \"landuse_mode\"\n",
    "#     g_small[landuse_col] = pd.NA\n",
    "\n",
    "# # --- aggregate total area by land use ---\n",
    "# area_by_landuse = (\n",
    "#     g_small.groupby(landuse_col, dropna=True, observed=True, as_index=False)[\"area_m2\"]\n",
    "#            .sum()\n",
    "#            .sort_values(\"area_m2\", ascending=False)\n",
    "# )\n",
    "\n",
    "# # --- save CSV ---\n",
    "# out_csv = cache_dir / \"bld_with_attr_landuse_total_area_m2.csv\"\n",
    "# area_by_landuse.to_csv(out_csv, index=False)\n",
    "\n",
    "# print(area_by_landuse.head())\n",
    "# print(f\"Saved totals to: {out_csv}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
