{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d707702f",
   "metadata": {},
   "source": [
    "## Package and dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7fc1d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Environment Info ===\n",
      "Python version : 3.11.13\n",
      "Python env     : c:\\Users\\pc\\.conda\\envs\\geo_env\\python.exe\n",
      "Platform       : Windows-10-10.0.22631-SP0\n",
      "numpy          : 1.26.4\n",
      "pandas         : 2.3.2\n",
      "geopandas      : 0.14.4\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "# %pip install pyarrow\n",
    "# %pip install --upgrade pandas\n",
    "# %pip install xlsxwriter\n",
    "\n",
    "\n",
    "import pyarrow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "import sys, platform\n",
    "print(\"=== Environment Info ===\")\n",
    "print(f\"Python version : {sys.version.split()[0]}\")\n",
    "print(f\"Python env     : {sys.executable}\")\n",
    "print(f\"Platform       : {platform.platform()}\")\n",
    "print(f\"numpy          : {np.__version__}\")\n",
    "print(f\"pandas         : {pd.__version__}\")\n",
    "print(f\"geopandas      : {gpd.__version__}\")\n",
    "print(\"========================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb614768",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the file path\n",
    "folder = Path(r\"E:\\London\\colouringbritain\\data-extract-2025-09-01\")\n",
    "csv_file = folder / \"building_attributes.csv\"\n",
    "\n",
    "# Subfolder for cached files\n",
    "cache_dir = folder / \"cache\"\n",
    "cache_dir.mkdir(exist_ok=True)   # create if not exists\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca807cd1",
   "metadata": {},
   "source": [
    "## Merge building polygons with attributes data from points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee0ee99",
   "metadata": {},
   "source": [
    "### 1. reload building attributes - points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e97e316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.feather as feather\n",
    "\n",
    "# cache_dir = Path(r\"C:\\Users\\yingjiel\\data\\colouringbritain\")  # --> when using SDSS remote desktop\n",
    "\n",
    "out_feather = cache_dir / \"building_attributes_subset.feather\"\n",
    "df_building_subset = feather.read_feather(out_feather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3be2b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['building_id', 'ref_toid', 'ref_osm_id', 'location_name', 'location_town', 'location_postcode', 'location_latitude', 'location_longitude', 'current_landuse_group', 'current_landuse_order', 'building_attachment_form', 'date_change_building_use', 'date_year', 'size_storeys_attic', 'size_storeys_core', 'size_storeys_basement', 'size_height_apex', 'size_floor_area_ground', 'size_floor_area_total', 'sust_breeam_rating', 'sust_dec', 'sust_retrofit_date', 'construction_core_material', 'construction_secondary_materials', 'construction_roof_covering', 'is_domestic', 'context_front_garden', 'context_back_garden', 'context_flats_garden', 'context_green_space_distance', 'context_green_space_distance', 'context_tree_distance', 'age_retrofit_date']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# df_building_subset.head()\n",
    "print(df_building_subset.columns.tolist())\n",
    "\n",
    "# # Find duplicates based on building_id\n",
    "# duplicates = df_building_subset[df_building_subset.duplicated(subset=\"building_id\", keep=False)]\n",
    "# # Optionally, sort for easier inspection\n",
    "# duplicates = duplicates.sort_values(by=\"building_id\")\n",
    "# print(duplicates)\n",
    "\n",
    "cols_pts_selected = ['building_id', 'location_postcode', \n",
    "                     'location_latitude', 'location_longitude', \n",
    "                     'current_landuse_group', 'current_landuse_order',\n",
    "                     'sust_breeam_rating', 'sust_dec', 'sust_retrofit_date',\n",
    "                     ]\n",
    "\n",
    "building_points = df_building_subset[cols_pts_selected].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57f60211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10180876, 9)\n",
      "(10180876, 9)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Drop duplicate rows across all columns\n",
    "building_points_nodup = building_points.drop_duplicates()               # step 2: drop duplicate rows\n",
    "\n",
    "print(building_points.shape)\n",
    "print(building_points_nodup.shape) # --> the same shape, so no duplicate rows\n",
    "\n",
    "\n",
    "# Auto-rename duplicated columns (if any) by appending _1, _2, etc.\n",
    "def dedup_columns(df):\n",
    "    seen = {}\n",
    "    new_cols = []\n",
    "    for col in df.columns:\n",
    "        if col not in seen:\n",
    "            seen[col] = 0\n",
    "            new_cols.append(col)\n",
    "        else:\n",
    "            seen[col] += 1\n",
    "            new_cols.append(f\"{col}_{seen[col]}\")\n",
    "    df.columns = new_cols\n",
    "    return df\n",
    "\n",
    "building_points = dedup_columns(building_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba45f2b",
   "metadata": {},
   "source": [
    "### convert to point shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d968a278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make points GeoDataFrame from lat/lon\n",
    "building_points_df = building_points.dropna(subset=[\"location_latitude\", \"location_longitude\"]).copy()\n",
    "\n",
    "building_pts = gpd.GeoDataFrame(\n",
    "    building_points_df,\n",
    "    geometry=gpd.points_from_xy(\n",
    "        x=building_points_df[\"location_longitude\"],\n",
    "        y=building_points_df[\"location_latitude\"],\n",
    "    ),\n",
    "    crs=\"EPSG:4326\",  # WGS84  # <<< CHANGED (move crs to GeoDataFrame for clarity)\n",
    ")\n",
    "\n",
    "\n",
    "# # Save as GeoPackage -- will take 13 mins\n",
    "# gpkg_path = cache_dir / \"building_pts.gpkg\"\n",
    "# building_pts.to_file(gpkg_path, driver=\"GPKG\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56884594",
   "metadata": {},
   "source": [
    "### 2. reload building polygon  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17ed8072",
   "metadata": {},
   "outputs": [],
   "source": [
    "buildings_parquet = cache_dir / \"buildings_clean.feather\"\n",
    "\n",
    "# Reload instantly\n",
    "building_poly = gpd.read_feather(buildings_parquet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10d4a9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fid', 'area', 'perimeter', 'geometry']\n",
      "(2223481, 4)\n"
     ]
    }
   ],
   "source": [
    "# print(building_poly.head())\n",
    "print(building_poly.columns.tolist())\n",
    "\n",
    "print(building_poly.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0853cdbf",
   "metadata": {},
   "source": [
    "### spatial join \n",
    "\n",
    "It might be poosible one polygon contains multiple points -- need to find a way to address this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a8c9b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPSG:4326\n",
      "{\"$schema\": \"https://proj.org/schemas/v0.7/projjson.schema.json\", \"type\": \"ProjectedCRS\", \"name\": \"OSGB36 / British National Grid\", \"base_crs\": {\"name\": \"OSGB36\", \"datum\": {\"type\": \"GeodeticReferenceFrame\", \"name\": \"Ordnance Survey of Great Britain 1936\", \"ellipsoid\": {\"name\": \"Airy 1830\", \"semi_major_axis\": 6377563.396, \"inverse_flattening\": 299.3249646}}, \"coordinate_system\": {\"subtype\": \"ellipsoidal\", \"axis\": [{\"name\": \"Geodetic latitude\", \"abbreviation\": \"Lat\", \"direction\": \"north\", \"unit\": \"degree\"}, {\"name\": \"Geodetic longitude\", \"abbreviation\": \"Lon\", \"direction\": \"east\", \"unit\": \"degree\"}]}, \"id\": {\"authority\": \"EPSG\", \"code\": 4277}}, \"conversion\": {\"name\": \"British National Grid\", \"method\": {\"name\": \"Transverse Mercator\", \"id\": {\"authority\": \"EPSG\", \"code\": 9807}}, \"parameters\": [{\"name\": \"Latitude of natural origin\", \"value\": 49, \"unit\": \"degree\", \"id\": {\"authority\": \"EPSG\", \"code\": 8801}}, {\"name\": \"Longitude of natural origin\", \"value\": -2, \"unit\": \"degree\", \"id\": {\"authority\": \"EPSG\", \"code\": 8802}}, {\"name\": \"Scale factor at natural origin\", \"value\": 0.9996012717, \"unit\": \"unity\", \"id\": {\"authority\": \"EPSG\", \"code\": 8805}}, {\"name\": \"False easting\", \"value\": 400000, \"unit\": \"metre\", \"id\": {\"authority\": \"EPSG\", \"code\": 8806}}, {\"name\": \"False northing\", \"value\": -100000, \"unit\": \"metre\", \"id\": {\"authority\": \"EPSG\", \"code\": 8807}}]}, \"coordinate_system\": {\"subtype\": \"Cartesian\", \"axis\": [{\"name\": \"Easting\", \"abbreviation\": \"E\", \"direction\": \"east\", \"unit\": \"metre\"}, {\"name\": \"Northing\", \"abbreviation\": \"N\", \"direction\": \"north\", \"unit\": \"metre\"}]}, \"scope\": \"Engineering survey, topographic mapping.\", \"area\": \"United Kingdom (UK) - offshore to boundary of UKCS within 49\\u00b045'N to 61\\u00b0N and 9\\u00b0W to 2\\u00b0E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\", \"bbox\": {\"south_latitude\": 49.75, \"west_longitude\": -9.01, \"north_latitude\": 61.01, \"east_longitude\": 2.01}, \"id\": {\"authority\": \"EPSG\", \"code\": 27700}}\n",
      "EPSG:27700\n",
      "EPSG:27700\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "print(building_pts.crs)\n",
    "print(building_poly.crs)\n",
    "\n",
    "\n",
    "# Reproject both GeoDataFrames\n",
    "# For London, use a projected CRS in meters. The two best choices:\n",
    "#   EPSG:27700 — OSGB 1936 / British National Grid (BNG)\n",
    "#   Standard for Great Britain; great local distance accuracy around London.\n",
    "building_pts = building_pts.to_crs(epsg=27700)\n",
    "building_poly = building_poly.to_crs(epsg=27700)\n",
    "\n",
    "print(building_pts.crs)\n",
    "print(building_poly.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e159efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2841826, 14)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Pick the right predicate:\n",
    "#    - \"contains\" : point must be strictly inside polygon (boundary points excluded)\n",
    "#    - \"covers\"   : includes points on polygon boundary (often safer)\n",
    "predicate = \"covers\"\n",
    "\n",
    "bld_with_pts = gpd.sjoin(\n",
    "    building_poly,   # LEFT: keep all buildings\n",
    "    building_pts,    # RIGHT: bring point attrs in where they fall inside\n",
    "    how=\"left\",\n",
    "    predicate=predicate,\n",
    "    lsuffix=\"bld\",\n",
    "    rsuffix=\"pt\",\n",
    ")\n",
    "\n",
    "print(bld_with_pts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb2acb2",
   "metadata": {},
   "source": [
    "`sjoin` is a row-wise join.\n",
    "\n",
    "If one polygon covers multiple points, then the polygon row is **repeated** once for each matching point.\n",
    "\n",
    "So you’ll get one row per (polygon, point) pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55cda2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fid          area  perimeter  \\\n",
      "2  3.0  4.776945e-09   0.000326   \n",
      "3  4.0  5.876390e-08   0.002627   \n",
      "3  4.0  5.876390e-08   0.002627   \n",
      "3  4.0  5.876390e-08   0.002627   \n",
      "3  4.0  5.876390e-08   0.002627   \n",
      "\n",
      "                                            geometry   index_pt  building_id  \\\n",
      "2  POLYGON ((530958.680 200117.570, 530962.290 20...  2981433.0    1164790.0   \n",
      "3  POLYGON ((526208.730 198238.610, 526212.740 19...  3787818.0     937225.0   \n",
      "3  POLYGON ((526208.730 198238.610, 526212.740 19...  4497128.0    3546140.0   \n",
      "3  POLYGON ((526208.730 198238.610, 526212.740 19...  2926106.0     956250.0   \n",
      "3  POLYGON ((526208.730 198238.610, 526212.740 19...  4040057.0    1003486.0   \n",
      "\n",
      "  location_postcode  location_latitude  location_longitude  \\\n",
      "2                             51.68447            -0.10702   \n",
      "3                             51.66841            -0.17639   \n",
      "3                             51.66847            -0.17635   \n",
      "3                             51.66848            -0.17594   \n",
      "3                             51.66866            -0.17636   \n",
      "\n",
      "  current_landuse_group current_landuse_order sust_breeam_rating sust_dec  \\\n",
      "2                  None           Residential                               \n",
      "3                  None           Residential                               \n",
      "3                  None                                                     \n",
      "3                  None           Residential                               \n",
      "3                  None           Residential                               \n",
      "\n",
      "   sust_retrofit_date  \n",
      "2                 NaN  \n",
      "3                 NaN  \n",
      "3                 NaN  \n",
      "3                 NaN  \n",
      "3                 NaN  \n"
     ]
    }
   ],
   "source": [
    "# print(bld_with_pts.columns.tolist())\n",
    "# print(building_poly.columns.tolist())\n",
    "# print(bld_with_pts.head())\n",
    "\n",
    "# Identify points that didn't match any building\n",
    "pts_matched = bld_with_pts[bld_with_pts[\"building_id\"].notna()]  # building_id from points that matched\n",
    "print(pts_matched.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9be8f0",
   "metadata": {},
   "source": [
    "#### Identify records that one polygon covers multiple points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0e7ab48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fid          area  perimeter  \\\n",
      "3  4.0  5.876390e-08   0.002627   \n",
      "3  4.0  5.876390e-08   0.002627   \n",
      "3  4.0  5.876390e-08   0.002627   \n",
      "3  4.0  5.876390e-08   0.002627   \n",
      "3  4.0  5.876390e-08   0.002627   \n",
      "\n",
      "                                            geometry   index_pt  building_id  \\\n",
      "3  POLYGON ((526208.730 198238.610, 526212.740 19...  3787818.0     937225.0   \n",
      "3  POLYGON ((526208.730 198238.610, 526212.740 19...  4497128.0    3546140.0   \n",
      "3  POLYGON ((526208.730 198238.610, 526212.740 19...  2926106.0     956250.0   \n",
      "3  POLYGON ((526208.730 198238.610, 526212.740 19...  4040057.0    1003486.0   \n",
      "3  POLYGON ((526208.730 198238.610, 526212.740 19...  3937069.0     927964.0   \n",
      "\n",
      "  location_postcode  location_latitude  location_longitude  \\\n",
      "3                             51.66841            -0.17639   \n",
      "3                             51.66847            -0.17635   \n",
      "3                             51.66848            -0.17594   \n",
      "3                             51.66866            -0.17636   \n",
      "3                             51.66871            -0.17597   \n",
      "\n",
      "  current_landuse_group current_landuse_order sust_breeam_rating sust_dec  \\\n",
      "3                  None           Residential                               \n",
      "3                  None                                                     \n",
      "3                  None           Residential                               \n",
      "3                  None           Residential                               \n",
      "3                  None           Residential                               \n",
      "\n",
      "   sust_retrofit_date  pts_count  \n",
      "3                 NaN          5  \n",
      "3                 NaN          5  \n",
      "3                 NaN          5  \n",
      "3                 NaN          5  \n",
      "3                 NaN          5  \n"
     ]
    }
   ],
   "source": [
    "# add a count of how many points each polygon covers\n",
    "bld_with_pts[\"pts_count\"] = bld_with_pts.groupby(bld_with_pts.index)[\"building_id\"].transform(\"count\")\n",
    "\n",
    "# filter polygons that cover more than 1 point\n",
    "polys_multi_pts = bld_with_pts[bld_with_pts[\"pts_count\"] > 1]\n",
    "\n",
    "print(polys_multi_pts.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca74fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Choose a stable polygon key\n",
    "poly_key = \"fid\"\n",
    "pts_key = \"building_id\"\n",
    "\n",
    "# helpers\n",
    "def mode_or_na(s: pd.Series):\n",
    "    \"\"\"\n",
    "    Return the most frequent (mode) value in a pandas Series.\n",
    "    - Drop missing values first.\n",
    "    - If the Series is not empty after dropping NaN, return the first mode.\n",
    "      (pandas mode() can return multiple values if there are ties, but\n",
    "       we just take the first to keep it simple.)\n",
    "    - If the Series is empty, return pandas.NA (missing).\n",
    "    \"\"\"\n",
    "    s = s.dropna()\n",
    "    return s.mode().iloc[0] if not s.empty else pd.NA\n",
    "\n",
    "def uniques_sorted(s: pd.Series):\n",
    "    \"\"\"\n",
    "    Return a sorted list of unique, non-missing values from a pandas Series.\n",
    "    - Drop missing values first.\n",
    "    - Use pandas.unique() to extract unique values in order of appearance.\n",
    "    - Sort the list so the output is consistent across runs.\n",
    "    - If the Series is empty, return an empty list [].\n",
    "    \"\"\"\n",
    "    s = s.dropna()\n",
    "    return sorted(pd.unique(s)) if not s.empty else []\n",
    "\n",
    "\n",
    "# Alternative mode function that breaks ties by first appearance\n",
    "def mode_safe(s: pd.Series):\n",
    "    # returns most frequent non-null value; if tie, pick first by appearance\n",
    "    s = s.dropna()\n",
    "    if s.empty:\n",
    "        return pd.NA\n",
    "    counts = s.value_counts()\n",
    "    # all values tied for max frequency\n",
    "    top = counts.index[counts.eq(counts.max())]\n",
    "    # preserve original order for tie-break\n",
    "    for v in s:\n",
    "        if v in set(top):\n",
    "            return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e4fb3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3) aggregate to one row per polygon\n",
    "grp = bld_with_pts.groupby(poly_key, dropna=False)\n",
    "agg_pts = grp.agg(\n",
    "    point_count          = (pts_key, \"count\"),                        # number of matched points\n",
    "    landuse_mode         = (\"current_landuse_order\", mode_or_na),     # most common land use\n",
    "    landuse_unique       = (\"current_landuse_order\", uniques_sorted), # all unique land uses\n",
    "    breeam_mode          = (\"sust_breeam_rating\", mode_or_na),        # most common BREEAM rating\n",
    "    breeam_unique        = (\"sust_breeam_rating\", uniques_sorted),    # all unique BREEAM ratings\n",
    ").reset_index()\n",
    "\n",
    "# 4) merge back to polygons so geometry is included\n",
    "polys_unique = building_poly.copy()\n",
    "if poly_key not in polys_unique.columns:\n",
    "    polys_unique = polys_unique.reset_index().rename(columns={\"index\": poly_key})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd91c9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fid  point_count landuse_mode   landuse_unique breeam_mode breeam_unique\n",
      "0  1.0            0         <NA>               []        <NA>            []\n",
      "1  2.0            0         <NA>               []        <NA>            []\n",
      "2  3.0            1  Residential    [Residential]                        []\n",
      "3  4.0            5  Residential  [, Residential]                        []\n",
      "4  5.0            0         <NA>               []        <NA>            []\n",
      "(2223481, 6)\n",
      "(2223481, 4)\n"
     ]
    }
   ],
   "source": [
    "print(agg_pts.head())\n",
    "print(agg_pts.shape)\n",
    "print(polys_unique.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65e37225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fid          area  perimeter  \\\n",
      "0  1.0  3.503437e-09   0.000266   \n",
      "1  2.0  3.830134e-09   0.000308   \n",
      "2  3.0  4.776945e-09   0.000326   \n",
      "3  4.0  5.876390e-08   0.002627   \n",
      "4  5.0  2.593630e-09   0.000232   \n",
      "\n",
      "                                            geometry  point_count  \\\n",
      "0  POLYGON ((530850.030 200467.340, 530853.140 20...            0   \n",
      "1  POLYGON ((530946.580 200101.170, 530942.780 20...            0   \n",
      "2  POLYGON ((530958.680 200117.570, 530962.290 20...            1   \n",
      "3  POLYGON ((526208.730 198238.610, 526212.740 19...            5   \n",
      "4  POLYGON ((524271.910 197156.219, 524277.781 19...            0   \n",
      "\n",
      "  landuse_mode   landuse_unique breeam_mode breeam_unique  \n",
      "0         <NA>               []        <NA>            []  \n",
      "1         <NA>               []        <NA>            []  \n",
      "2  Residential    [Residential]                        []  \n",
      "3  Residential  [, Residential]                        []  \n",
      "4         <NA>               []        <NA>            []  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "merge1 = polys_unique.merge(agg_pts, on=poly_key, how=\"left\")\n",
    "merge1 = gpd.GeoDataFrame(merge1, geometry=\"geometry\", crs=building_poly.crs)\n",
    "\n",
    "# merge1: one row per polygon with counts, modes, and unique lists\n",
    "print(merge1.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7969e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2223481, 9)\n"
     ]
    }
   ],
   "source": [
    "print(merge1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8663518e",
   "metadata": {},
   "source": [
    "#### Fallback nearest join for unmatched points\n",
    "\n",
    "(1) finds which points were already matched by your primary join, \n",
    "\n",
    "(2) computes a nearest polygon only for the unmatched points (in meters), and \n",
    "\n",
    "(3) returns a per-point assignment table you can merge anywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e213ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Matched points: 2632484\n",
      "\n",
      " Unmatched points: 5693084\n"
     ]
    }
   ],
   "source": [
    "# 1) Points matched by the primary join (some polygons may repeat; we just need the set)\n",
    "matched_pts = set(bld_with_pts[pts_key].dropna().unique())\n",
    "print(f\"\\n Matched points: {len(matched_pts)}\")\n",
    "\n",
    "# 2) Split points into matched / unmatched\n",
    "unmatched_pts = building_pts[~building_pts[pts_key].isin(matched_pts)].copy()\n",
    "print(f\"\\n Unmatched points: {len(unmatched_pts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2afd786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fid                                           geometry\n",
      "0    1.0  POLYGON ((530850.030 200467.340, 530853.140 20...\n",
      "1    2.0  POLYGON ((530946.580 200101.170, 530942.780 20...\n",
      "4    5.0  POLYGON ((524271.910 197156.219, 524277.781 19...\n",
      "10  11.0  POLYGON ((523756.723 197059.965, 523761.078 19...\n",
      "12  13.0  POLYGON ((526317.830 197736.050, 526317.720 19...\n"
     ]
    }
   ],
   "source": [
    "# print(merge1.head())\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Assumptions:\n",
    "# - `merge1` is already built from the PRIMARY polygons-left join (\"covers\")\n",
    "#   and contains per-polygon fields like `point_count`, plus mode/unique cols.\n",
    "# - We now only handle polygons with point_count == 0 via NEAREST fallback.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# 1) Find polygons that did not get any matched points in the primary join\n",
    "no_match_polys = merge1.loc[merge1[\"point_count\"].fillna(0).eq(0), [poly_key, \"geometry\"]]\n",
    "attr_cols = [\"current_landuse_order\", \"sust_breeam_rating\",\n",
    "             \"sust_dec\", \"sust_retrofit_date\"]\n",
    "print(no_match_polys.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75d8cd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fid', 'geometry', 'point_count', 'landuse_mode', 'landuse_unique', 'breeam_mode', 'breeam_unique', 'nearest_dist_m', 'match_method', 'point_count_fb', 'nearest_dist_m_fb', 'landuse_mode_fb', 'landuse_unique_fb', 'breeam_mode_fb', 'breeam_unique_fb']\n",
      "   fid  point_count match_method nearest_dist_m landuse_mode breeam_mode\n",
      "0  1.0          1.0      nearest       0.605724  Residential            \n",
      "1  2.0          1.0      nearest       0.707167  Residential            \n",
      "2  3.0          1.0          pip           <NA>  Residential            \n",
      "3  4.0          5.0          pip           <NA>  Residential            \n",
      "4  5.0          1.0      nearest       1.694663  Residential            \n"
     ]
    }
   ],
   "source": [
    "# constants for clarity / reruns\n",
    "BNG_EPSG = 27700       # British National Grid (meters)\n",
    "MAX_DIST = 5           # meters in BNG; tune as needed\n",
    "\n",
    "if not no_match_polys.empty:\n",
    "    # 2) Project both layers for distance-based join (London → EPSG:27700)\n",
    "    poly_m = no_match_polys.to_crs(BNG_EPSG) # British National Grid (meters)\n",
    "    pts_m  = building_pts.to_crs(BNG_EPSG)\n",
    "\n",
    "    # 3) Nearest join: polygon-left to pull attributes from closest point\n",
    "    #    Nearest join (polygons-left, without k=1) → may return multiple matches per polygon\n",
    "    nearest = gpd.sjoin_nearest(\n",
    "        poly_m,\n",
    "        pts_m[[pts_key, \"geometry\"] + attr_cols],\n",
    "        how=\"left\",\n",
    "        distance_col=\"nearest_dist_m\",\n",
    "        max_distance=MAX_DIST,\n",
    "        # k=1, # intentionally omitted to allow ties at equal distance\n",
    "        lsuffix=\"bld\",\n",
    "        rsuffix=\"pt\",\n",
    "    ).to_crs(no_match_polys.crs)\n",
    "\n",
    "    # 4) Aggregate fallback attributes per polygon\n",
    "    #    Because we may now have >1 nearest point at equal distance,\n",
    "    #    we summarize all their attributes.\n",
    "    fallback_agg = nearest.groupby(poly_key).agg(\n",
    "        point_count    = (pts_key, \"count\"),               # count of tied nearest points\n",
    "        nearest_dist_m = (\"nearest_dist_m\", \"min\"),        # min = actual nearest distance\n",
    "        landuse_mode   = (\"current_landuse_order\", mode_safe),\n",
    "        landuse_unique = (\"current_landuse_order\", uniques_sorted),\n",
    "        breeam_mode    = (\"sust_breeam_rating\", mode_safe),\n",
    "        breeam_unique  = (\"sust_breeam_rating\", uniques_sorted),\n",
    "    ).reset_index()\n",
    "\n",
    "    # Identify polygons that truly received a nearest match (within MAX_DIST)\n",
    "    fb_ids = set(fallback_agg.loc[fallback_agg[\"point_count\"].fillna(0) > 0, poly_key])\n",
    "\n",
    "\n",
    "    # # Mark source\n",
    "    # fallback_agg[\"match_method\"] = \"nearest\"\n",
    "\n",
    "    # 5) Update `merge1`: attach fallback columns (keep primary values!)\n",
    "    # Ensure primary columns exist so combine_first works even if they were absent in merge1\n",
    "    for col in [\"point_count\", \"nearest_dist_m\",\n",
    "                \"landuse_mode\", \"landuse_unique\", \"breeam_mode\", \"breeam_unique\"]:\n",
    "        if col not in merge1.columns:\n",
    "            merge1[col] = pd.NA\n",
    "\n",
    "    # Q: Should we add `match_method` now?\n",
    "    # A: Yes — add it before merging and default to \"pip\" for polygons\n",
    "    #    that had any primary points; leave as NA for those with none.\n",
    "    if \"match_method\" not in merge1.columns:\n",
    "        merge1[\"match_method\"] = pd.NA\n",
    "        # mark polygons that had primary matches as \"pip\"\n",
    "        merge1.loc[merge1[\"point_count\"].fillna(0) > 0, \"match_method\"] = \"pip\"\n",
    "\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # Merge fallback summaries as *_fb columns (do NOT drop primary columns) \n",
    "    # ----------------------------------------------------------------------------\n",
    "    merge2 = merge1.merge(\n",
    "        fallback_agg,               # contains fallback summaries\n",
    "        on=poly_key,\n",
    "        how=\"left\",\n",
    "        suffixes=(\"\", \"_fb\")        # fallback columns carry \"_fb\"; primary keep base name\n",
    "    )\n",
    "\n",
    "    print(merge2.columns.tolist())\n",
    "\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # For polygons that had primary matches, keep original values.\n",
    "    # For polygons with no primary (point_count == 0 or NA), fill from fallback (_fb).\n",
    "    # --------------------------------------------------------------------------\n",
    "    import numpy as np\n",
    "\n",
    "    # build a mask where we should use fallback values:\n",
    "    # - primary point_count is 0 or NA\n",
    "    # - AND fallback point_count_fb > 0 (i.e., a nearest point was actually found)\n",
    "    pc_primary  = merge2.get(\"point_count\")\n",
    "    pc_fallback = merge2.get(\"point_count_fb\")\n",
    "    use_fb = pc_primary.fillna(0).eq(0) & pc_fallback.fillna(0).gt(0)\n",
    "\n",
    "    # 1) point_count: replace 0/NA with fallback count where use_fb is True\n",
    "    if \"point_count_fb\" in merge2.columns:\n",
    "        merge2[\"point_count\"] = np.where(use_fb, merge2[\"point_count_fb\"], merge2[\"point_count\"])\n",
    "        merge2.drop(columns=[\"point_count_fb\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    # 2) other attributes: fill from fallback ONLY where use_fb is True; keep primary otherwise\n",
    "    for col in [\"nearest_dist_m\", \n",
    "                \"landuse_mode\", \"landuse_unique\", \"breeam_mode\", \"breeam_unique\"]:\n",
    "        fb_col = col + \"_fb\"\n",
    "        if fb_col in merge2.columns:\n",
    "            # np.where(use_fb, merge2[fb_col], merge2[col]) works row by row:\n",
    "            # - If the polygon had no primary match (use_fb == True),\n",
    "            #   take the fallback value from the *_fb column.\n",
    "            # - Otherwise, keep the original primary value from the base column.\n",
    "            # This ensures we only overwrite for polygons that really need fallback.\n",
    "            merge2[col] = np.where(use_fb, merge2[fb_col], merge2[col])\n",
    "\n",
    "            # Drop the temporary fallback column once we've merged its values in.\n",
    "            # This keeps the final dataframe clean (only the unified column remains).\n",
    "            merge2.drop(columns=[fb_col], inplace=True)\n",
    "\n",
    "    \n",
    "    # Update match_method:\n",
    "    # - If a polygon got a valid nearest match (in fb_ids) AND its current method is NA,\n",
    "    #   label it \"nearest\".\n",
    "    # - If it already was \"pip\", keep it as \"pip\" (primary wins).\n",
    "    if \"match_method\" not in merge2.columns:\n",
    "        merge2[\"match_method\"] = pd.NA\n",
    "    mask_nearest_fill = merge2[poly_key].isin(fb_ids) & merge2[\"match_method\"].isna()\n",
    "    merge2.loc[mask_nearest_fill, \"match_method\"] = \"nearest\"\n",
    "\n",
    "\n",
    "    # --- final enriched polygons (merge2) ---\n",
    "    print(\n",
    "        merge2[\n",
    "            [poly_key, \"point_count\", \"match_method\", \"nearest_dist_m\",\n",
    "             \"landuse_mode\", \"breeam_mode\"]\n",
    "        ].head()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bede5d",
   "metadata": {},
   "source": [
    "The above code has merged the primary and fallback data\n",
    "\n",
    "- merge1 → polygons with attributes from the primary join (covers)\n",
    "\n",
    "- merge2 → same polygons, but with fallback nearest values filled for those that had no points in the primary join → The final complete data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d390645e",
   "metadata": {},
   "source": [
    "### check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dac91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2223481, 9)\n",
      "(2223481, 9)\n"
     ]
    }
   ],
   "source": [
    "print(merge1.shape)\n",
    "print(merge2.shape)\n",
    "\n",
    "\n",
    "# merge1 = merge1.drop(columns=[\"area\", \"perimeter\"], errors=\"ignore\")\n",
    "# merge2 = merge2.drop(columns=[\"area\", \"perimeter\"], errors=\"ignore\")\n",
    "\n",
    "\n",
    "# print(merge1.head())\n",
    "# print(merge2.head())\n",
    "\n",
    "\n",
    "# print(merge1.columns)\n",
    "# print(merge2.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21c21ac",
   "metadata": {},
   "source": [
    "### save data\n",
    "\n",
    "`.copy()` is important: without it, bld_with_pts = merge2 just creates a second reference to the same object. Any edits to one would also change the other.\n",
    "\n",
    "With .copy(), you get an independent object that you can modify safely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce2f8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bld_with_pts = merge2.copy()\n",
    "\n",
    "\n",
    "# 1. Save as Feather (very fast reload in Python, good for local caching) -- will take 12 seconds\n",
    "feather_path = cache_dir / \"bld_with_pts.feather\"\n",
    "bld_with_pts.to_feather(feather_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27f1557",
   "metadata": {},
   "source": [
    "GeoPackage fields can’t store Python lists/dicts, so Fiona raises that error. Convert those columns to something GPKG understands (e.g., TEXT via JSON) or explode them before writing.\n",
    "\n",
    "Here’s a drop-in fixer that:\n",
    "- detects non-scalar columns (list/dict/set/tuple/ndarray),\n",
    "- converts them to JSON strings (keeping None as null),\n",
    "- then writes the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62bb04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_9548\\831226030.py:41: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  elif pd.api.types.is_categorical_dtype(s):\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_9548\\831226030.py:41: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  elif pd.api.types.is_categorical_dtype(s):\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_9548\\831226030.py:41: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  elif pd.api.types.is_categorical_dtype(s):\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_9548\\831226030.py:41: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  elif pd.api.types.is_categorical_dtype(s):\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_9548\\831226030.py:41: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  elif pd.api.types.is_categorical_dtype(s):\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_9548\\831226030.py:41: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  elif pd.api.types.is_categorical_dtype(s):\n"
     ]
    }
   ],
   "source": [
    "import json, numpy as np, pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "def sanitize_for_gpkg(gdf: gpd.GeoDataFrame):\n",
    "    g = gdf.copy()\n",
    "    geom = g.geometry.name\n",
    "\n",
    "    # 1) Rename reserved PK names\n",
    "    # avoid writing the pandas index as a field\n",
    "    # rename any user column named 'fid' (and similar reserved names)\n",
    "    # common reserved PK names in GPKG\n",
    "    reserved = {\"fid\", \"ogc_fid\", \"rowid\"}\n",
    "    rename_map = {c: f\"{c}_attr\" for c in g.columns if c.lower() in reserved}\n",
    "    if rename_map:\n",
    "        g = g.rename(columns=rename_map)\n",
    "\n",
    "    # 2) Convert non-scalar values (list/dict/array/tuple/set) to JSON TEXT\n",
    "    def is_non_scalar(v):\n",
    "        return isinstance(v, (list, dict, tuple, set, np.ndarray))\n",
    "\n",
    "    def to_text(v):\n",
    "        if v is None or (isinstance(v, float) and pd.isna(v)):\n",
    "            return None\n",
    "        if is_non_scalar(v):\n",
    "            try:\n",
    "                return json.dumps(v, ensure_ascii=False)\n",
    "            except Exception:\n",
    "                return str(v)\n",
    "        return v\n",
    "\n",
    "    for c in g.columns:\n",
    "        if c == geom:\n",
    "            continue\n",
    "        if g[c].map(is_non_scalar).any():\n",
    "            g[c] = g[c].map(to_text).astype(\"string\")\n",
    "\n",
    "    # 3) Normalize dtypes to OGR-friendly ones\n",
    "    for c in g.columns:\n",
    "        if c == geom:\n",
    "            continue\n",
    "        s = g[c]\n",
    "        if pd.api.types.is_datetime64_any_dtype(s):\n",
    "            g[c] = s.dt.strftime(\"%Y-%m-%d %H:%M:%S\")  # TEXT\n",
    "        elif pd.api.types.is_categorical_dtype(s):\n",
    "            g[c] = s.astype(\"string\")\n",
    "        elif s.dtype == \"object\":\n",
    "            g[c] = s.astype(\"string\")\n",
    "        elif pd.api.types.is_bool_dtype(s):\n",
    "            g[c] = s.astype(\"int8\")  # or keep bool; int is safest\n",
    "        elif pd.api.types.is_integer_dtype(s):\n",
    "            g[c] = s.astype(\"int64\")\n",
    "        elif pd.api.types.is_float_dtype(s):\n",
    "            g[c] = s.astype(\"float64\")\n",
    "\n",
    "    # Safety check: ensure no reserved names remain\n",
    "    assert not any(c.lower() in reserved for c in g.columns), \"Reserved field name still present.\"\n",
    "\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adc6329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Save as GeoPackage (portable, works with GIS software like QGIS/ArcGIS) -- will take 13 mins\n",
    "gpkg_path = cache_dir / \"bld_with_pts.gpkg\"\n",
    "\n",
    "# ---- use it ---- \n",
    "g = sanitize_for_gpkg(bld_with_pts) \n",
    "g.to_file(gpkg_path, driver=\"GPKG\", layer=\"bld_with_pts\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
