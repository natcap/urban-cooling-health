{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory:\n",
      " d:\\natcap\\urban-cooling-health\\code\n",
      "Parent folder:\n",
      " d:\\natcap\\urban-cooling-health\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define file path\n",
    "file1_path = r\"G:/Shared drives/Wellcome Trust Project Data/0_source_data/GiGL land use data/GiGL_Trees_font_point/GiGL_GLATrees_Pre2023.shp\"\n",
    "file2_path = r\"G:/Shared drives/Wellcome Trust Project Data/0_source_data/GiGL land use data/GiGL_Trees_font_point/GiGL_GLATrees_2023-24.shp\"\n",
    "\n",
    "# Define output path\n",
    "current_path = os.getcwd()\n",
    "print(\"Current working directory:\\n\", current_path)\n",
    "\n",
    "parent_folder = os.path.dirname(current_path)\n",
    "print(\"Parent folder:\\n\", parent_folder)\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Where to save\n",
    "out_dir = Path(parent_folder) / \"data\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # Read the spatial data one by one\n",
    "# d = gpd.read_file(file1_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine tree data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import geopandas as gpd\n",
    "# import pandas as pd\n",
    "# from pathlib import Path\n",
    "\n",
    "# # --- inputs ---\n",
    "# paths = [file1_path, file2_path]  # str or Path\n",
    "# out_path = Path(output_path)         # e.g., \"merged.gpkg\"\n",
    "# out_layer = out_path.stem         # gpkg layer name\n",
    "\n",
    "# # ---- read all, tag source ----\n",
    "# gdfs = []\n",
    "# for p in paths:\n",
    "#     p = Path(p)\n",
    "#     gdf = gpd.read_file(p, engine=\"pyogrio\")\n",
    "#     # ensure geometry column exists\n",
    "#     if \"geometry\" not in gdf.columns:\n",
    "#         raise ValueError(f\"{p} has no geometry column.\")\n",
    "#     gdf[\"source_file\"] = p.name\n",
    "#     gdfs.append(gdf)\n",
    "\n",
    "# if not gdfs:\n",
    "#     raise ValueError(\"No input files read.\")\n",
    "\n",
    "# # ---- choose a target CRS ----\n",
    "# # prefer the first non-None CRS; you can also set a known CRS explicitly\n",
    "# target_crs = next((g.crs for g in gdfs if g.crs is not None), None)\n",
    "\n",
    "# # ---- reproject (only when safe) ----\n",
    "# aligned = []\n",
    "# for g in gdfs:\n",
    "#     if target_crs is None:\n",
    "#         # leave CRS as-is; you may want to set one: g.set_crs(\"EPSG:5070\", inplace=True)\n",
    "#         aligned.append(g)\n",
    "#     elif g.crs is None:\n",
    "#         # if one file lacks CRS but you *know* it matches target_crs, set it:\n",
    "#         g = g.set_crs(target_crs, allow_override=False)\n",
    "#         aligned.append(g)\n",
    "#     elif g.crs != target_crs:\n",
    "#         aligned.append(g.to_crs(target_crs))\n",
    "#     else:\n",
    "#         aligned.append(g)\n",
    "\n",
    "# # ---- align schemas (union of columns across all) ----\n",
    "# all_cols = sorted(set().union(*[set(g.columns) for g in aligned]))\n",
    "# # always ensure 'geometry' is present and last for readability\n",
    "# all_cols = [c for c in all_cols if c != \"geometry\"] + [\"geometry\"]\n",
    "\n",
    "# for i, g in enumerate(aligned):\n",
    "#     # reindex may cast geometry -> object; we re-assert below\n",
    "#     aligned[i] = g.reindex(columns=all_cols)\n",
    "#     aligned[i] = gpd.GeoDataFrame(aligned[i], geometry=\"geometry\", crs=target_crs)\n",
    "\n",
    "# # ---- optional: clean geometries ----\n",
    "# try:\n",
    "#     from shapely import make_valid\n",
    "#     for i, g in enumerate(aligned):\n",
    "#         g[\"geometry\"] = make_valid(g[\"geometry\"])\n",
    "# except Exception:\n",
    "#     # fallback: simple fix for some invalids (not all cases)\n",
    "#     for i, g in enumerate(aligned):\n",
    "#         g[\"geometry\"] = g.buffer(0)\n",
    "\n",
    "# # drop empties if needed\n",
    "# for i, g in enumerate(aligned):\n",
    "#     aligned[i] = g[~g.geometry.is_empty & g.geometry.notnull()]\n",
    "\n",
    "# # ---- concatenate ----\n",
    "# combined = pd.concat(aligned, ignore_index=True, sort=False)\n",
    "# combined = gpd.GeoDataFrame(combined, geometry=\"geometry\", crs=target_crs)\n",
    "\n",
    "# # ---- sanity checks (optional) ----\n",
    "# # print(combined.geom_type.value_counts(dropna=False))\n",
    "# # print(combined.crs)\n",
    "\n",
    "# # ---- write (GeoPackage) ----\n",
    "# combined.to_file(out_path, driver=\"GPKG\", layer=out_layer)\n",
    "# print(f\"Wrote {len(combined)} features to {out_path} (layer='{out_layer}')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tree species at risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the tree list that will be at risk \n",
    "f = r\"D:\\natcap\\urban-cooling-health\\data\\tree_list_GiGL_combined_2050.csv\"\n",
    "\n",
    "at_risk = pd.read_csv(f)\n",
    "\n",
    "## left join the full data\n",
    "trees_at_risk = d.merge(at_risk[['TaxonName', 'country', 'city', 'family', 'year']], \n",
    "                        how=\"left\", on=\"TaxonName\")\n",
    "\n",
    "\n",
    "print(at_risk.columns)\n",
    "print(trees_at_risk.columns)\n",
    "\n",
    "# Step 2: Keep only columns from df + geometry\n",
    "cols_to_keep = list(at_risk.columns) + ['geometry']\n",
    "gdf_trimmed = trees_at_risk[cols_to_keep]\n",
    "\n",
    "\n",
    "# Step 3: Save as new shapefile\n",
    "gdf_trimmed.to_file(output_path)\n",
    "\n",
    "print(f\"Saved merged shapefile to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\natcap\\urban-cooling-health\\data\\GiGL_GLATrees_Pre2023_risk_2050.shp\n",
      "d:\\natcap\\urban-cooling-health\\data\\GiGL_GLATrees_2023-24_risk_2050.shp\n"
     ]
    }
   ],
   "source": [
    "## Load all the saved data\n",
    "\n",
    "f1 = os.path.join(parent_folder, \"data\", os.path.basename(file1_path).replace('.shp', '') + \"_risk_2050.shp\"); print(f1)\n",
    "f2 = os.path.join(parent_folder, \"data\", os.path.basename(file2_path).replace('.shp', '') + \"_risk_2050.shp\"); print(f2)\n",
    "\n",
    "\n",
    "# Read the spatial data\n",
    "tree_risk_layer1 = gpd.read_file(f1, engine=\"pyogrio\")\n",
    "tree_risk_layer2 = gpd.read_file(f2, engine=\"pyogrio\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 1050054 features to d:\\natcap\\urban-cooling-health\\data\\GiGL_GLATrees_at_risk_2050_merged.gpkg (layer='GiGL_GLATrees_at_risk_2050_merged')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# You already have:\n",
    "# f1, f2\n",
    "# tree_risk_layer1 = gpd.read_file(f1, engine=\"pyogrio\")\n",
    "# tree_risk_layer2 = gpd.read_file(f2, engine=\"pyogrio\")\n",
    "\n",
    "g1 = tree_risk_layer1.copy()\n",
    "g2 = tree_risk_layer2.copy()\n",
    "\n",
    "# Tag source\n",
    "g1[\"source_file\"] = Path(f1).name\n",
    "g2[\"source_file\"] = Path(f2).name\n",
    "\n",
    "# Pick a target CRS (prefer the first non-None)\n",
    "target_crs = g1.crs or g2.crs\n",
    "\n",
    "# Align CRS\n",
    "def _align_crs(g, target):\n",
    "    if target is None:\n",
    "        return g\n",
    "    if g.crs is None:\n",
    "        return g.set_crs(target)\n",
    "    if g.crs != target:\n",
    "        return g.to_crs(target)\n",
    "    return g\n",
    "\n",
    "g1 = _align_crs(g1, target_crs)\n",
    "g2 = _align_crs(g2, target_crs)\n",
    "\n",
    "# Union schema (keep geometry last)\n",
    "all_cols = [c for c in sorted(set(g1.columns) | set(g2.columns)) if c != \"geometry\"] + [\"geometry\"]\n",
    "g1 = gpd.GeoDataFrame(g1.reindex(columns=all_cols), geometry=\"geometry\", crs=target_crs)\n",
    "g2 = gpd.GeoDataFrame(g2.reindex(columns=all_cols), geometry=\"geometry\", crs=target_crs)\n",
    "\n",
    "# Fix invalid geometries (best effort)\n",
    "try:\n",
    "    from shapely import make_valid\n",
    "    g1[\"geometry\"] = make_valid(g1.geometry)\n",
    "    g2[\"geometry\"] = make_valid(g2.geometry)\n",
    "except Exception:\n",
    "    g1[\"geometry\"] = g1.buffer(0)\n",
    "    g2[\"geometry\"] = g2.buffer(0)\n",
    "\n",
    "# Drop empty/null geoms\n",
    "g1 = g1[g1.geometry.notnull() & ~g1.geometry.is_empty]\n",
    "g2 = g2[g2.geometry.notnull() & ~g2.geometry.is_empty]\n",
    "\n",
    "# Concatenate\n",
    "combined = gpd.GeoDataFrame(\n",
    "    pd.concat([g1, g2], ignore_index=True, sort=False),\n",
    "    geometry=\"geometry\",\n",
    "    crs=target_crs\n",
    ")\n",
    "\n",
    "# (Optional) drop exact duplicates (including geometry)\n",
    "# combined = combined.drop_duplicates(subset=[c for c in combined.columns if c != \"geometry\"] + [\"geometry\"])\n",
    "\n",
    "# Save\n",
    "out_path = os.path.join(parent_folder, \"data\", \"GiGL_GLATrees_at_risk_2050_merged.gpkg\")\n",
    "layer_name = Path(out_path).stem  # \"GiGL_GLATrees_at_risk_2050_merged\"\n",
    "\n",
    "combined.to_file(out_path, driver=\"GPKG\", layer=layer_name)\n",
    "print(f\"Wrote {len(combined)} features to {out_path} (layer='{layer_name}')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year year_label   count  percent\n",
      "0  2050       2050  657622    62.63\n",
      "1  <NA>    Unknown  392432    37.37\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- make 'year' sortable & consistent ---\n",
    "if 'year' not in combined.columns:\n",
    "    raise KeyError(\"'year' column not found in combined\")\n",
    "\n",
    "# if it's datetime-like, grab the year; else coerce to numeric\n",
    "if pd.api.types.is_datetime64_any_dtype(combined['year']):\n",
    "    combined['year'] = combined['year'].dt.year.astype('Int64')\n",
    "else:\n",
    "    combined['year'] = pd.to_numeric(combined['year'], errors='coerce').astype('Int64')\n",
    "\n",
    "# --- summarize ---\n",
    "summary = (\n",
    "    combined\n",
    "    .groupby('year', dropna=False)\n",
    "    .size()\n",
    "    .rename('count')\n",
    "    .reset_index()\n",
    "    .sort_values('year', na_position='last')\n",
    ")\n",
    "\n",
    "total_n = len(combined)\n",
    "summary['percent'] = (summary['count'] / total_n * 100).round(2)\n",
    "\n",
    "# for display, keep a nice label while preserving numeric 'year'\n",
    "summary['year_label'] = summary['year'].astype(str).replace('<NA>', 'Unknown')\n",
    "\n",
    "print(summary[['year', 'year_label', 'count', 'percent']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10m grid\n",
    "\n",
    "Turn the 1 m × 1 m grid to a 10 m × 10 m grid -- snap cells to a 10 m lattice and aggregate. This avoids creating 100 overlapping squares per tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 777007 tiles to d:\\natcap\\urban-cooling-health\\data\\tiles10.gpkg (layer='tiles10')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "\n",
    "gdf = combined.copy()\n",
    "\n",
    "# 0) Ensure CRS is metric (meters)\n",
    "if gdf.crs is None or not gdf.crs.is_projected:\n",
    "    raise ValueError(\"Project to a metric CRS first (e.g., EPSG:5070 or UTM).\")\n",
    "\n",
    "# 1) Define 10 m grid origin (snap to a multiple of 10 m)\n",
    "cell = 10.0\n",
    "xmin, ymin, xmax, ymax = gdf.total_bounds\n",
    "x0 = np.floor(xmin / cell) * cell\n",
    "y0 = np.floor(ymin / cell) * cell\n",
    "\n",
    "# 2) Compute grid indices for each 1 m cell using its lower-left corner\n",
    "b = gdf.geometry.bounds  # DataFrame with minx, miny, maxx, maxy\n",
    "i = np.floor((b[\"minx\"] - x0) / cell).astype(\"int64\")\n",
    "j = np.floor((b[\"miny\"] - y0) / cell).astype(\"int64\")\n",
    "gdf[\"i\"] = i\n",
    "gdf[\"j\"] = j\n",
    "\n",
    "# 3) Build unique 10 m tiles as polygons\n",
    "keys = gdf[[\"i\", \"j\"]].drop_duplicates().reset_index(drop=True)\n",
    "xs = x0 + keys[\"i\"].to_numpy() * cell\n",
    "ys = y0 + keys[\"j\"].to_numpy() * cell\n",
    "geoms = [box(x, y, x + cell, y + cell) for x, y in zip(xs, ys)]\n",
    "tiles10 = gpd.GeoDataFrame(keys, geometry=geoms, crs=gdf.crs)\n",
    "\n",
    "# 4) (Optional) carry aggregations to 10 m grid\n",
    "# Count how many 1 m cells fell into each 10 m tile (0–100, typically <=100 if partial coverage)\n",
    "counts = gdf.groupby([\"i\", \"j\"]).size().rename(\"n_cells\").reset_index()\n",
    "tiles10 = tiles10.merge(counts, on=[\"i\", \"j\"], how=\"left\")\n",
    "\n",
    "# Example: majority year per 10 m tile (if you have a 'year' column)\n",
    "if \"year\" in gdf.columns:\n",
    "    per_year = gdf.groupby([\"i\", \"j\", \"year\"]).size().rename(\"n\").reset_index()\n",
    "    idx = per_year.groupby([\"i\", \"j\"])[\"n\"].idxmax()\n",
    "    majority_year = per_year.loc[idx, [\"i\", \"j\", \"year\"]]\n",
    "    tiles10 = tiles10.merge(majority_year, on=[\"i\", \"j\"], how=\"left\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Best option: GeoPackage ---\n",
    "gpkg_path = out_dir / \"GiGL_GLATrees_at_risk_2050_merged_tiles10.gpkg\"\n",
    "layer_name = \"tiles10\"\n",
    "\n",
    "# keep grid keys as ints\n",
    "t10 = tiles10.copy()\n",
    "for c in (\"i\", \"j\"):\n",
    "    if c in t10.columns:\n",
    "        t10[c] = t10[c].astype(\"int64\")\n",
    "\n",
    "t10.to_file(gpkg_path, driver=\"GPKG\", layer=layer_name)  # engine=\"pyogrio\" also OK\n",
    "print(f\"Wrote {len(t10)} tiles to {gpkg_path} (layer='{layer_name}')\")\n",
    "\n",
    "# --- (Optional) Shapefile (use short field names, 32-bit ints) ---\n",
    "# shp_path = out_dir / \"tiles10_shp\" / \"tiles10.shp\"\n",
    "# (out_dir / \"tiles10_shp\").mkdir(exist_ok=True)\n",
    "# t_shp = t10.copy()\n",
    "# for c in (\"i\", \"j\"):\n",
    "#     if c in t_shp.columns:\n",
    "#         t_shp[c] = t_shp[c].astype(\"int32\")\n",
    "# t_shp.to_file(shp_path, driver=\"ESRI Shapefile\")\n",
    "# print(f\"Also wrote Shapefile to {shp_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate lc scenario - tree at risk\n",
    "\n",
    "only changes LC pixels that (a) overlap the risk polygons and (b) are currently one of your tree codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated land cover raster saved at: G:\\Shared drives\\Wellcome Trust Project Data\\1_preprocess\\UrbanCoolingModel\\EP_preliminary_tests\\clipped_lulc\\UKECH\\LCM2021_london_scenario3_TreeRisk.tif\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.features import rasterize\n",
    "\n",
    "\n",
    "## Load shapefile with tree cover will be at risk or loss\n",
    "shp_tree_risk = out_dir / \"GiGL_GLATrees_at_risk_2050_merged_tiles10.gpkg\"\n",
    "tree_cover_at_risk = gpd.read_file(shp_tree_risk)\n",
    "\n",
    "## Filter the tree cover data with year = 2050, which indicate these trees will be at risk by 2050\n",
    "tree_cover_at_risk = tree_cover_at_risk[tree_cover_at_risk['year'].isin([2050])]\n",
    "\n",
    "## land cover data\n",
    "lc_input = r\"G:\\Shared drives\\Wellcome Trust Project Data\\1_preprocess\\UrbanCoolingModel\\EP_preliminary_tests\\clipped_lulc\\UKECH\\LCM2021_london.tif\"\n",
    "lc_output = lc_input.replace(\".tif\", \"_scenario3_TreeRisk.tif\")\n",
    "tree_cover_code = [1, 2]\n",
    "after_risk_lc_code = 5  # 5: 'Neutral Grassland',\n",
    "\n",
    "\n",
    "\n",
    "# Ensure output folder exists\n",
    "Path(os.path.dirname(lc_output)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Load and filter polygons ---\n",
    "tree_cover_at_risk = gpd.read_file(shp_tree_risk)\n",
    "tree_cover_at_risk = tree_cover_at_risk[tree_cover_at_risk[\"year\"].isin([2050])]\n",
    "tree_cover_at_risk = tree_cover_at_risk[\n",
    "    tree_cover_at_risk.geometry.notna() & ~tree_cover_at_risk.geometry.is_empty\n",
    "]\n",
    "\n",
    "# --- Read raster ---\n",
    "with rasterio.open(lc_input) as src:\n",
    "    meta      = src.meta.copy()\n",
    "    transform = src.transform\n",
    "    lc_crs    = src.crs\n",
    "    nodata    = src.nodata\n",
    "    arr       = src.read(1)  # 2D array of land cover codes\n",
    "\n",
    "# Reproject polygons to raster CRS if needed\n",
    "if tree_cover_at_risk.crs and (tree_cover_at_risk.crs != lc_crs):\n",
    "    tree_cover_at_risk = tree_cover_at_risk.to_crs(lc_crs)\n",
    "\n",
    "\n",
    "# --- Rasterize risk polygons to a mask (1 = at risk) ---\n",
    "shape_mask = rasterize(\n",
    "    [(geom, 1) for geom in tree_cover_at_risk.geometry],\n",
    "    out_shape=arr.shape,\n",
    "    transform=transform,\n",
    "    fill=0,\n",
    "    dtype=\"uint8\",\n",
    "    # all_touched=True,  # uncomment if you want a slightly more inclusive burn-in\n",
    ")\n",
    "\n",
    "# --- Build masks ---\n",
    "valid_mask = np.ones_like(arr, dtype=bool)\n",
    "if nodata is not None:\n",
    "    valid_mask &= (arr != nodata)\n",
    "\n",
    "# If LC==0 is background you want to preserve, keep it out of edits:\n",
    "valid_mask &= (arr != 0)\n",
    "\n",
    "# Only modify cells that are both \"at risk\" and currently a tree class\n",
    "tree_mask = np.isin(arr, tree_cover_code)\n",
    "target_mask = (shape_mask == 1) & tree_mask & valid_mask\n",
    "\n",
    "# --- Apply scenario ---\n",
    "remapped = arr.copy()\n",
    "remapped[target_mask] = after_risk_lc_code\n",
    "\n",
    "# --- Save updated raster ---\n",
    "meta_out = meta.copy()\n",
    "# Keep original dtype unless you explicitly want uint8\n",
    "meta_out.update(\n",
    "    dtype=remapped.dtype,\n",
    "    compress=\"lzw\",\n",
    "    nodata=nodata\n",
    ")\n",
    "\n",
    "with rasterio.open(lc_output, \"w\", **meta_out) as dst:\n",
    "    dst.write(remapped, 1)\n",
    "\n",
    "print(f\"Updated land cover raster saved at: {lc_output}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
