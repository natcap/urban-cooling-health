---
title: "Untitled"
output: html_document
date: "2025-11-20"
---



## Load data

```{r}

## To clear your environment
# remove(list = ls())

library(sf)
library(tmap)
library(tidyverse)
library(stringr)

library(ggplot2)
library(tmap)


library(conflicted)
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")


sf::st_drivers()[sf::st_drivers()$name == "ESRI Shapefile", ]


## AOI layer
dir_g <- "G:/Shared drives/Wellcome Trust Project Data/1_preprocess/UrbanCoolingModel/OfficialWorkingInputs/AOIs"
f_aoi <- file.path(dir_g, 'London_Borough_aoi.shp')
aoi <- st_read(f_aoi, quiet = T)

ymd   <- format(Sys.time(), "%Y%m%d"); ymd
```


### equity data

```{r}
## data
f <- './data/Social_Vulnerability_Index_london.gpkg'

# --- metrics ---
metrics <- c(
  "pIncomeDeprivation",
  "p5under",
  "p75over",
  "pNotEnglishProficient",
  "pSocial_housing",
  "pBAME"
)


svi_lsoa <- st_read(f) %>%
  # ----- pooled quintiles for all EP_* variables -----
  mutate(
    across(
      all_of(metrics),
      ~ {
        brks <- quantile(.x, probs = seq(0, 1, by = 0.2), na.rm = TRUE, type = 7)
        brks <- unique(brks)                       # handle duplicate quantiles
        if (length(brks) < 2) {
          # all values identical or mostly NA: return all-NA factor with Q1–Q5 levels
          factor(NA_real_, levels = paste0("Q", 1:5))
        } else {
          labs <- paste0("Q", seq_len(length(brks) - 1))
          cut(.x, breaks = brks, include.lowest = TRUE, labels = labs)
        }
      },
      .names = "{.col}_q"
    )
  )

names(svi_lsoa)

## save a copy for other use 
f <- './data/Social_Vulnerability_Index_london_q.gpkg'
st_write(svi_lsoa, f,delete_dsn = TRUE)



svi_lsoa_subset <- svi_lsoa %>%
  ## remove percentile cols 
  dplyr::select(-ends_with('_pct'))


# 2. Calculate quintiles and add them as a new column -- the same result as above
# We use ntile() from dplyr for easy splitting, or cut() with quantile() for base R
df <- svi_lsoa %>%
  mutate(quintile = factor(ntile(pIncomeDeprivation, 5)))
```


```{r}

svi_lsoa_subset %>%
  ggplot(., aes(x = pIncomeDeprivation, fill = pIncomeDeprivation_q)) +
  geom_histogram(binwidth = 2, color = "white", alpha = 0.8) +
  scale_fill_brewer(palette = "Spectral", direction = -1) +
  labs(title = "Histogram Colored by Quintile",
       x = "Value",
       y = "Count",
       fill = "Quintile") +
  theme_minimal()




# 2. Reshape, Group, and Calculate Quintiles
p_data <- svi_lsoa_subset %>%
  select(-starts_with('mean_')) %>%
  select(-starts_with('overall_')) %>%
  select(-ends_with('_q')) %>%
  # Select only the columns you want + pivot them to long format
  pivot_longer(cols = all_of(metrics), 
               names_to = "Metric", 
               values_to = "Value") %>%
  # Group by the Metric name so quintiles are calculated per column
  group_by(Metric) %>%
  mutate(Quintile = factor(ntile(Value, 5))) %>%
  ungroup()

# 3. Plot
ggplot(p_data, aes(x = Value, fill = Quintile)) +
  geom_histogram(
    # binwidth = 2,
    # bins = 30, 
    color = "white", alpha = 0.8) +
  # Create a separate panel for each metric
  facet_wrap(~Metric, scales = "free") + 
  scale_fill_brewer(palette = "Spectral", direction = -1) +
  labs(title = "Distribution of Demographics by Quintile",
       x = "Percentage / Value",
       y = "Count") +
  theme_minimal() +
  theme(legend.position = "bottom")

```




### health data

```{r}

## load data 

dir_ucm_out <- 'G:/Shared drives/Wellcome Trust Project Data/2_postprocess_intermediate/UCM_official_runs'


## --> update this input data
# f <- file.path(dir_ucm_out, 'health_output_stats', 'health_london_18_zonal_stats_long.csv'); post_fix <- '00_2021_2050'; 
f <- file.path(dir_ucm_out, 'health_output_stats', 'health_london_18_zonal_stats_long_LSOA_2050_2050.csv'); post_fix <- 'i0_2050_2050'

health_df <- read_csv(f, show_col_types = F) %>%
  mutate(
    indicator = gsub('\\.tif|Excess_', '', raster),
    lc_scenario = gsub('health_output_', '', lc_scenario),
    lc_scenario = gsub('_2050', '', lc_scenario),
    indicator = ifelse(indicator == 'self_harm', "suicide", indicator),
    indicator = str_to_title(indicator)
  ) %>%
  
  # mutate(climate = str_extract(raster, "\\d+deg_\\d+uhi")) %>%
  # mutate(
  #    climate = case_when(
  #      indicator < 2030 ~ 'current',
  #      T ~ 'future'
  #    )
  #    ) %>%
  ## re-code: change scenario-3 (tree risk) as scenario-2
  mutate(
    lc_scenario = gsub('s0_', '', lc_scenario),
    lc_scenario = case_when(
      lc_scenario == 's1'  ~ 'scenario1',
      lc_scenario == 's2'  ~ 'scenario3_TO',
      lc_scenario == 's3'  ~ 'scenario2_TR',
      lc_scenario == 's41' ~ 'scenario4_10',
      lc_scenario == 's42' ~ 'scenario4_20',
      lc_scenario == 's43' ~ 'scenario4_30',
      TRUE                 ~ NA_character_
    ) 
  ) %>%
  rename('cases' = 'sum') %>%
  
  ## convert "Excess deaths" to "Preventable deaths" to indicate "benefits" of nature
  mutate(cases = -cases) %>%
  
  
  select(any_of(c("id", "Neighborhood_name","cases", "lc_scenario", "indicator")))

names(health_df)

## add sf 
health_sf <- health_df %>%
  left_join(svi_lsoa_subset, ., by = 'id')
```




## Plot 

### data for plot

```{r cont. }

outcome_ind <- 'Cardiovascular'

# lc_snr <- 'scenario1'
# lc_snr <- 'scenario2_TR'
# lc_snr <- 'scenario4_10'
lc_snr <- 'scenario4_30'


df_cont <- health_sf %>%
  filter(indicator == outcome_ind) %>%
  filter(lc_scenario == lc_snr) %>%
  select(cases, all_of(metrics))

```



```{r categorical }

# 1. Reshape the data from Wide to Long
df_cat <- health_sf %>%
  select(cases, ends_with("_q"), lc_scenario, indicator) %>%   # Select 'cases' and all columns ending in '_q'
  pivot_longer(
    cols = ends_with("_q"),            # Take all the quintile columns
    names_to = "Metric",               # New column for the variable name (e.g., "pIncomeDeprivation_q")
    values_to = "Quintile"             # New column for the values ("Q1", "Q5", etc.)
  ) %>%
  # Optional: Clean the Metric names by removing "_q" for nicer plot titles
  mutate(Metric = gsub("_q", "", Metric))

# 2. Plot
df_cat2 <- df_cat %>%
  st_drop_geometry() %>%
  filter(indicator == outcome_ind) %>%
  filter(lc_scenario == lc_snr) 
```


 Which one to map? 
  
Figure 1 Layout:
  Panel A: Map of pBAME vs Cases.
  Caption: "Ethnicity and Cases: High divergence in low-vulnerability areas."
  Panel B: Map of p75over vs Cases.
  Caption: "Age and Cases: Co-location of risks in high-vulnerability areas."

How to select the "Best" representative? 
  - If you aren't sure which of the socioeconomic variables to pick (pBAME, Income, Housing), pick the one with the strongest spatial clustering or the one most relevant to local policy.
  
```{r collinearity }

# Check correlation among your metrics

library(dplyr)
library(ggcorrplot)

# 1. Select only the numeric metric columns
# We also include 'cases' to see which metrics correlate most strongly with the outcome
corr_vars <- 
  # df_scaled %>%
  df_cont %>% st_drop_geometry() %>%
  select(all_of(metrics), cases)

# 2. Compute the correlation matrix
# use = "complete.obs" handles missing values by ignoring rows with NAs
cor_matrix <- cor(corr_vars, use = "complete.obs", method = "pearson")

# 3. Compute P-values and FORCE TO MATRIX
p_mat <- ggcorrplot::cor_pmat(corr_vars)
p_mat <- as.matrix(p_mat)  # <--- This fixes the "Subscript" error

# 4. Plot
ggcorrplot(cor_matrix,
           method = "square",
           type = "lower",
           lab = TRUE,
           lab_size = 3,
           p.mat = p_mat,          # Now passing a safe matrix
           sig.level = 0.05,
           insig = "blank",
           colors = c("#d73027", "white", "#2c7bb6"),
           title = "Correlation Matrix",
           ggtheme = theme_minimal(base_size = 10)) +
  theme(
    axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
    panel.grid.major = element_blank()
  )

f <- paste0('./figures/', 'health_outcome_equity_cor.png')
ggsave(plot = last_plot(), filename = f, width = 7, height = 5, units = 'in', dpi = 300)
```

```{r update SVI metrics}

## due to the high correlation, we decided to exclude some of the variables 

metrics_to_remove <- c(
  # "pIncomeDeprivation",
  # "p5under",
  # "p75over",
  "pNotEnglishProficient",
  "pSocial_housing"
  # "pBAME"
)

## update the metrics list 
metrics <- metrics[!(metrics %in% metrics_to_remove)]

df_cont <- df_cont %>%
  select(-any_of(metrics_to_remove))


df_cat2 <- df_cat2 %>%
  filter(!Metric %in% metrics_to_remove)
```


### boxplot

```{r data}

# Calculate the typical range (e.g., 5th to 95th percentile) to determine zoom limits automatically
lower_bound <- quantile(df_cat2$cases, 0.05, na.rm = TRUE)
upper_bound <- quantile(df_cat2$cases, 0.95, na.rm = TRUE)
  
# 1. Define a function to calculate N and position it
stat_box_data <- function(y) {
  return( 
    data.frame(
      y = max(y, na.rm = T) * 1.05,  # Position: Just above the max value (whisker or outlier)
      label = paste0("n = ", length(y)) # Label: "n = 123"
    )
  )
}

stat_box_data_bottom <- function(y) {
  return( 
    data.frame(
      y = min(y, na.rm = T) - (max(y, na.rm = T) - min(y, na.rm = T)) * 0.05, # Just below min
      label = length(y) # simple number "123" (cleaner for bottom axis)
    )
  )
}


library(ggpubr)

df_cat2 %>%
  ggplot(., aes(x = Quintile, y = cases, fill = Quintile)) +
  # geom_boxplot(outlier.alpha = 0.5) +
  geom_boxplot(outlier.shape = NA) +  # Hides the outlier points
  
  # # Add the sample size text -- not needed as the sample size among groups are similar 
  # stat_summary(
  #   fun.data = stat_box_data,
  #   hjust = 0.5,
  #   vjust = -0.5,
  #   
  #   # fun.data = stat_box_data_bottom, 
  #   geom = "text", 
  #   size = 3 # Adjust text size
  # ) +
  
  
  # 1. Add Global P-value (Is there a difference overall?)
  stat_compare_means(method = "kruskal.test", label.y.npc = "top") + 
  
  # 2. Add Pairwise Comparisons (Optional - can get messy with 5 groups)
  stat_compare_means(comparisons = list(c("Q1", "Q5")), label = "p.signif") +
  
  
  scale_fill_brewer(palette = "Spectral", direction = -1) +
  coord_cartesian(ylim = c(lower_bound, upper_bound)) + # Zooms the view without deleting data
  facet_wrap(~Metric, scales = "free") +
  scale_y_continuous(expand = c(0, 0.02)) +
  labs(title = "Distribution of preventable cases by quintile",
       y = "Preventable cases", 
       x = "Quintile") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 0, hjust = 1)) # Angled text helps if labels are long


# 2. Pairwise Test (Dunn's Test or Pairwise Wilcoxon)
# If the global test is significant, check Q1 vs Q5 specifically:
pairwise_results <- df_cat2 %>%
  group_by(Metric) %>%
  summarise(
    # Compare Q1 vs Q5 (assuming specific interest in the gap)
    q1_vs_q5_p = wilcox.test(cases[Quintile == "Q1"], cases[Quintile == "Q5"])$p.value
  )
```



```{r brackets, eval=FALSE, include=FALSE}

library(ggpubr)  # For plotting p-values
library(rstatix) # For calculating the tests easily

# 1. Run the Pairwise Wilcoxon Test
# We group by 'Metric' so the test is done separately for each panel
stat_test <- df_cat2 %>%
  group_by(Metric) %>%
  wilcox_test(cases ~ Quintile) %>%
  adjust_pvalue(method = "bonferroni") %>% # Adjust for multiple comparisons to avoid false positives
  add_significance() %>%                   # Adds stars (*, **, ns)
  filter(p.adj < 0.05) %>%                 # <--- THIS LINE removes non-significant pairs
  add_xy_position(x = "Quintile", fun = 'median_iqr', step.increase = 1) # Auto-calculates bracket height

# 2. Plot
ggplot(df_cat2, aes(x = Quintile, y = cases, fill = Quintile)) +
  
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  
  geom_boxplot(outlier.shape = NA) +
  
  # Add the p-values manually using the dataframe we created above
  stat_pvalue_manual(
    stat_test, 
    color = 'gray',
    label = "p.adj.signif", # Use stars (*, **) instead of numbers
    tip.length = 0.01,      # Length of the bracket tips
    hide.ns = TRUE          # Double check to ensure NS are hidden
  ) +
  
  facet_wrap(~Metric, scales = "free_y") +
  
  coord_cartesian(ylim = c(lower_bound, upper_bound)) + # Zooms the view without deleting data
  # Important: Add extra space at the top for the brackets
  scale_y_continuous(expand = expansion(mult = c(0.05, 0.5))) +
  
  scale_fill_brewer(palette = "Spectral", direction = -1) +
  labs(title = "Significant Pairwise Differences by Quintile",
       subtitle = "Only significant comparisons (p.adj < 0.05) are shown",
       y = "Cases") +
  theme_minimal() +
  theme(legend.position = "none")

# f <- paste0('./figures/', 'health_outcome_equity_q_brackets_', lc_snr, '.png')
# ggsave(plot = last_plot(), filename = f, width = 7, height = 5, units = 'in', dpi = 300)
```

  To use the Compact Letter Display (CLD) method (where *groups sharing the same letter are not significantly different*), you need to combine a statistical test with the multcompView package.
  Since you are likely using non-parametric data (given the previous discussion on Quintiles/Cases), here is the robust workflow using *Dunn’s Test* (the post-hoc test for Kruskal-Wallis) to generate the letters.
  
```{r cld}

library(multcompView)
library(rstatix)


# 1. Define a helper function to generate letters for one metric
get_cld_letters <- function(data) {
  # Run Dunn's Test (Non-parametric pairwise)
  dunn_res <- dunn_test(data, cases ~ Quintile, p.adjust.method = "bonferroni")
  
  # Format p-values for multcompView ("Group1-Group2" = p.val)
  p_values <- setNames(dunn_res$p.adj, paste(dunn_res$group1, dunn_res$group2, sep = "-"))
  
  # Convert p-values to Letters
  letters_list <- multcompLetters(p_values)$Letters
  
  # Return as a dataframe
  return(data.frame(Quintile = names(letters_list), Letter = letters_list))
}

# 2. Apply this function to every Metric group
cld_data <- df_cat2 %>%
  group_by(Metric) %>%
  # Nest data to run the function per group
  nest() %>%
  mutate(cld = purrr::map(data, get_cld_letters)) %>%
  unnest(cld) %>%
  select(Metric, Quintile, Letter)

# 3. Calculate Y-positions (Just above the highest outlier per group)
# This ensures the letter sits nicely on top of the boxplot
y_positions <- df_cat2 %>%
  group_by(Metric, Quintile) %>%
  summarise(
    # max_y = max(cases, na.rm = TRUE), 
    max_y = quantile(cases, 0.9, na.rm = TRUE),  # 95th percentile
    q3  = quantile(cases, 0.75 + 0.05, na.rm = TRUE), # add a bit more for better viz
    iqr = IQR(cases, na.rm = TRUE),
    .groups = "drop") %>%
  # Add a little buffer (5% of height) so the letter doesn't touch the whisker
  mutate(
    upper_cap = q3 + 1.5 * iqr,
    max_y     = q3,
    y_pos = max_y * 1.05) 

# 4. Merge Letters with Positions
final_labels <- left_join(cld_data, y_positions, by = c("Metric", "Quintile"))

# Check the result
head(final_labels)



## Step 3: Plot with Letters
ggplot(df_cat2, aes(x = Quintile, y = cases, fill = Quintile)) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  geom_boxplot(
               # outlier.shape = NA,
               outlier.shape = 1, outlier.size = 1, 
               outlier.alpha = 0.2) + # Show outliers lightly
  
  # Add the Letters
  geom_text(data = final_labels, 
            aes(x = Quintile, y = y_pos, label = Letter), 
            vjust = 0, # Sits right on the y_pos line
            color = 'gray50',hjust = -0.2,
            size = 3, 
            fontface = "bold") +
  
  facet_wrap(~Metric, scales = "free_y") +
  
  # Expand Y axis slightly to fit the letters
  scale_y_continuous(expand = expansion(mult = c(0.1, 0.2))) +
  
  coord_cartesian(ylim = c(lower_bound, upper_bound)) + # Zooms the view without deleting data
  
  scale_fill_brewer(palette = "Spectral", direction = -1) +
  labs(title = "Differences in Cases by Quintile",
       caption = "Groups sharing the same letter are not significantly different (Dunn's Test, p < 0.05)",
       y = "Cases") +
  theme_minimal() +
  theme(legend.position = "none")

f <- paste0('./figures/', 'health_outcome_equity_q_cld_', lc_snr, '_', ymd, '.png')
ggsave(plot = last_plot(), filename = f, width = 7, height = 5, units = 'in', dpi = 300)
```


## Plot - Regression 

```{r reg}

df_cont_l <- df_cont %>%
  st_drop_geometry() %>%
  pivot_longer(cols = any_of(metrics), names_to = "Metric", values_to = "Value") 


ggplot(df_cont_l, aes(x = Value, y = cases)) +
  # Use alpha to handle overlapping points
  geom_point(alpha = 0.1, size = 0.5, color = "gray50") + 
  # Add a smoothed trend line (blue) with confidence interval
  geom_smooth(method = "loess", color = "#2c7bb6", fill = "#2c7bb6", alpha = 0.2) +
  
  coord_cartesian(ylim = c(lower_bound, upper_bound)) + # Zooms the view without deleting data
  
  facet_wrap(~Metric, scales = "free_x") +
  labs(title = paste("Continuous Relationship: Demographics vs", outcome_ind, "Cases", sep = ' '),
       x = "Percentage",
       y = "Cases") +
  theme_minimal()

# f <- paste0('./figures/', 'health_outcome_equity_reg_', lc_snr, '_', ymd, '.png')
# ggsave(plot = last_plot(), filename = f, width = 7, height = 5, units = 'in', dpi = 300)
```



```{r lm}

library(broom)
library(purrr)


# 1. Standardize predictors (Z-score) so coefficients are comparable

df_scaled <- df_cont %>%
  st_drop_geometry() %>%
  mutate(across(any_of(metrics), scale))

# 2. Run regressions and create significance columns
models <- map_dfr(metrics, function(m) {
  formula <- as.formula(paste("cases ~", m))
  broom::tidy(lm(formula, data = df_scaled), conf.int = TRUE) %>%
    filter(term != "(Intercept)") %>%
    mutate(Metric = m)
}) %>%
  # Create significance categories and stars
  mutate(
    # Create a categorical column for coloring
    Significance = ifelse(p.value < 0.05, "Significant", "Not Significant"),
    
    # Create a text column for stars
    Stars = case_when(
      p.value < 0.001 ~ "***",
      p.value < 0.01  ~ "**",
      p.value < 0.05  ~ "*",
      TRUE            ~ ""
    )
  )

# 3. Plot the coefficients (Forest Plot)
# 3. Plot with Significance highlighting
ggplot(models, aes(x = estimate, y = reorder(Metric, estimate), color = Significance)) +
  # Add vertical line at 0 (Null effect)
  geom_vline(xintercept = 0, linetype = "dashed", color = "red") +
  
  # Error bars
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0.2) +
  
  # The Point Estimate
  geom_point(size = 3) +
  
  # Add the Stars just slightly above the point
  geom_text(aes(label = Stars), vjust = -0.8, show.legend = FALSE, size = 5) +
  
  # Manual colors: Blue for significant, Gray for not
  scale_color_manual(values = c("Significant" = "#2c7bb6", "Not Significant" = "gray70")) +
  
  labs(title = paste("Standardized Effect Sizes on", outcome_ind, "Cases", sep = " "), 
       caption = "Statistically significant associations are highlighted in blue (* p<0.05)",
       x = "Coefficient Estimate (Change in Cases per 1 SD)",
       y = "") +
  theme_minimal() +
  theme(legend.position = c(0.8, 0.1))

f <- paste0('./figures/', 'health_outcome_equity_es_', lc_snr, '_', ymd, '.png')
ggsave(plot = last_plot(), filename = f, width = 7, height = 5, units = 'in', dpi = 300)
```

Fig. x | Standardized associations between social vulnerability indicators and preventable cases. Estimates represent the change in preventable case rate per one standard deviation (SD) increase in each vulnerability metric (linear regression coefficients). Error bars denote 95% confidence intervals. Points colored in blue indicate statistically significant associations (P<0.05); gray points indicate non-significance. All metrics are standardized (Z-scores) to allow for direct comparison of effect sizes.



  
### gam
  
```{r}
library(mgcv)
library(ggplot2)

# s() tells R to make this variable "smooth" (non-linear)
gam_model <- mgcv::gam(cases ~ s(pIncomeDeprivation), data = df_cont, method = "REML")

# Check if the curve is actually non-linear
summary(gam_model) 
# Look at "Effective degrees of freedom" (edf):
# edf = 1  -> It is a straight line (Linear was fine).
# edf > 1  -> It is a curve (GAM was needed).

# Visualize the curve
plot(gam_model, pages = 1, scheme = 1, all.terms = TRUE, shade = TRUE, 
     main = "Non-linear Relationship (GAM)")





# # 1. Install 'remotes' if you don't have it
# if (!require("remotes")) install.packages("remotes")
# 
# # 2. Install 'rms' from Frank Harrell's GitHub repository
# remotes::install_github("harrelfe/rms")
# 
# # 3. Load the package
# library(rms)
# 
# # 1. Set up the data distribution
# dd <- datadist(df_cont)
# options(datadist = "dd")
# 
# # 2. Fit the model (ols = Ordinary Least Squares, same as lm but allows splines)
# rcs_model <- ols(cases ~ rcs(pIncomeDeprivation, 4), data = df)
# 
# # 3. Plot
# ggplot(Predict(rcs_model), anova = anova(rcs_model), pval = TRUE)
```



```{r - gam data}

# 1. Standardize your data (optional, but helps compare "strength" visually)
# df_scaled

# ---------------------------------------------------------
# STEP 1: Prepare Raw Data for the Rug Plot
# ---------------------------------------------------------
# We need the raw data in long format so geom_rug knows 
# where the data points are for EACH metric.
df_cont_raw <- df_scaled %>%
  select(all_of(metrics)) %>%
  pivot_longer(cols = everything(), names_to = "Metric", values_to = "x_raw")




# 2. Extract curve data for every metric
plot_data_list <- list()

# Initialize an empty list to store the stats
stats_list <- list()

for (m in metrics) {
  
  # 1. Fit the Non-Linear GAM (The Winner)
  formula <- as.formula(paste("cases ~ s(", m, ")"))
  gam_model <- gam(formula, data = df_scaled, method = "REML")
  
  
  #####################################
  ## collect stats data for later use 
  #####################################
  # 2. Fit the Linear Model (The Baseline Comparison)
  formula_lm <- as.formula(paste("cases ~", m))
  lm_model <- lm(formula_lm, data = df_scaled)
  
  
  # 3. Extract the key numbers
  summ <- summary(gam_model)
  
  # Get smooth term stats (edf, F, p-value)
  # s.table contains the significance of smooth terms
  edf_val <- summ$s.table[1, "edf"]
  f_val   <- summ$s.table[1, "F"]
  p_val   <- summ$s.table[1, "p-value"]
  
  # Get AICs
  aic_gam <- AIC(gam_model)
  aic_lm  <- AIC(lm_model)
  
  # 4. Store in a neat dataframe row
  stats_list[[m]] <- data.frame(
    Metric = m,
    EDF = round(edf_val, 2),          # Effective Degrees of Freedom (>1 means curvy)
    F_Statistic = round(f_val, 2),    # Strength of association
    P_Value = p_val,                  # Significance
    AIC_GAM = round(aic_gam, 1),
    AIC_Linear = round(aic_lm, 1),
    Delta_AIC = round(aic_lm - aic_gam, 1), # Positive = GAM is better
    Adj_R_Sq = round(summ$r.sq, 3)    # How much variance explained?
  )

  #####################################
  ## Generate data for viz 
  #####################################
  
  # Generate predictions for the curve 
  #' 1.1 Extrapolation from -2 SD to +2 SD
  # We create a new sequence of data points to draw a smooth line
  # pred_data <- data.frame(
  #   Var_Value = seq(-2.5, 2.5, length.out = 100) # Range of Z-scores
  # )
  
  # A. Define Range based on ACTUAL data (No Extrapolation)
  #' 1.2 Predict only where data exists
  actual_min <- min(df_scaled[[m]], na.rm = TRUE)
  actual_max <- max(df_scaled[[m]], na.rm = TRUE)
  
  # Create sequence only within real data range
  pred_data <- data.frame(
    Var_Value = seq(actual_min, actual_max, length.out = 100)
  )


  # Rename column to match the current metric name so predict() works
  colnames(pred_data) <- m 
  
  
  # B. Predict "Terms" (The Critical Fix)
  # type = "terms" returns the Centered Effect (0 = Mean). 
  # This makes "Predicted Change" accurate and aligns with y=0 line.
  # preds <- predict(gam_model , newdata = pred_data, type = "terms", se.fit = TRUE)
  
  # # Predict values (fit) and standard error (se.fit)
  preds <- predict(gam_model , newdata = pred_data, se.fit = TRUE)
  
  # Store clean dataframe
  plot_data_list[[m]] <- data.frame(
    Metric = m,
    x = pred_data[,1], # The Z-score
    y = preds$fit,     # The predicted Cases
    se = preds$se.fit  # The Error
  )
}

# Combine `plot data` all into one dataframe
df_cont_curve <- do.call(rbind, plot_data_list)


# Combine `model stats` into one final table
model_stats_table <- do.call(rbind, stats_list) %>%
  # Add a "Significance" column for easier reading
  mutate(Signif_Code = case_when(
    P_Value < 0.001 ~ "***",
    P_Value < 0.01  ~ "**",
    P_Value < 0.05  ~ "*",
    TRUE            ~ "ns"
  )) %>%
  # Sort by strongest non-linearity (optional)
  arrange(desc(Delta_AIC))
```


Effective Degrees of Freedom (edf): This measures "how curvy" the line is.
  edf = 1 ->  Linear (Straight line).
  edf > 1 ->  Non-linear.
  edf > 2 ->  Highly complex (supports your "J-shape" or "Saturation" claims).
 
`F-statistic` (or Chi-sq): The test statistic for the smooth term.
`AIC Difference`: Evidence that the non-linear GAM is better than a standard linear regression.
  If it is positive and large (> 10), you have mathematical proof that the Linear Model was insufficient.


```{r - gam}
# ---------------------------------------------------------
# STEP 3: The Publication Quality Plot
# ---------------------------------------------------------

gam_plot_title <- paste0("Non-linear associations between SV and preventable ", 
                          str_to_lower(outcome_ind), ' cases (', lc_snr, ")")

ggplot(df_cont_curve, aes(x = x, y = y)) +
  # Add the Confidence Interval Ribbon
  geom_ribbon(aes(ymin = y - 1.96 * se, ymax = y + 1.96 * se), 
              fill = "#2c7bb6", alpha = 0.2) +
  
  # Add the main Curve Line
  geom_line(color = "#2c7bb6", linewidth = 1) +
  
  # Facet by Metric
  # facet_wrap(~Metric) + # , scales = "free_y"
  facet_wrap(~Metric, scales = 'free_x') + # , scales = "free_y"
  
  # Add visual reference lines
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") + # Zero Effect line
  geom_vline(xintercept = 0, linetype = "dotted", color = "gray50") + # Mean Value line
  
  labs(title = gam_plot_title,
       caption = "Modeled using Generalized Additive Models (GAM)",
       x = "Vulnerability Metric (Standardized Z-Score)",
       y = "Predicted Change in Cases") +
  theme_minimal(base_size = 10) +
  theme(panel.grid.minor = element_blank())

f <- paste0('./figures/', 'health_outcome_equity_esGAM_', lc_snr, '_', ymd, '_actual.png')
# ggsave(plot = last_plot(), filename = f, width = 7, height = 5, units = 'in', dpi = 300)
```




```{r - gam plot + rug}

# ---------------------------------------------------------
# STEP 3: The Publication Quality Plot
# ---------------------------------------------------------
ggplot() +
  # --- Layer 1: The Rug Plot (Using RAW data) ---
  # distinct() prevents overplotting if you have ties, making the rug cleaner
  geom_rug(data = df_cont_raw, aes(x = x_raw), 
           sides = "b", alpha = 0.1, length = unit(0.03, "npc"), color = "gray20") +
  
  # --- Layer 2: Confidence Interval ---
  geom_ribbon(data = df_cont_curve, 
              aes(x = x, ymin = y - 1.96 * se, ymax = y + 1.96 * se), 
              fill = "#2c7bb6", alpha = 0.2) +
  
  # --- Layer 3: Main Curve ---
  geom_line(data = df_cont_curve, aes(x = x, y = y), 
            color = "#2c7bb6", linewidth = 1) +
  
  # --- Layer 4: Reference Lines ---
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") + 
  geom_vline(xintercept = 0, linetype = "dotted", color = "gray50") +
  
  # --- Aesthetics ---
  # Recommendation: Use scales = "fixed" for Z-scores so readers compare apples-to-apples.
  # If one variable has huge outliers (Z=10), then switch back to "free_x".
  facet_wrap(~Metric, scales = "fixed") + 
  
  labs(title = gam_plot_title,
       caption = "Curves represent the partial effect of each metric (centered at mean);\ntick marks indicate data density",
       x = "Vulnerability Metric (Standardized Z-Score)",
       y = "Predicted Change in Cases (Relative to Mean)") +
  
  # theme_minimal(base_size = 11) +
  theme_light(base_size = 11) +
  theme(
    panel.grid.minor = element_blank(),
    panel.grid.major = element_line(color = 'gray95', linewidth = 0.3),
    strip.text = element_text(face = "bold", size = 11)
  )

# f <- paste0('./figures/', 'health_outcome_equity_esGAM_', lc_snr, '_', ymd, '_actual2.png')
# ggsave(plot = last_plot(), filename = f, width = 7, height = 5, units = 'in', dpi = 300)
```



```{r - gam plot + density}

# Define a scaling factor to make the density fit nicely at the bottom
# 0.04 is the height (adjust to make taller/shorter)
# -0.03 is the y-position floor (adjust to move up/down)
density_scale <- 0.01
y_floor <- 0 

# Define where you want the arrows vertically
arrow_y <- 0  # Adjust this value to move arrows up/down
arrow_len <- 0.1 # Length of the arrow tip


#' --- alternatively, use `geom_ribbon`
total_y_range <- max(df_cont_curve$y, na.rm=T) - min(df_cont_curve$y, na.rm=T)
density_height <- 0.15 * total_y_range  # Density will take up 15% of the curve's height
# Set the floor just below the lowest Confidence Interval
# We subtract a tiny buffer (5% of range) so it doesn't touch the CI
y_floor <- min(df_cont_curve$y - 1.96 * df_cont_curve$se, na.rm = TRUE) - (0.05 * total_y_range)


ggplot() +
  # # --- LAYER 1: The Density Distribution (Replaces Rug) ---
  # geom_density(data = df_cont_raw, 
  #              aes(x = x_raw, 
  #                  y = after_stat(scaled) * density_scale + y_floor), 
  #              
  #              fill = "gray50", color = NA, alpha = 0.5) +
  
  # --- LAYER 1: The "Floating" Density ---
  # We use geom_ribbon with stat="density".
  # This allows us to hard-code 'ymin' to our floor, preventing the "fill to zero" issue.
  geom_ribbon(data = df_cont_raw, 
              aes(x = x_raw, 
                  ymin = y_floor,  
                  ymax = after_stat(scaled) * density_height + y_floor),
              stat = "density", 
              inherit.aes = FALSE,
              fill = "gray60", 
              color = "gray40",      # Adds a thin outline
              linewidth = 0.1,       # Thin outline
              alpha = 0.6) +
  
  # --- LAYER 2: Confidence Interval ---
  geom_ribbon(data = df_cont_curve, aes(x = x, ymin = y - 1.96 * se, ymax = y + 1.96 * se), 
              fill = "#2c7bb6", alpha = 0.2) +
  
  # --- LAYER 3: Main Curve ---
  geom_line(data = df_cont_curve, aes(x = x, y = y), 
            color = "#2c7bb6", linewidth = 1) +
  
  # --- Layer 4: Reference Lines ---
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray50", linewidth = 0.3) + 
  geom_vline(xintercept = 0, linetype = "dotted", color = "gray50") +
  
  # --- Aesthetics ---
  facet_wrap(~Metric, scales = "fixed") + 
  
  # Automatically puts a tick every 1 unit
  scale_x_continuous(breaks = scales::breaks_width(1)) + 
  
  # --- 1. Arrow pointing Left (Negative / Less Vulnerable) ---
  annotate("segment", 
           x = -0.05, xend = -2,       # Start near 0, point to -2
           y = arrow_y, yend = arrow_y, 
           colour = "gray40", alpha = 0.6, 
           arrow = arrow(length = unit(arrow_len, "cm"), type = "open")) +
  
  annotate("text", 
           x = -1.5, y = arrow_y + 0.01, # Position text above arrow
           label = "Less Vulnerable", 
           size = 2.2, color = "gray40", fontface = "italic") +

  # --- 2. Arrow pointing Right (Positive / More Vulnerable) ---
  annotate("segment", 
           x = 0.05, xend = 2,         # Start near 0, point to 2
           y = arrow_y, yend = arrow_y, 
           colour = "gray40", alpha = 0.6, 
           arrow = arrow(length = unit(arrow_len, "cm"), type = "open")) +
  
  annotate("text", 
           x = 1.5, y = arrow_y + 0.01, 
           label = "More Vulnerable", 
           size = 2.2, color = "gray40", fontface = "italic") + 

  
  
  
  labs(title = gam_plot_title,
       caption = paste0("Curves represent the partial effect of each metric (centered at mean);",
                        "Gray area = Distribution of observed data density"),
       x = "Vulnerability (Standardized Z-Score)",
       y = "Predicted Change in Cases (Relative to Mean)") +
  
  # theme_minimal(base_size = 11) +
  theme_light(base_size = 10) +
  theme(
    plot.title = element_text(size = 10),
    panel.grid.minor = element_blank(),
    panel.grid.major = element_line(color = 'gray90', linewidth = 0.1), 
    strip.text = element_text(face = "bold"),
    plot.caption = element_text(color = "gray50", face = "italic")
  )


f <- paste0('./figures/', 'health_outcome_equity_esGAM_', lc_snr, '_', ymd, '_actual.png')
ggsave(plot = last_plot(), filename = f, width = 7, height = 5, units = 'in', dpi = 300)
```


  This is not better than the above one. 
```{r - gam plot + density-ggExtra, eval=FALSE, include=FALSE}


library(ggplot2)
library(ggExtra)  # For ggMarginal
library(cowplot)  # For combining the individual plots
library(dplyr)

# List to store the final plots
final_plots_list <- list()

for (m in metrics) {
  
  # 1. Filter data for just THIS metric
  # We need the RAW data for the marginal density (so it's accurate)
  # and the CURVE data for the line.
  data_subset <- df_cont_raw %>% filter(Metric == m)
  curve_subset <- df_cont_curve %>% filter(Metric == m)
  
  # 2. Create the Base Plot
  # Note: We keep the theme very minimal to save space in the grid
  p <- ggplot() +
    # Dummy point layer (invisible) so ggMarginal knows where the data is
    geom_point(data = data_subset, aes(x = x_raw, y = 0), alpha = 0) +
    
    # Confidence Interval
    geom_ribbon(data = curve_subset, aes(x = x, ymin = y - 1.96 * se, ymax = y + 1.96 * se), 
                fill = "#2c7bb6", alpha = 0.2) +
    
    # Main Curve
    geom_line(data = curve_subset, aes(x = x, y = y), 
              color = "#2c7bb6", linewidth = 1) +
    
    scale_y_continuous(limits = c(-0.01, 0.17)) +
    scale_x_continuous(limits = c(-4, 7)) +
    
    # Reference Lines
    geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
    geom_vline(xintercept = 0, linetype = "dotted", color = "gray50") +
    
    # Labels (We keep titles strictly for the Metric name)
    labs(title = m, x = NULL, y = NULL) +
    theme_minimal(base_size = 10) +
    theme(
      plot.title = element_text(hjust = 0.5, size = 8, face = "bold"), # Center title
      axis.text = element_text(size = 8)
    )

  # 3. Apply ggMarginal
  # type = "density": Smooth curve
  # margins = "x": Only show density on the bottom (x-axis)
  # size = 4: Ratio of main plot to marginal plot height
  p_marginal <- ggMarginal(p, type = "density", margins = "x", 
                           fill = "gray80", color = NA, size = 10)
  
  # 4. Save to list
  final_plots_list[[m]] <- p_marginal
}

# 5. Combine into one grid
# adjust 'ncol' depending on how many metrics you have (e.g., 2 or 3)
combined_plot <- plot_grid(plotlist = final_plots_list, ncol = 3)

# 6. Add common X and Y labels (Optional but recommended)
final_figure <- ggdraw(add_sub(combined_plot, "Vulnerability Metric (Standardized Z-Score)", 
                               vpadding = grid::unit(0, "lines"), y = 6, x = 0.5, vjust = 4.5))
# Note: Adding a common Y-axis label with cowplot is tricky, usually easier to add in a document editor.

print(combined_plot)

f <- paste0('./figures/', 'health_outcome_equity_esGAM_', lc_snr, '_', ymd, '_actual3.5.png')
ggsave(plot = last_plot(), filename = f, width = 7, height = 5, units = 'in', dpi = 300)
```




  This is not better than the density one. 
  
```{r - gam plot + jitter points, eval=FALSE, include=FALSE}

y_floor <- -0.025

ggplot() +
  # --- LAYER 1: Jittered Points (Replaces Rug) ---
  # y = y_floor puts them at the bottom. 
  # position_jitter(height=...) scatters them vertically slightly so they don't overlap
  geom_point(data = df_cont_raw, 
             aes(x = x_raw, y = y_floor), 
             position = position_jitter(height = 0.005, width = 0), 
             color = "black", alpha = 0.02, size = 0.3) +
             
  # --- Layers 2 & 3 (Same as before) ---
  geom_ribbon(data = df_cont_curve, aes(x = x, ymin = y - 1.96 * se, ymax = y + 1.96 * se), 
              fill = "#2c7bb6", alpha = 0.2) +
  geom_line(data = df_cont_curve, aes(x = x, y = y), 
            color = "#2c7bb6", linewidth = 1) +
            
  # --- Aesthetics ---
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") + 
  facet_wrap(~Metric, scales = "fixed") + 
  theme_minimal()

# f <- paste0('./figures/', 'health_outcome_equity_esGAM_', lc_snr, '_', ymd, '_actual4.png')
# ggsave(plot = last_plot(), filename = f, width = 7, height = 5, units = 'in', dpi = 300)
```



  This is not necessary ... but just wanted to see the figures as a reference. 

```{r - gam - multivariable [x], eval=FALSE, include=FALSE}

# 1. Fit ONE big model (Multivariable)
# This controls for Income while looking at Age, etc.
full_model <- gam(cases ~ s(pIncomeDeprivation) + s(p75over) + s(pBAME) + 
                          s(pSocial_housing) + s(p5under) + s(pNotEnglishProficient),
                  data = df_scaled, method = "REML")

# 2. Use 'gratia' to draw the Partial Effects
# This extracts the exact curve for each variable, holding others constant.
library(gratia)
draw(full_model, scales = "fixed") & # scales="fixed" allows comparison of magnitude
  theme_minimal() &
  labs(title = NULL, subtitle = NULL, caption = NULL) # 

```

  This is not very intuitive ...

```{r - gam + box, eval=FALSE, include=FALSE}

# 1. Prepare Data: Pivot Long and Ensure Quintiles are calculated
plot_data_hybrid <- df_scaled %>%
  select(cases, all_of(metrics)) %>%
  pivot_longer(cols = all_of(metrics), names_to = "Metric", values_to = "Z_Score") %>%
  group_by(Metric) %>%
  mutate(
    # Recalculate Quintiles (just to be safe)
    Quintile = ntile(Z_Score, 5),
    Quintile_Factor = factor(Quintile)
  ) %>%
  ungroup()

# 2. Calculate the 'Position' for each Boxplot
# We want the boxplot to sit at the MEDIAN Z-score of that quintile
boxplot_positions <- plot_data_hybrid %>%
  group_by(Metric, Quintile_Factor) %>%
  summarise(
    center_x = median(Z_Score, na.rm = TRUE), # Where to put the box on X-axis
    .groups = "drop"
  )

# 3. Create the Plot
ggplot() +
  # --- LAYER 1: The Raw Data (Optional, faint jitter to show density) ---
  geom_point(data = plot_data_hybrid, aes(x = Z_Score, y = cases, color = Quintile_Factor), 
             alpha = 0.05, size = 0.5) +
  
  # --- LAYER 2: The Boxplots (Positioned correctly on X-axis) ---
  # We use 'group' to force distinct boxes, but map 'x' to the continuous value
  geom_boxplot(data = plot_data_hybrid, 
               aes(x = Z_Score, y = cases, group = Quintile_Factor, fill = Quintile_Factor), 
               outlier.shape = NA, alpha = 0.3, width = 0.5) +
  
  # --- LAYER 3: The GAM Curve (The Trend) ---
  # We use geom_smooth directly here
  geom_smooth(data = plot_data_hybrid, aes(x = Z_Score, y = cases), 
              method = "gam", color = "purple", linewidth = 1, se = FALSE) +
  
  # --- Styling ---
  # facet_wrap(~Metric, scales = "fixed") + # Fixed scales to compare magnitude
  facet_wrap(~Metric, scale = 'free') +
  scale_fill_brewer(palette = "Spectral") +
  scale_color_brewer(palette = "Spectral") +
  scale_y_continuous(limits = c(0, 0.15)) +
  
  labs(title = "Synthesis: Continuous Trends vs. Grouped Quintiles",
       subtitle = "Black Line = GAM Mean Trend | Boxes = Quintile Distribution (Median/IQR)",
       x = "Vulnerability Metric (Standardized Z-Score)",
       y = "Predicted Change in Cases") +
  theme_minimal() +
  theme(legend.position = "bottom")


# f <- paste0('./figures/', 'health_outcome_equity_esGAM_', lc_snr, '_box_', ymd, '.png')
# ggsave(plot = last_plot(), filename = f, width = 7, height = 5, units = 'in', dpi = 300)
```



*glm*

Summary of Recommendations
- lm (Linear)	                    Data is normally distributed (Rare for cases).	
- glm.nb (Negative Binomial)	    Data is counts, skewed, variance > mean. (Most likely)	IRR (Rate Ratio)
- lme4::glmer.nb (Mixed Effects)	You have data from many different cities/regions and need to control for that grouping.	IRR (Rate Ratio)

*glm.nb (Negative Binomial), poisson, and Gamma cannot handle negative numbers. They will crash or give wrong answers.*

```{r - glm_nb [x], eval=FALSE, include=FALSE}

library(dplyr)
library(ggplot2)
library(broom)
library(purrr)
library(MASS) # Required for glm.nb

# 1. Standardize your metrics (Z-scores)
# This ensures the Rate Ratio is "per 1 SD increase" in deprivation
df_scaled <- df_scaled

# 2. Run Negative Binomial Regression for each metric
models_nb <- map_dfr(metrics, function(m) {
  
  # Formula: cases ~ metric + offset(log(population))
  # Note: If you have a 'population' column, use offset() to model RATES, not just counts.
  # If 'cases' is already a rate, remove the offset.
  
  formula_str <- paste("cases ~", m) # Add "+ offset(log(population))" if 'cases' is a raw count
  
  tryCatch({
    model <- glm.nb(as.formula(formula_str), data = df_scaled)
    
    # Tidy the results, asking for Exponentiated coefficients (Rate Ratios)
    broom::tidy(model, conf.int = TRUE, exponentiate = TRUE) %>%
      filter(term != "(Intercept)") %>%
      mutate(Metric = m)
  }, error = function(e) return(NULL)) # Skips if model fails to converge
}) %>%
  mutate(
    Significance = ifelse(p.value < 0.05, "Significant", "NS"),
    Stars = case_when(
      p.value < 0.001 ~ "***",
      p.value < 0.01  ~ "**",
      p.value < 0.05  ~ "*",
      TRUE            ~ ""
    )
  )

# 3. Plot the Rate Ratios (Forest Plot)
ggplot(models_nb, aes(x = estimate, y = reorder(Metric, estimate), color = Significance)) +
  # Reference line is now at 1 (Rate Ratio of 1 = No Effect), not 0
  geom_vline(xintercept = 1, linetype = "dashed", color = "gray50") +
  
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0.2) +
  geom_point(size = 3) +
  geom_text(aes(label = Stars), vjust = -0.8, show.legend = FALSE) +
  
  scale_color_manual(values = c("Significant" = "#2c7bb6", "NS" = "gray70")) +
  
  labs(title = "Impact of Vulnerability on Disease Risk (Negative Binomial)",
       subtitle = "Values represent Incidence Rate Ratios (IRR) per 1 SD increase in metric",
       x = "Incidence Rate Ratio (IRR) [1 = No Change]",
       y = "") +
  theme_minimal() +
  theme(legend.position = c(0.8, 0.1))

f <- paste0('./figures/', 'health_outcome_equity_esIRR_', lc_snr, '.png')
# ggsave(plot = last_plot(), filename = f, width = 7, height = 5, units = 'in', dpi = 300)
```



### Compare models

```{r}

# Calculate Mean and Variance of your outcome
mean_val <- mean(df_scaled$cases, na.rm = TRUE)
var_val  <- var(df_scaled$cases, na.rm = TRUE)

print(paste("Mean:", round(mean_val, 2)))
print(paste("Variance:", round(var_val, 2)))

hist(df_scaled$cases, nclass = 300)

# Compare simplest versions
model_lm  <- lm(cases ~ pIncomeDeprivation, data = df_scaled)
model_nb  <- MASS::glm.nb(cases ~ pIncomeDeprivation, data = df_scaled)
model_gam <- mgcv::gam(cases ~ s(pIncomeDeprivation), data = df_scaled, method = "REML")


# model_lm  <- lm(cases ~ pIncomeDeprivation + p75over, data = df_scaled)
# model_nb  <- MASS::glm.nb(cases ~ pIncomeDeprivation  + p75over, data = df_scaled)

AIC(model_lm, model_nb, model_gam)

library(performance)
# # Compare performance visually
# check_model(model_lm)
# check_model(model_nb)

# # return a list of single plots
# diagnostic_plots <- plot(check_model(model_lm, panel = FALSE))
# # posterior predicive checks
# diagnostic_plots[[1]]
# 
# diagnostic_plots <- plot(check_model(model_nb, panel = FALSE))
# diagnostic_plots[[1]]

# model_performance(model_lm)
# model_performance(model_nb)

# compare_performance(model_lm, model_nb, verbose = FALSE)
compare_performance(model_lm, model_nb, model_gam, verbose = FALSE, rank = T)

plot(compare_performance(model_lm, model_nb, model_gam, rank = TRUE, verbose = FALSE))
```


## Map - bicolor?


 
  

```{r - one}
# install.packages("biscale")

library(biscale)
library(cowplot)

# 1. Create Bivariate Classes (3x3 Grid)
# style = "quantile" splits data into equal groups (like your quintiles)
# dim = 3 creates a 3x3 grid (Low/Med/High)
data_bivariate <- bi_class(df_cont, x = pIncomeDeprivation, y = cases, style = "quantile", dim = 3)

# 2. Create the Map
map <- ggplot() +
  geom_sf(data = data_bivariate, aes(fill = bi_class), color = 'gray', size = 0.05, show.legend = FALSE) +
  bi_scale_fill(pal = "DkBlue", dim = 3) + # Palette options: "DkBlue", "DkCyan", "GrPink", etc.
  labs(title = "Income Deprivation vs. Preventable Cases") +
  bi_theme(base_size = 10) +
  # This removes the white box behind the square legend
  theme(
    plot.background = element_rect(fill = "transparent", color = NA),
    panel.background = element_rect(fill = "transparent", color = NA)
  )

# 3. Create the Square Legend
legend <- bi_legend(pal = "DkBlue",
                    dim = 3,
                    xlab = "Deprivation ",
                    ylab = "Cases ",
                    size = 8) +
  theme(
    # This sets the overall background of the legend plot to transparent
    plot.background = element_rect(fill = "transparent", color = NA),
    panel.background = element_rect(fill = "transparent", color = NA),
    # This ensures no other rectangle elements (like borders) have color
    rect = element_rect(fill = "transparent", color = NA)
  )


# 4. Combine Map and Legend
final_plot <- ggdraw() +
  draw_plot(map, 0, 0, 1, 1) +
  draw_plot(legend, x = 0.7, y = 0.05, width = 0.2, height = 0.2) # Adjust x/y position of legend

print(final_plot)

f <- paste0('./figures/', 'health_outcome_equity_map_', lc_snr, '_pIncomeDeprivation--.png')
ggsave(plot = last_plot(), filename = f, width = 7, height = 5, units = 'in', dpi = 300)
```

```{r - loop}

# List of your metrics
# metrics <- c("pIncomeDeprivation", "pBAME", "pSocial_housing", "p75over")

# Loop through each metric
for (var in metrics) {
  
  # 1. Prepare data (Note: We rename columns dynamically for bi_class to work easier)
  temp_gdf <- df_cont %>%
    mutate(x_var = .data[[var]], 
           y_var = -cases) %>%
    bi_class(x = x_var, y = y_var, style = "quantile", dim = 3)
  
  # 2. Map
  map <- ggplot() +
    geom_sf(data = temp_gdf, aes(fill = bi_class), color = NA, size = 0.05, show.legend = FALSE) +
    bi_scale_fill(pal = "DkBlue", dim = 3) +
    labs(title = paste0(var, " vs. Cases")) +
    bi_theme(base_size = 10)
  
  # 3. Legend
  legend <- bi_legend(pal = "DkBlue", dim = 3, xlab = "Vulnerability", ylab = "Cases", size = 8) +
    theme(
      # This sets the overall background of the legend plot to transparent
      plot.background = element_rect(fill = "transparent", color = NA),
      panel.background = element_rect(fill = "transparent", color = NA),
      # This ensures no other rectangle elements (like borders) have color
      rect = element_rect(fill = "transparent", color = NA)
  )
  
  # 4. Combine
  final_plot <- ggdraw() +
    draw_plot(map, 0, 0, 1, 1) +
    draw_plot(legend, 0.7, 0.05, 0.2, 0.2)
  
  # 5. Save
  f <- paste0('./figures/', 'health_outcome_equity_map_', lc_snr, '_', var, "-neg.png"); f
  ggsave(filename = f, plot = final_plot, width = 7, height = 5)
}
```




